---
layout: single
title:  "[ECC DS 1ì£¼ì°¨] íƒ€ì´íƒ€ë‹‰ íŠœí† ë¦¬ì–¼1 - EDA To Prediction(DieTanic)
categories: ML
tags: [ECC, DS] 
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


# **0. í”„ë¡œì íŠ¸ ì†Œê°œ**


- **ë…¸íŠ¸ë¶ì˜ ëª©í‘œ**: ì˜ˆì¸¡ ëª¨ë¸ë§ ë¬¸ì œì—ì„œ ì›Œí¬ í”Œë¡œìš°ê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ì— ëŒ€í•œ ì•„ì´ë””ì–´ë¥¼ ì œê³µ

- feature í™•ì¸ ë°©ë²•, ìƒˆë¡œìš´ featureì˜ ì¶”ê°€ ë° Machine Learning ê°œë… ì ìš©



# **1. Exploratory Data Analysis(EDA)**



```python
### Import Libraries

import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns # í˜„ì¬ ì½”ë©ì—ì„œëŠ” 0.12.0 ë²„ì „

plt.style.use('fivethirtyeight')

import warnings
warnings.filterwarnings('ignore')

%matplotlib inline
```

**âŒ Version Issue**

- seaborn version issueë¡œ ì¸í•´ ì›ë³¸ ë…¸íŠ¸ë¶ì—ì„œ ì¼ë¶€ í•¨ìˆ˜ ë³€ê²½(factorplot -> pointplot, catplot)

- plottingì„ ìˆ˜í–‰í•  ë•Œ ëª‡ëª‡ í•¨ìˆ˜ëŠ” ìœ„ì¹˜ ë§¤ê°œë³€ìˆ˜ ì§€ì •ì´ ì˜ ë˜ì§€ x -> í‚¤ì›Œë“œ ë§¤ê°œë³€ìˆ˜ë¡œ ë³€ê²½



```python
### ë°ì´í„° ì¤€ë¹„í•˜ê¸°

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ECC 48á„€á…µ á„ƒá…¦á„€á…ªB/1á„Œá…®á„á…¡/data/train.csv')
```


```python
### ì¼ë¶€ ë°ì´í„° í™•ì¸

data.head()
```

<pre>
   PassengerId  Survived  Pclass  \
0            1         0       3   
1            2         1       1   
2            3         1       3   
3            4         1       1   
4            5         0       3   

                                                Name     Sex   Age  SibSp  \
0                            Braund, Mr. Owen Harris    male  22.0      1   
1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   
2                             Heikkinen, Miss. Laina  female  26.0      0   
3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   
4                           Allen, Mr. William Henry    male  35.0      0   

   Parch            Ticket     Fare Cabin Embarked  
0      0         A/5 21171   7.2500   NaN        S  
1      0          PC 17599  71.2833   C85        C  
2      0  STON/O2. 3101282   7.9250   NaN        S  
3      0            113803  53.1000  C123        S  
4      0            373450   8.0500   NaN        S  
</pre>

```python
### ê²°ì¸¡ì¹˜ í™•ì¸

data.isnull().sum() 
```

<pre>
PassengerId      0
Survived         0
Pclass           0
Name             0
Sex              0
Age            177
SibSp            0
Parch            0
Ticket           0
Fare             0
Cabin          687
Embarked         2
dtype: int64
</pre>
- Age, Cabin, Embarkedì— ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•œë‹¤.


## **1-1. Features**



```python
### Survived

f,ax = plt.subplots(1,2,figsize = (18,8))

data['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)
ax[0].set_title('Survived')
ax[0].set_ylabel('')

sns.countplot(x = 'Survived', data = data, ax = ax[1])
ax[1].set_title('Survived')

plt.show()
```

<pre>
<Figure size 1296x576 with 2 Axes>
</pre>
- ì‚¬ê³ ì—ì„œ ì‚´ì•„ë‚¨ì€ ìŠ¹ê°(Survived = 1)ì´ ë§ì§€ ì•Šì€ ê²ƒì€ ë¶„ëª…í•˜ë‹¤.

  - 891ëª…ì˜ ìŠ¹ê° ì¤‘ 350ëª…ë§Œì´ ì‚´ì•„ë‚¨ìŒ


### **ğŸ“Œ Featureì˜ ì¢…ë¥˜**


**1. ë²”ì£¼í˜• ë³€ìˆ˜(Categorical Features)**  

- ë‘ ê°œ ì´ìƒì˜ ë²”ì£¼ê°€ ìˆëŠ” ë³€ìˆ˜

  - í•´ë‹¹ í”¼ì³ì˜ ê° ê°’ì€ ë²”ì£¼ë³„ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆìŒ

- ë³€ìˆ˜ë¥¼ ë¶„ë¥˜í•˜ê±°ë‚˜ ìˆœì„œë¥¼ ì§€ì •í•  ìˆ˜ ì—†ìŒ

- Sex, Embarkedê°€ í•´ë‹¹ë¨



**2. ìˆœì„œí˜• ë³€ìˆ˜(Ordinal Features)**  

- ë²”ì£¼í˜• ê°’ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ê°’ ì‚¬ì´ì— ìƒëŒ€ì  ìˆœì„œ ë˜ëŠ” ì •ë ¬ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì´ ì°¨ì´

  - ex> ë†’ì´(ë†’ì´), ì¤‘ê°„(ì¤‘ê°„), ì§§ì€ ê°’ê³¼ ê°™ì€ ê¸°ëŠ¥ì´ ìˆëŠ” ê²½ìš° ë†’ì´ëŠ” ìˆœì„œí˜• ë³€ìˆ˜

- ë³€ìˆ˜ì— ìƒëŒ€ì ì¸ ì •ë ¬ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

- PClassê°€ í•´ë‹¹ë¨



**3. ì—°ì†í˜• ë³€ìˆ˜(Continous Feature)**  

- featureê°€ ë‘ ì  ì‚¬ì´ ë˜ëŠ” feature columnì˜ ìµœì†Œê°’ ë˜ëŠ” ìµœëŒ€ê°’ ì‚¬ì´ì˜ ê°’ì„ ì·¨í•  ìˆ˜ ìˆëŠ” ê²½ìš°

- Ageê°€ í•´ë‹¹ë¨


## **1-2. ë‹¤ì–‘í•œ featureë“¤ ì‚¬ì´ì˜ ê´€ê³„**





### **Sex**

- Categorical ë³€ìˆ˜



```python
data.groupby(['Sex','Survived'])['Survived'].count()
```

<pre>
Sex     Survived
female  0            81
        1           233
male    0           468
        1           109
Name: Survived, dtype: int64
</pre>

```python
f,ax = plt.subplots(1,2,figsize = (18,8))

data[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax = ax[0])
ax[0].set_title('Survived vs Sex')

sns.countplot(x = 'Sex',hue = 'Survived',data = data,ax = ax[1])
ax[1].set_title('Sex:Survived vs Dead')

plt.show()
```

<pre>
<Figure size 1296x576 with 2 Axes>
</pre>
- ë°°ì— íƒ€ê³  ìˆë˜ ë‚¨ì„±ì˜ ìˆ˜ëŠ” ì—¬ì„±ì˜ ìˆ˜ë³´ë‹¤ í›¨ì”¬ ë§ë‹¤.

  - ê·¸ëŸ¬ë‚˜ êµ¬ì¡°ëœ ì—¬ì„±ì˜ ìˆ˜ëŠ” êµ¬ì¡°ëœ ë‚¨ì„±ì˜ ê±°ì˜ ë‘ ë°°ì„

- ì—¬ì„±ì˜ ìƒì¡´ìœ¨ì€ ì•½ 75%ì¸ ë°˜ë©´ ë‚¨ì„±ì˜ ìƒì¡´ìœ¨ì€ ì•½ 18~19%ì„

- SexëŠ” ëª¨ë¸ë§ ì‹œ êµ‰ì¥íˆ ì¤‘ìš”í•œ ë³€ìˆ˜ì¸ ê²ƒ ê°™ìŒ


### **PClass**

- ìˆœì„œí˜• ë³€ìˆ˜



```python
pd.crosstab(data.Pclass,data.Survived,margins = True).style.background_gradient(cmap = 'summer_r')
```

<pre>
<pandas.io.formats.style.Styler at 0x7f3cc1b2d0a0>
</pre>

```python
f,ax = plt.subplots(1,2,figsize = (18,8))

data['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])
ax[0].set_title('Number Of Passengers By Pclass')
ax[0].set_ylabel('Count')

sns.countplot(x = 'Pclass',hue='Survived',data=data,ax=ax[1])
ax[1].set_title('Pclass:Survived vs Dead')

plt.show()
```

<pre>
<Figure size 1296x576 with 2 Axes>
</pre>
- PClassê°€ 1ì¸ ìŠ¹ê°ë“¤ì´ ìš°ì„ ì ìœ¼ë¡œ êµ¬ì¡°ë˜ì—ˆìŒì„ ì§ì‘í•  ìˆ˜ ìˆìŒ

  - PClassê°€ 3ì¸ ìŠ¹ê° ìˆ˜ê°€ í›¨ì”¬ ë” ë§ì•˜ì§€ë§Œ, ì—¬ì „íˆ ê·¸ë“¤ ì¤‘ ìƒì¡´ì ìˆ˜ëŠ” 25% ì •ë„ë¡œ ë§¤ìš° ë‚®ìŒ

  - Pclassê°€ 1ì¸ ê²½ìš° ìƒì¡´ìœ¨ì´ ì•½ 63%ì¸ ë°˜ë©´ Pclassê°€ 2ì¸ ê²½ìš° ì•½ 48%ì„



### **Sex + Pclass**



```python
pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).style.background_gradient(cmap='summer_r')
```

<pre>
<pandas.io.formats.style.Styler at 0x7f3cc263ae50>
</pre>

```python
sns.pointplot(x = 'Pclass', y = 'Survived',hue='Sex',data = data) 
plt.show()
```

<pre>
<Figure size 432x288 with 1 Axes>
</pre>
- ë²”ì£¼í˜• ê°’ë“¤ì„ ì‰½ê²Œ ë¶„ë¦¬í•˜ì—¬ íŒŒì•…í•˜ê¸° ìœ„í•´ ```pointplot()```ì„ ì‚¬ìš©

- Pclass = 1ì¸ ì—¬ì„±ì˜ ìƒì¡´ìœ¨ì´ ì•½ 95~96%(94ëª… ì¤‘ 3ëª…ë§Œ ì‚¬ë§)

  - PClassì™€ ìƒê´€ì—†ì´ êµ¬ì¡° ê³¼ì •ì—ì„œ ì—¬ì„±ì—ê²Œ

  ìš°ì„  ìˆœìœ„ê°€ ë¶€ì—¬ëœ ê²ƒì€ ë¶„ëª…í•¨

- PClassë„ ì¤‘ìš”í•œ featureë¼ê³  ìƒê°í•  ìˆ˜ ìˆìŒ


### **Age**

- ì—°ì†í˜• ë³€ìˆ˜



```python
print('Oldest Passenger was of:',data['Age'].max(),'Years')
print('Youngest Passenger was of:',data['Age'].min(),'Years')
print('Average Age on the ship:',data['Age'].mean(),'Years')
```

<pre>
Oldest Passenger was of: 80.0 Years
Youngest Passenger was of: 0.42 Years
Average Age on the ship: 29.69911764705882 Years
</pre>

```python
f,ax = plt.subplots(1,2,figsize=(18,8))

sns.violinplot(x = "Pclass",y = "Age", hue="Survived", data=data,split=True,ax=ax[0])
ax[0].set_title('Pclass and Age vs Survived')
ax[0].set_yticks(range(0,110,10))

sns.violinplot(x = "Sex",y = "Age", hue="Survived", data=data,split=True,ax=ax[1])
ax[1].set_title('Sex and Age vs Survived')
ax[1].set_yticks(range(0,110,10))

plt.show()
```

<pre>
<Figure size 1296x576 with 2 Axes>
</pre>
**âœ” Observations**  

- ì–´ë¦°ì´ì˜ ìˆ˜ëŠ” PClassì— ë”°ë¼ ì¦ê°€í•˜ë©° 10ì„¸ ë¯¸ë§Œì˜ ìŠ¹ê°(ì¦‰, ì–´ë¦°ì´)ì˜ ìƒì¡´ìœ¨ì€ PClassì™€ ìƒê´€ì—†ì´ ì–‘í˜¸í•œ ê²ƒìœ¼ë¡œ ë³´ì„

- Pclass = 1ì—ì„œ 20-50ì„¸ ìŠ¹ê°ì˜ ìƒì¡´ ê°€ëŠ¥ì„±ì€ ë†’ê³  ì—¬ì„±ì´ í›¨ì”¬ ë” ë†’ìŒ

- ë‚¨ì„±ì˜ ê²½ìš°, ìƒì¡´ ê°€ëŠ¥ì„±ì€ ë‚˜ì´ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ê°ì†Œ







**âœ” ê²°ì¸¡ì¹˜(NaN) ì²˜ë¦¬**  

- Age featureì—ëŠ” 177ê°œì˜ null ê°’ì´ ìˆìŒ

- NaN ê°’ì„ ëŒ€ì²´í•˜ê¸° ìœ„í•´ ë°ì´í„° ì„¸íŠ¸ì˜ í‰ê·  ì—°ë ¹ì„ í• ë‹¹í•  ìˆ˜ ìˆìŒ

  - but ì‚¬ëŒë“¤ì˜ ì—°ë ¹ì€ ë§¤ìš° ë‹¤ì–‘í•¨

- ì´ë¦„ ì•ì˜ ë¶™ì€ í‚¤ì›Œë“œ(Mr, Mrs.)ë¥¼ í†µí•´ ê·¸ë£¹í™” í›„ ê° ê·¸ë£¹ì˜ í‰ê· ê°’ì„ í• ë‹¹ ê°€ëŠ¥



```python
### ì´ë¦„ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ê¸°

data['Initial'] = 0
for i in data:
    data['Initial'] = data.Name.str.extract('([A-Za-z]+)\.') # ì´ë¦„ì—ì„œ .(dot) ì•ì— ë¶€ë¶„ë§Œ ì¶”ì¶œ
```


```python
### Sexì™€ í•¨ê»˜ ì´ë‹ˆì…œ í™•ì¸í•˜ê¸°

pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r') 
```

<pre>
<pandas.io.formats.style.Styler at 0x7f3cc130a820>
</pre>

```python
### ì˜ëª» í‘œê¸°ëœ ì´ë‹ˆì…œ ë³€ê²½
# Mileì´ë‚˜ Mme ë“±

data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace = True)
```


```python
### ì´ë‹ˆì…œ ë³„ í‰ê·  ë‚˜ì´

data.groupby('Initial')['Age'].mean() 
```

<pre>
Initial
Master     4.574167
Miss      21.860000
Mr        32.739609
Mrs       35.981818
Other     45.888889
Name: Age, dtype: float64
</pre>

```python
### ê²°ì¸¡ì¹˜ ì²˜ë¦¬(NaN ì±„ìš°ê¸°)
# í‰ê·  ì—°ë ¹ì˜ ì˜¬ë¦¼ ê°’ì„ í™œìš©

data.loc[(data.Age.isnull()) & (data.Initial=='Mr'),'Age'] = 33
data.loc[(data.Age.isnull()) & (data.Initial=='Mrs'),'Age'] = 36
data.loc[(data.Age.isnull()) & (data.Initial=='Master'),'Age'] = 5
data.loc[(data.Age.isnull()) & (data.Initial=='Miss'),'Age'] = 22
data.loc[(data.Age.isnull()) & (data.Initial=='Other'),'Age'] = 46
```


```python
data.Age.isnull().any() 

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!
```

<pre>
False
</pre>

```python
### ì‹œê°í™”

f,ax = plt.subplots(1,2,figsize=(20,10))

data[data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')
ax[0].set_title('Survived= 0')
x1 = list(range(0,85,5))
ax[0].set_xticks(x1)

data[data['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')
ax[1].set_title('Survived= 1')
x2=list(range(0,85,5))
ax[1].set_xticks(x2)

plt.show()
```

<pre>
<Figure size 1440x720 with 2 Axes>
</pre>
**âœ” Observations**  

- ìœ ì•„(ì—°ë ¹ì´ 5ì„¸ ë¯¸ë§Œ)ëŠ” ìƒì¡´ë¥ ì´ ë†’ìŒ

- ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ìŠ¹ê°ì€ êµ¬ì¡°ë˜ì—ˆìŒ(80ì„¸)

- ê°€ì¥ ë§ì´ ì‚¬ë§í•œ ìŠ¹ê°ë“¤ì˜ ì—°ë ¹ëŒ€ëŠ” 30 ~ 40ëŒ€


### **Embarked**  

- ë²”ì£¼í˜• ë³€ìˆ˜



```python
pd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap='summer_r')
```

<pre>
<pandas.io.formats.style.Styler at 0x7f3cc11b9fa0>
</pre>

```python
### Embarkedì— ë”°ë¥¸ ìƒì¡´ë¥ 

sns.pointplot(x = 'Embarked',y = 'Survived',data = data)
fig = plt.gcf()
fig.set_size_inches(5,3)
plt.show()
```

<pre>
<Figure size 360x216 with 1 Axes>
</pre>
- í•­êµ¬ Cì—ì„œì˜ ìƒì¡´ë¥ ì´ 0.55 ì •ë„ë¡œ ê°€ì¥ ë†’ìŒ

- í•­êµ¬ Sì—ì„œì˜ ìƒì¡´ë¥ ì´ ê°€ì¥ ë‚®ìŒ



```python
f,ax = plt.subplots(2,2,figsize = (20,15))

sns.countplot(x = 'Embarked',data = data,ax = ax[0,0])
ax[0,0].set_title('No. Of Passengers Boarded')

sns.countplot(x = 'Embarked',hue = 'Sex',data = data,ax = ax[0,1])
ax[0,1].set_title('Male-Female Split for Embarked')

sns.countplot(x = 'Embarked',hue = 'Survived',data = data,ax = ax[1,0])
ax[1,0].set_title('Embarked vs Survived')

sns.countplot(x = 'Embarked',hue = 'Pclass',data = data,ax = ax[1,1])
ax[1,1].set_title('Embarked vs Pclass')

plt.subplots_adjust(wspace = 0.2,hspace = 0.5)
plt.show()
```

<pre>
<Figure size 1440x1080 with 4 Axes>
</pre>
**âœ” Observations**  

- Sì—ì„œ íƒ‘ìŠ¹í•œ ìŠ¹ê°ë“¤ ì¤‘ ëŒ€ë‹¤ìˆ˜ëŠ” Pclass = 3 ì¶œì‹ 

- Cì—ì„œ ì˜¨ ìŠ¹ê°ë“¤ì€ ê·¸ë“¤ ì¤‘ ìƒë‹¹í•œ ë¹„ìœ¨ì´ ì‚´ì•„ë‚¨ìŒ

  - Pclass = 1ê³¼ Pclass = 2 ìŠ¹ê° ì „ì›ì„ êµ¬ì¡°í•œ ê²ƒì¼ ìˆ˜ ìˆìŒ

- Embarked =  SëŠ” ëŒ€ë¶€ë¶„ ë¶€ìë“¤ì´ íƒ‘ìŠ¹í•œ í•­êµ¬ë¡œ ë³´ì„

  - ì—¬ì „íˆ ì´ê³³ì—ì„œëŠ” ìƒì¡´ ê°€ëŠ¥ì„±ì´ ë‚®ìŒ

  - PClass = 3 ìŠ¹ê°ì˜ 81%ê°€ ì •ë„ ì‚´ì•„ë‚¨ì§€ ëª»í•¨

- Port Q had almost 95% of the passengers were from Pclass3



```python
sns.catplot(x = 'Pclass',y = 'Survived',hue = 'Sex',col = 'Embarked',data = data, kind = 'point')
plt.show()
```

<pre>
<Figure size 1150.5x360 with 3 Axes>
</pre>
**âœ” Observations**  

- PClassì— ê´€ê³„ì—†ì´ PClass = 1ê³¼ PClass = 2ì˜ ì—¬ì„±ì˜ ìƒì¡´ í™•ë¥ ì€ ê±°ì˜ 1ì´ë‹¤.

- Pclass = 3ì— ëŒ€í•´ Embarked = SëŠ” ë‚¨ë…€ ëª¨ë‘ ìƒì¡´ìœ¨ì´ ë§¤ìš° ë‚®ìŒ

- Embarked = QëŠ” ê±°ì˜ ëª¨ë‘ PClass = 3 ì¶œì‹ 

  - ë‚¨ì„±ì—ê²Œ ê°€ì¥ ë¶ˆìš´í•œ ê²ƒìœ¼ë¡œ ë³´ì„



**âœ” ê²°ì¸¡ì¹˜(NaN) ì²˜ë¦¬**  

- ë§ì€ ìŠ¹ê°ë“¤ì´ S í•­êµ¬ì—ì„œ íƒ‘ìŠ¹í•˜ì˜€ìŒ

  - NaNì„ Së¡œ ëŒ€ì²´



```python
data['Embarked'].fillna('S',inplace = True)
```


```python
data.Embarked.isnull().any()

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ê°€ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë¨
```

<pre>
False
</pre>
### **SibSip**

- ì´ì‚° ë³€ìˆ˜(Discrete Feature)

- í˜¼ì íƒ”ëŠ”ì§€ ì•„ë‹ˆë©´ ê·¸ì˜ ê°€ì¡± êµ¬ì„±ì›ê³¼ í•¨ê»˜ íƒ”ëŠ”ì§€

- í˜•ì œ => í˜•ì œ, ìë§¤, ì˜ë¶“ë™ìƒ, ì˜ë¶“ì–¸ë‹ˆ

- ë°°ìš°ì => ë‚¨í¸, ì•„ë‚´



```python
pd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap='summer_r')
```

<pre>
<pandas.io.formats.style.Styler at 0x7f3cc23f8250>
</pre>

```python
### ì‹œê°í™”

f,ax = plt.subplots(1,2,figsize = (20,8))

sns.barplot(x = 'SibSp',y = 'Survived',data = data, ax = ax[0])
ax[0].set_title('SibSp vs Survived')

sns.pointplot(x = 'SibSp',y = 'Survived',data = data, ax = ax[1])
ax[1].set_title('SibSp vs Survived')
plt.close(2)

plt.show()
```

<pre>
<Figure size 1440x576 with 2 Axes>
</pre>

```python
### PClassì— ë”°ë¥¸ ìì‹ ìˆ˜

pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap='summer_r')
```

<pre>
<pandas.io.formats.style.Styler at 0x7f3cc12a8e80>
</pre>
**âœ” Observartions**  

- barplot ë° factorplotì€ ìŠ¹ê°ì´ í˜¼ì íƒ‘ìŠ¹í•œ ê²½ìš°(SibSp = 0) ìƒì¡´ìœ¨ì´ 34.5%ì„ì„ ë³´ì—¬ì¤Œ

  - í˜•ì œìë§¤ì˜ ìˆ˜ê°€ ì¦ê°€í•˜ë©´ ê·¸ë˜í”„ëŠ” ëŒ€ëµ ê°ì†Œ

  - ì¦‰, ë§Œì•½ ê°€ì¡±ì„ íƒœìš´ë‹¤ë©´, ìì‹ ì„ ë¨¼ì € êµ¬í•˜ëŠ” ëŒ€ì‹  ê·¸ë“¤ì„ êµ¬í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•  ê²ƒ

-  êµ¬ì„±ì›ì´ 5-8ëª…ì¸ ê°€ì¡±ì˜ ìƒì¡´ìœ¨ì€ 0%

  - ì´ìœ : PClass

  - crosstab => SibSp > 3ë¥¼ ê°€ì§„ ì‚¬ëŒì´ ëª¨ë‘ Pclass = 3ì„ì„ íŒŒì•…í•  ìˆ˜ ìˆìŒ


### **Parch**  




```python
pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')
```

<pre>
<pandas.io.formats.style.Styler at 0x7f3cc12ba430>
</pre>
- ê°€ì¡± êµ¬ì„±ì›ì˜ ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ì£¼ë¡œ PClass = 3ì— ì†í•¨



```python
### ì‹œê°í™”

f,ax = plt.subplots(1,2,figsize = (20,8))

sns.barplot(x = 'Parch',y = 'Survived',data=data,ax=ax[0])
ax[0].set_title('Parch vs Survived')

sns.pointplot(x = 'Parch',y = 'Survived',data=data,ax=ax[1])
ax[1].set_title('Parch vs Survived')
plt.close(2)

plt.show()
```

<pre>
<Figure size 1440x576 with 2 Axes>
</pre>
### **Fare**

- ì—°ì†í˜• ë³€ìˆ˜




```python
print('Highest Fare was:',data['Fare'].max())
print('Lowest Fare was:',data['Fare'].min())
print('Average Fare was:',data['Fare'].mean())
```

<pre>
Highest Fare was: 512.3292
Lowest Fare was: 0.0
Average Fare was: 32.204207968574636
</pre>

```python
f,ax = plt.subplots(1,3,figsize=(20,8))

sns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])
ax[0].set_title('Fares in Pclass 1')

sns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])
ax[1].set_title('Fares in Pclass 2')

sns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])
ax[2].set_title('Fares in Pclass 3')
plt.show()
```

<pre>
<Figure size 1440x576 with 3 Axes>
</pre>
- Pclass = 1ì˜ ìŠ¹ê° ìš”ê¸ˆì˜ ë¶„ì‚°ì´ í° ê²ƒìœ¼ë¡œ ë³´ì´ë©°, ê¸°ì¤€ì´ ê°ì†Œí•¨ì— ë”°ë¼ ë¶„ì‚°ì´ ê³„ì†í•´ì„œ ê°ì†Œí•˜ê³  ìˆìŒ

- ì—°ì† ë³€ìˆ˜ì´ê¸°ì—, binningì„ ì‚¬ìš©í•˜ì—¬ ì´ì‚° ê°’ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŒ



## **1-3. ê²½í–¥ì„± íŒŒì•…**


### **ì „ì²´ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ìš”ì•½**

- Sex: ì—¬ì„±ì´ ë‚¨ì„±ì— ë¹„í•´ ìƒì¡´ ê°€ëŠ¥ì„±ì´ ë†’ìŒ

- PClass

  - ë“±ê¸‰ì´ ì¢‹ì„ìˆ˜ë¡ ìƒì¡´ë¥ ì´ ë†’ì•„ì§€ëŠ” ì¶”ì„¸ë¥¼ ë³´ì„

  - PClass = 3ì˜ ìƒì¡´ë¥ ì´ ë§¤ìš° ë‚®ìŒ

  - ì—¬ì„±ì˜ ê²½ìš° PClass = 1ì—ì„œì˜ ìƒì¡´ë¥ ì€ ê±°ì˜ 1ì´ê³ , PClass = 2ì˜ ê²½ìš°ë„ ìƒì¡´ë¥ ì´ ë†’ìŒ

- Age

  - 5-10ì„¸ ë¯¸ë§Œ ì–´ë¦°ì´ë“¤ì˜ ê²½ìš° ìƒì¡´ ê°€ëŠ¥ì„±ì´ ë†’ìŒ

  - 15ì„¸-30ì„¸ ìŠ¹ê°ë“¤ì´ ë§ì´ ì£½ìŒ

- Embarked

  - PClass = 1 ìŠ¹ê° ëŒ€ë¶€ë¶„ì´ Sì—ì„œ íƒ‘ìŠ¹í•˜ì˜€ë‹¤ë§Œ, Cì—ì„œì˜ ìƒì¡´ë¥ ì´ í›¨ì”¬ ì¢‹ìŒ

  - Qì—ì„œ íƒ‘ìŠ¹í•œ ì†ë‹˜ë“¤ì€ ëª¨ë‘ PClass = 3

- Parch + SibSp

  - 1-2ëª…ì˜ í˜•ì œìë§¤ê°€ ìˆê±°ë‚˜, ë°°ìš°ìê°€ ìˆê±°ë‚˜, 1-3ëª…ì˜ ë¶€ëª¨ê°€ ìˆëŠ” ìŠ¹ê°ë“¤ì´ í˜¼ì íƒ€ê±°ë‚˜ ëŒ€ê°€ì¡±ì¸ ê²½ìš°ë³´ë‹¤ ìƒì¡´ë¥ ì´ ë†’ìŒ


### **Featureë“¤ ê°„ì˜ ìƒê´€ê²Œìˆ˜**



```python
sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) # correlation matrix

fig = plt.gcf()
fig.set_size_inches(10,8)

plt.show()
```

<pre>
<Figure size 720x576 with 2 Axes>
</pre>
- ê° featureë“¤ ê°„ì˜ í° ìƒê´€ê´€ê³„ëŠ” ì—†ìŒì„ íŒŒì•…í•  ìˆ˜ ìˆìŒ

- ê°€ì¥ í° ìƒê´€ê´€ê³„

  - SibSpì™€ Parch(0.41)

> ëª¨ë“  featureë¥¼ í™œìš©í•  ìˆ˜ ìˆìŒ


**âœ” Heatmapì— ëŒ€í•œ í•´ì„**

- ì•ŒíŒŒë²³ì´ë‚˜ ë¬¸ìì—´ ì‚¬ì´ì—ì„œ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ìŒ

  - numeric featureë“¤ë§Œ ë¹„êµë¨



- ì–‘ì˜ ìƒê´€ê´€ê³„(positive correlation)

  - feature Aì˜ ì¦ê°€ê°€ feature Bì˜ ì¦ê°€ë¡œ ì´ì–´ì§€ëŠ” ê²½ìš°

  - 1ì€ ì™„ì „í•œ ì–‘ì˜ ìƒê´€ ê´€ê³„ë¥¼ ì˜ë¯¸



- ìŒì˜ ìƒê´€ê´€ê³„(negative correlation)

  - feature Aì˜ ì¦ê°€ê°€ feature Bì˜ ê°ì†Œë¡œ ì´ì–´ì§€ëŠ” ê²½ìš°

  - -1ì€ ì™„ì „í•œ ì–‘ì˜ ìƒê´€ ê´€ê³„ë¥¼ ì˜ë¯¸











**âœ” ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity)**  

- ë‘ íŠ¹ì§•ì´ ë§¤ìš°/ ì™„ë²½í•˜ê²Œ ìƒê´€ë¨

  - í•œ featureì˜ ì¦ê°€ê°€ ë‹¤ë¥¸ featureì˜ ì¦ê°€ë¡œ ì´ì–´ì§

  - ì¦‰, ë‘ feature ëª¨ë‘ ë§¤ìš° ìœ ì‚¬í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©° ì •ë³´ì˜ ì°¨ì´ê°€ ê±°ì˜/ ì „í˜€ ì—†ìŒ

- ë‘ ê°œì˜ featureë“¤ì´ ì¤‘ë³µë˜ê¸°ì—, ë‘˜ ë‹¤ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  í•˜ë‚˜ë§Œ ì‚¬ìš©í•´ë„ ë¬´ë°©í•¨



# **2. íŠ¹ì„± ê³µí•™(Feature Engineering) & ë°ì´í„° í´ë Œì§•**

- ëª¨ë“  featureë“¤ì´ ì¤‘ìš”í•œ ê²ƒì€ ì•„ë‹˜

  - ì œê±°í•´ì•¼ í•  ì¤‘ë³µëœ ì„±ê²©ì˜ featureë“¤ì´ ë§ì´ ìˆì„ ìˆ˜ ìˆìŒ

- ë‹¤ë¥¸ featureë“¤ì„ ê´€ì°°í•˜ê±°ë‚˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ featureë¡œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì¶”ê°€í•  ìˆ˜ ìˆìŒ


## **2-1. ìƒˆë¡œìš´ feature ì¶”ê°€**


### **Age_band**



**âœ” Age featureì˜ ë¬¸ì œì **  

- AgeëŠ” ì—°ì†í˜• ë³€ìˆ˜

  - MLì—ì„œ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆìŒ

- **binning** ë˜ëŠ” **ì •ê·œí™”**ë¥¼ í†µí•´ ë²”ì£¼í˜• ë³€ìˆ˜ë¡œ ë³€í™˜ í•„ìš”

  - binningì„ í™œìš©

  - ì—°ë ¹ ë²”ìœ„ë¥¼ ë‹¨ì¼ ë¹ˆìœ¼ë¡œ groupí™” í•˜ê±°ë‚˜ ë‹¨ì¼ ê°’ í• ë‹¹

  - 0 - 80ì„¸ë¥¼ 5ê°œì˜ binìœ¼ë¡œ ë‚˜ëˆ„ê¸°



```python
### ì—°ë ¹ëŒ€ ë‚˜ëˆ„ê¸°

data['Age_band'] = 0
data.loc[data['Age']<= 16,'Age_band'] = 0
data.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1
data.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2
data.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3
data.loc[data['Age']>64,'Age_band']=4
```


```python
data.head(2)
```

<pre>
   PassengerId  Survived  Pclass  \
0            1         0       3   
1            2         1       1   

                                                Name     Sex   Age  SibSp  \
0                            Braund, Mr. Owen Harris    male  22.0      1   
1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   

   Parch     Ticket     Fare Cabin Embarked Initial  Age_band  
0      0  A/5 21171   7.2500   NaN        S      Mr         1  
1      0   PC 17599  71.2833   C85        C     Mrs         2  
</pre>

```python
### ê° ì—°ë ¹ëŒ€ì— ì†í•˜ëŠ” ìŠ¹ê° ìˆ˜

data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')
```

<pre>
<pandas.io.formats.style.Styler at 0x7f3cc1a556a0>
</pre>

```python
sns.catplot(x = 'Age_band', y = 'Survived',data=data,col='Pclass', kind = 'point')
plt.show()
```

<pre>
<Figure size 1080x360 with 3 Axes>
</pre>
- PClassì™€ ìƒê´€ ì—†ì´ ë‚˜ì´ê°€ ì¦ê°€í• ìˆ˜ë¡ ìƒì¡´ë¥ ì´ ë‚®ì•„ì§


### **Family_Size & Alone**

- Parch + SibSp

- ìƒì¡´ë¥ ì´ ê°€ì¡± êµ¬ì„±ì› ìˆ˜ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸

- ë‹¨ë…ìœ¼ë¡œ ìŠ¹ê°ì´ í˜¼ìì¸ì§€ ì•„ë‹Œì§€ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ



```python
data['Family_Size'] = 0
data['Family_Size'] = data['Parch'] + data['SibSp'] #family size

data['Alone'] = 0
data.loc[data.Family_Size == 0,'Alone'] = 1 #Alone

### ì‹œê°í™”

f,ax = plt.subplots(1,2,figsize = (18,6))
sns.pointplot(x = 'Family_Size',y = 'Survived',data = data, ax = ax[0])
ax[0].set_title('Family_Size vs Survived')
sns.pointplot(x = 'Alone',y = 'Survived',data = data, ax = ax[1])
ax[1].set_title('Alone vs Survived')
plt.close(2)
plt.close(3)
plt.show()
```

<pre>
<Figure size 1296x432 with 2 Axes>
</pre>
- Family_Size = 0: ìŠ¹ê°ì´ **í˜¼ì**ì„ì„ ì˜ë¯¸

- í˜¼ìì´ê±°ë‚˜ family_size = 0ì´ë©´ ìƒì¡´ë¥ ì´ ë§¤ìš° ë‚®ìŒ

- family_size > 4ì¸ ê²½ìš°ë„ ìƒì¡´ë¥  ê°ì†Œ




```python
sns.catplot(x = 'Alone',y = 'Survived',data=data,hue='Sex',col='Pclass', kind = 'point')
plt.show()
```

<pre>
<Figure size 1150.5x360 with 3 Axes>
</pre>
- ê°€ì¡±ì´ ìˆëŠ” ì‚¬ëŒë³´ë‹¤ í˜¼ìì¸ ì—¬ì„±ì˜ í™•ë¥ ì´ ë†’ì€ Pclass = 3ì„ ì œì™¸í•˜ê³ ëŠ” Sex, Pclass êµ¬ë¶„ ì—†ì´ í˜¼ì ìˆëŠ” ê²ƒì´ ìœ„í—˜í•¨



### **Fare_Range**

- Fare ë˜í•œ ì—°ì†í˜• ë³€ìˆ˜ -> ì „ì²˜ë¦¬ í•„ìš”

- ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ ```pandas.qcut()```ì„ í™œìš©



- ```pd.qcut(data, bins)```

  - í†µê³¼í•œ binì˜ ìˆ˜ì— ë”°ë¼ ê°’ì„ ë¶„í• /ë°°ì—´

  - 5ê°œì˜ binì— ëŒ€í•´ ì „ë‹¬ ì‹œ ê°’ì´ 5ê°œì˜ bin ë˜ëŠ” ê°’ ë²”ìœ„ë¡œ ê· ë“±í•˜ê²Œ ë°°ì—´ë¨



```python
data['Fare_Range'] = pd.qcut(data['Fare'], 4)
data.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')
```

<pre>
<pandas.io.formats.style.Styler at 0x7f3cc611ce50>
</pre>
- fare_rangeê°€ ì¦ê°€í•¨ì— ë”°ë¼ ìƒì¡´ë¥ ì´ ì¦ê°€

- Fare_range ê°’ì„ Age_bandì™€ ê°™ì´ ë²”ì£¼í˜• ê°’ìœ¼ë¡œ ë³€ê²½



```python
data['Fare_cat'] = 0
data.loc[data['Fare'] <= 7.91,'Fare_cat'] = 0
data.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454),'Fare_cat'] = 1
data.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31),'Fare_cat'] = 2
data.loc[(data['Fare'] > 31) & (data['Fare'] <= 513),'Fare_cat'] = 3
```


```python
sns.pointplot(x = 'Fare_cat',y = 'Survived',data=data,hue='Sex')
plt.show()
```

<pre>
<Figure size 432x288 with 1 Axes>
</pre>
- Fare_catì´ ì¦ê°€í•¨ì— ë”°ë¼ ìƒì¡´ë¥ ì´ ì¦ê°€í•¨

- Sexì™€ ë”ë¶ˆì–´ ëª¨ë¸ë§ ì‹œ ì¤‘ìš”í•œ featureë¡œ ì˜ˆìƒë¨


## **2-2. feature ë³€í™˜**

- ëª¨ë¸ì— ì í•©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ featureë“¤ì„ ë³€í™˜


### **String -> ìˆ˜ì¹˜í˜•**

- ML ëª¨í˜•ì€ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ë§Œ ì²˜ë¦¬ ê°€ëŠ¥



```python
data['Sex'].replace(['male','female'],[0,1],inplace = True)
data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace = True)
data['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace = True)
```

## **2-3. ë¶ˆí•„ìš”í•œ feature ì œê±°**

- Name: ë²”ì£¼í˜• ë³€ìˆ˜ë¡œ ë³€í™˜ ë¶ˆê°€

- Age: Age_band ë³€ìˆ˜ë¡œ ëŒ€ì²´

- Ticket: ë²”ì£¼í˜• ë³€ìˆ˜ë¡œ ë³€í™˜í•˜ê¸°ì—” ë„ˆë¬´ ë‹¤ì±„ë¡œì›€

- Fare: Fare_cat ë³€ìˆ˜ë¡œ ëŒ€ì²´

- Cabin: ë§ì€ ê²°ì¸¡ì¹˜(NaN), í•œ ìŠ¹ê°ì´ ì—¬ëŸ¬ ê°œì˜ Cabin

- Fare_range: fare_cat ë³€ìˆ˜ë¡œ ëŒ€ì²´

- PassengerId: ë²”ì£¼í˜• ë³€ìˆ˜ë¡œ ë³€í™˜ ë¶ˆê°€



```python
data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis = 1,inplace = True)
```


```python
### ìµœì¢… ë³€ìˆ˜ë“¤ì˜ heatmap

sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})

fig = plt.gcf()
fig.set_size_inches(18,15)
plt.xticks(fontsize = 14)
plt.yticks(fontsize = 14)
plt.show()
```

<pre>
<Figure size 1296x1080 with 2 Axes>
</pre>
- ëª‡ëª‡ ë³€ìˆ˜ë“¤ì´ **ì–‘ì˜ ìƒê´€ê´€ê³„**ë¥¼ ê°€ì§ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

  - SibSp & Family_Size

  - Parch & Family_Size

- ë³€ìˆ˜ë“¤ ê°„ì˜ **ìŒì˜ ìƒê´€ê´€ê³„**ë„ í™•ì¸í•  ìˆ˜ ìˆìŒ

  - Alone & Family_Size


# **3. ì˜ˆì¸¡ì  ëª¨ë¸ë§**

- ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìŠ¹ê°ì˜ ìƒì¡´ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡

- í™œìš© ì•Œê³ ë¦¬ì¦˜ë“¤

  - ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (Support Vector Machine)

  - ë¡œì§€ìŠ¤í‹± íšŒê·€(LogisticRegression)

  - ê²°ì • íŠ¸ë¦¬(Decision Tree)

  - K-ìµœê·¼ì ‘ ì´ì›ƒ(K-Nearest Neighbors)

  -ê°€ìš°ìŠ¤ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(Gauss Naive Bayes)

  - ëœë¤ í¬ë ˆìŠ¤íŠ¸(RandomForest)

  - ë¡œì§€ìŠ¤í‹± íšŒê·€(LogisticRegression)



```python
### MLì— í•„ìš”í•œ ëª¨ë“  library import

from sklearn.linear_model import LogisticRegression 
from sklearn import svm 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.naive_bayes import GaussianNB 
from sklearn.tree import DecisionTreeClassifier 

from sklearn.model_selection import train_test_split 
from sklearn import metrics # accuracy measure
from sklearn.metrics import confusion_matrix # for confusion matrix
```


```python
### ë°ì´í„° ë¶„í• 

train, test = train_test_split(data,test_size = 0.3,random_state = 0,stratify = data['Survived'])

train_X = train[train.columns[1:]]
train_Y = train[train.columns[:1]]
test_X = test[test.columns[1:]]
test_Y = test[test.columns[:1]]

X = data[data.columns[1:]]
Y = data['Survived']
```

## **3-1. ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜**


### **Support Vector Machines**

- C: ë§ˆì§„ ì˜¤ë¥˜ë¥¼ ì–¼ë§ˆë‚˜ í—ˆìš©í•  ê²ƒì¸ê°€

  - í´ìˆ˜ë¡ ë§ˆì§„ì´ ë„“ì–´ì§€ê³  ì˜¤ë¥˜ ì¦ê°€

  - ì‘ì„ìˆ˜ë¡ ë§ˆì§„ì´ ì¢ì•„ì§€ê³  ì˜¤ë¥˜ ê°ì†Œ

- kernel: ì»¤ë„ í•¨ìˆ˜ ì¢…ë¥˜

  - 'linear', 'poly', 'rbf', 'sigmoid'

- gamma: ì»¤ë„ ê³„ìˆ˜ ì§€ì •

  - kernelì´ 'poly', 'rbf', 'sigmoid'ì¼ ë•Œ ìœ íš¨



---

â€» Reference: í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹



```python
### Radial Support Vector Machines(rbf-SVM)

model = svm.SVC(kernel = 'rbf',C = 1,gamma = 0.1) # ëª¨ë¸ ê°ì²´ ìƒì„±

model.fit(train_X,train_Y) # í•™ìŠµ
prediction1 = model.predict(test_X) # ì˜ˆì¸¡
print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y)) # í‰ê°€
```

<pre>
Accuracy for rbf SVM is  0.835820895522388
</pre>

```python
### Linear Support Vector Machine(linear-SVM)

model = svm.SVC(kernel = 'linear',C = 0.1,gamma = 0.1)
model.fit(train_X,train_Y)
prediction2=model.predict(test_X)
print('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))
```

<pre>
Accuracy for linear SVM is 0.8171641791044776
</pre>
### **Logistic Regression**



```python
model = LogisticRegression()
model.fit(train_X,train_Y)
prediction3 = model.predict(test_X)
print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))
```

<pre>
The accuracy of the Logistic Regression is 0.8134328358208955
</pre>
### **ê²°ì • íŠ¸ë¦¬**



```python
model = DecisionTreeClassifier()
model.fit(train_X,train_Y)
prediction4 = model.predict(test_X)
print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))
```

<pre>
The accuracy of the Decision Tree is 0.8059701492537313
</pre>
### **K-Nearest Neighbours(KNN)**




```python
model = KNeighborsClassifier() 
model.fit(train_X,train_Y)
prediction5 = model.predict(test_X)
print('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))
```

<pre>
The accuracy of the KNN is 0.8134328358208955
</pre>

```python
### n_neignbors ê°’ì„ ë³€ê²½í•˜ë©° KNN ëª¨ë¸ì˜ ì •í™•ë„ í™•ì¸í•˜ê¸°

a_index = list(range(1,11))
a = pd.Series() # ì •í™•ë„ê°€ ì €ì¥ë  Series ê°ì²´
x = [0,1,2,3,4,5,6,7,8,9,10]

for i in list(range(1,11)):
    model = KNeighborsClassifier(n_neighbors = i)  
    model.fit(train_X, train_Y)
    prediction = model.predict(test_X)
    a = a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))

plt.plot(a_index, a)
plt.xticks(x)
fig = plt.gcf()
fig.set_size_inches(12,6)
plt.show()

print()
print('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())
```

<pre>
<Figure size 864x432 with 1 Axes>
</pre>
<pre>

Accuracies for different values of n are: [0.73134328 0.76119403 0.79477612 0.80597015 0.81343284 0.80223881
 0.82835821 0.83208955 0.84701493 0.82835821] with the max value as  0.8470149253731343
</pre>
### **Gaussian Naive Bayes**



```python
model = GaussianNB()
model.fit(train_X,train_Y)
prediction6 = model.predict(test_X)
print('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))
```

<pre>
The accuracy of the NaiveBayes is 0.8134328358208955
</pre>
### **Random Forests**



```python
model = RandomForestClassifier(n_estimators = 100)
model.fit(train_X,train_Y)
prediction7 = model.predict(test_X)
print('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))
```

<pre>
The accuracy of the Random Forests is 0.8059701492537313
</pre>
- ëª¨ë¸ì˜ ì •í™•ì„±ì´ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ ê²°ì •í•˜ëŠ” ìœ ì¼í•œ ìš”ì†ŒëŠ” ì•„ë‹˜

  - ë¶„ë¥˜ê¸°ê°€ ìì²´ êµìœ¡ì— ì‚¬ìš©í•  ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê²°ì •í•  ìˆ˜ëŠ” ì—†ìŒ

  - train ë° test ë°ì´í„°ê°€ ë³€ê²½ë¨ì— ë”°ë¼ ì •í™•ë„ë„ ë³€ê²½ë¨

> ëª¨í˜• ë¶„ì‚°(model variance)



- ì¼ë°˜í™”ëœ ëª¨ë¸ì„ ì–»ê¸° ìœ„í•´ **êµì°¨ ê²€ì¦**ì„ í™œìš©


## **3-2. êµì°¨ ê²€ì¦(Cross Validation)**

- ë§ì€ ê²½ìš° ë°ì´í„°ê°€ ë¶ˆê· í˜•í•¨

  - ìµœëŒ€í•œ ë°ì´í„° ì„¸íŠ¸ì˜ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì•Œê³ ë¦¬ì¦˜ì„ trainí•˜ê³  test í•´ì•¼ í•¨

  - ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ì•Œë ¤ì§„ ëª¨ë“  ì •í™•ë„ì˜ í‰ê· ì„ ì–»ê¸°









### **K-Fold êµì°¨ ê²€ì¦**  

- ë¨¼ì € ë°ì´í„° ì„¸íŠ¸ë¥¼ k-ë¶€ë¶„ ì§‘í•©ìœ¼ë¡œ ë‚˜ëˆ„ê¸°

- ë°ì´í„° ì„¸íŠ¸ë¥¼ (k = 5) ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆˆë‹¤ê³  ê°€ì •í•˜ë©´ testë¥¼ ìœ„í•´ 1ê°œì˜ ë¶€ë¶„ì„ ì •í•˜ê³  4ê°œì˜ ë¶€ë¶„ì— ê±¸ì³ ì•Œê³ ë¦¬ì¦˜ì„ train

- ê° ë°˜ë³µì—ì„œ test ë¶€ë¶„ì„ ë³€ê²½í•˜ê³  ë‹¤ë¥¸ ë¶€ë¶„ì— ëŒ€í•´ ì•Œê³ ë¦¬ì¦˜ì„ í›ˆë ¨

  - ì´í›„ ê° ë°˜ë³µë§ˆë‹¤ ì–»ì€ ì •í™•ë„ì™€ ì˜¤ì°¨ë¥¼ í‰ê· 

- ì•Œê³ ë¦¬ì¦˜ì´ ì¼ë¶€ train ë°ì´í„°ì— ì í•©í•˜ì§€ ì•Šê±°ë‚˜(underfitting) ì§€ë‚˜ì¹˜ê²Œ ì í•©ë˜ëŠ” ê²ƒ(overfitting) ë°©ì§€



```python
### import libraries
from sklearn.model_selection import KFold 
from sklearn.model_selection import cross_val_score # score evaluation
from sklearn.model_selection import cross_val_predict # prediction

kfold = KFold(n_splits = 10, random_state = 22, shuffle = True) # k=10, split the data into 10 equal parts
xyz = []
accuracy = []
std = []
classifiers = ['Linear Svm','Radial Svm','Logistic Regression','KNN',
               'Decision Tree','Naive Bayes','Random Forest']

models = [svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),
          KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),
          RandomForestClassifier(n_estimators=100)]
for i in models:
    model = i
    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = "accuracy") # êµì°¨ ê²€ì¦ ìˆ˜í–‰
    cv_result = cv_result
    xyz.append(cv_result.mean())
    std.append(cv_result.std())
    accuracy.append(cv_result)

new_models_dataframe2 = pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       
new_models_dataframe2
```

<pre>
                      CV Mean       Std
Linear Svm           0.784607  0.057841
Radial Svm           0.828377  0.057096
Logistic Regression  0.799176  0.040154
KNN                  0.808140  0.035630
Decision Tree        0.805855  0.042848
Naive Bayes          0.795843  0.054861
Random Forest        0.811486  0.049518
</pre>

```python
### ì‹œê°í™”

plt.subplots(figsize = (12,6))
box = pd.DataFrame(accuracy,index = [classifiers])
box.T.boxplot()
```

<pre>
<Axes: >
</pre>
<pre>
<Figure size 864x432 with 1 Axes>
</pre>

```python
new_models_dataframe2['CV Mean'].plot.barh(width = 0.8)

plt.title('Average CV Mean Accuracy')
fig = plt.gcf()
fig.set_size_inches(8,5)
plt.show()
```

<pre>
<Figure size 576x360 with 1 Axes>
</pre>
- ë¶„ë¥˜ ì •í™•ë„ëŠ” ë¶ˆê· í˜•ìœ¼ë¡œ ì¸í•´ ë•Œë•Œë¡œ ì˜¤í•´ì˜ ì†Œì§€ê°€ ìˆì„ ìˆ˜ ìˆìŒ

- ëª¨ë¸ì´ ì–´ë””ì„œ ì˜ëª»ë˜ì—ˆëŠ”ì§€, ë˜ëŠ” ëª¨ë¸ì´ ì–´ë–¤ í´ë˜ìŠ¤ë¥¼ ì˜ëª» ì˜ˆì¸¡í–ˆëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” **ì˜¤ì°¨ í–‰ë ¬**(confusion matrix)ì˜ ë„ì›€ìœ¼ë¡œ ìš”ì•½ëœ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ


### **ì˜¤ì°¨ í–‰ë ¬** 

- ë¶„ë¥˜ê¸°ì— ì˜í•´ ë§Œë“¤ì–´ì§„ ì •í™•í•œ ë¶„ë¥˜ì™€ ì˜ëª»ëœ ë¶„ë¥˜ì˜ ìˆ˜ë¥¼ ì œê³µ



```python
f,ax = plt.subplots(3,3,figsize=(12,10))

y_pred = cross_val_predict(svm.SVC(kernel = 'rbf'),X,Y,cv = 10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')
ax[0,0].set_title('Matrix for rbf-SVM')

y_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')
ax[0,1].set_title('Matrix for Linear-SVM')

y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')
ax[0,2].set_title('Matrix for KNN')

y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')
ax[1,0].set_title('Matrix for Random-Forests')

y_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')
ax[1,1].set_title('Matrix for Logistic Regression')

y_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')
ax[1,2].set_title('Matrix for Decision Tree')

y_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')
ax[2,0].set_title('Matrix for Naive Bayes')

plt.subplots_adjust(hspace=0.2,wspace=0.2)
plt.show()
```

<pre>
<Figure size 864x720 with 16 Axes>
</pre>
**âœ” ì˜¤ì°¨ í–‰ë ¬ í•´ì„**  

- ì™¼ìª½ ëŒ€ê°ì„ : ê° í´ë˜ìŠ¤ì— ëŒ€í•´ ìˆ˜í–‰ëœ **ì •í™•í•œ** ì˜ˆì¸¡ì˜ ìˆ˜/ ì˜¤ë¥¸ìª½ ëŒ€ê°ì„ : **ì˜ëª»ëœ** ì˜ˆì¸¡ì˜ ìˆ˜ë¥¼ í‘œì‹œ



-  rbf-SVMì— ëŒ€í•œ ì²« ë²ˆì§¸ ê·¸ë¦¼ í•´ì„  

  - ì •í™•í•œ ì˜ˆì¸¡ì˜ ìˆ˜ëŠ” 491(ì‚¬ë§ìì˜ ê²½ìš°) + 247(ìƒì¡´ìì˜ ê²½ìš°)ì´ë©° í‰ê·  CV ì •í™•ë„ëŠ” (491+247)/891 = 82.8%

  - ì˜¤ë¥˜--> 58ëª…ì˜ ì‚¬ë§ìë¥¼ ìƒì¡´ìë¡œ ì˜ëª» ë¶„ë¥˜í•˜ê³  95ëª…ì€ ì‚¬ë§ìë¥¼ ìƒì¡´ìë¡œ ì˜ëª» ë¶„ë¥˜

  - ëª¨ë“  í–‰ë ¬ì„ ì‚´í´ë³¸ í›„, ìš°ë¦¬ëŠ” rbf-SVMì´ ì‚¬ë§í•œ ìŠ¹ê°ì„ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•  ê°€ëŠ¥ì„±ì´ ë” ë†’ì§€ë§Œ, ë‚˜ì´ë¸Œ ë² ì´ì¦ˆëŠ” ìƒì¡´í•œ ìŠ¹ê°ì„ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•  ê°€ëŠ¥ì„±ì´ ë” ë†’ë‹¤ê³  ë§í•  ìˆ˜ ìˆìŒ


**âœ” í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹**  

- í•˜ì´í¼ íŒŒë¼ë¯¸í„°: ì„œë¡œ ë‹¤ë¥¸ ë¶„ë¥˜ê¸°ì— ëŒ€í•œ ì„œë¡œ ë‹¤ë¥¸ ë§¤ê°œ ë³€ìˆ˜

- ì´ë¥¼ ì¡°ì •í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì˜ í•™ìŠµ ì†ë„ ë³€ê²½ ë“± ë” ë‚˜ì€ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆìŒ => íŠœë‹




```python
### SVM í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹

from sklearn.model_selection import GridSearchCV

C = [0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]
gamma = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]
kernel = ['rbf','linear']
hyper = {'kernel':kernel,'C':C,'gamma':gamma}

# í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹
gd = GridSearchCV(estimator = svm.SVC(),param_grid = hyper,verbose = True)
gd.fit(X,Y)
print(gd.best_score_)
print(gd.best_estimator_)
```

<pre>
Fitting 5 folds for each of 240 candidates, totalling 1200 fits
0.8282593685267716
SVC(C=0.4, gamma=0.3)
</pre>
- C = 0.4, gamma = 0.3ì¼ ë•Œ ì •í™•ë„ê°€ 82.82%ë¡œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥



```python
### RandomForest í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹

n_estimators = range(100,1000,100)
hyper = {'n_estimators':n_estimators}

gd = GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)
gd.fit(X,Y)
print(gd.best_score_)
print(gd.best_estimator_)
```

<pre>
Fitting 5 folds for each of 9 candidates, totalling 45 fits
0.819327098110602
RandomForestClassifier(n_estimators=300, random_state=0)
</pre>
- n_estimators = 300ì¼ ë•Œ ì •í™•ë„ê°€ 81.9% ì •ë„ë¡œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥


## **3-3. ì•™ìƒë¸”(Ensembling)**

- ë‹¤ì–‘í•œ ë‹¨ìˆœí•œ ëª¨ë¸ë“¤ì´ ê²°í•©í•˜ì—¬ **í•˜ë‚˜**ì˜ ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒ

  - ëª¨ë¸ì˜ ì •í™•ë„ë‚˜ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ì¢‹ì€ ë°©ë²•

- ë°©ë²•

  - Voting Classifier

  - Bagging

  - Boosting


### **VotingClassifier**

- ë‹¤ì–‘í•œ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•

- ëª¨ë“  í•˜ìœ„ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê¸°ë°˜ìœ¼ë¡œ **í‰ê· ** ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œê³µ

  - ì´ë•Œ í•˜ìœ„ ëª¨ë¸ ë˜ëŠ” ê¸°ë³¸ ëª¨ë¸ì€ ëª¨ë‘ **ë‹¤ë¥¸** ìœ í˜•



```python
from sklearn.ensemble import VotingClassifier

ensemble_lin_rbf = VotingClassifier(estimators = [('KNN',KNeighborsClassifier(n_neighbors=10)),
                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),
                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),
                                              ('LR',LogisticRegression(C=0.05)),
                                              ('DT',DecisionTreeClassifier(random_state=0)),
                                              ('NB',GaussianNB()),
                                              ('svm',svm.SVC(kernel='linear',probability=True))
                                             ], 
                       voting = 'soft').fit(train_X,train_Y) # soft voting
print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))
cross = cross_val_score(ensemble_lin_rbf,X,Y, cv = 10, scoring = "accuracy")
print('The cross validated score is',cross.mean())
```

<pre>
The accuracy for ensembled model is: 0.8171641791044776
The cross validated score is 0.8249188514357053
</pre>
### **Bagging**

- ì¼ë°˜ì ì¸ ì•™ìƒë¸” ë°©ì‹

- ë°ì´í„° ì„¸íŠ¸ì˜ ì‘ì€ íŒŒí‹°ì…˜(ì¼ë¶€ë¶„)ì— **ìœ ì‚¬í•œ** ë¶„ë¥˜ê¸°ë¥¼ ì ìš©í•œ ë‹¤ìŒ ëª¨ë“  ì˜ˆì¸¡ì˜ í‰ê· ì„ ì·¨í•¨ìœ¼ë¡œì¨ ì‘ë™

- í‰ê· í™”ë¡œ ì¸í•´ ë¶„ì‚°ì´ ê°ì†Œë¨



**âœ” Baged KNN**  

- ë°°ê¹…ì€ ë¶„ì‚°ì´ **ë†’ì€** ëª¨í˜•ì—ì„œ ê°€ì¥ ì˜ ì‘ë™

ex> ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬, ëœë¤ í¬ë ˆìŠ¤íŠ¸

- n_neighborsë¡œ ì‘ì€ ê°’ì„ ê°€ì§€ëŠ” KNNê³¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ



```python
from sklearn.ensemble import BaggingClassifier

model = BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)
model.fit(train_X,train_Y)
prediction=model.predict(test_X)
print('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))

result = cross_val_score(model,X,Y,cv = 10,scoring='accuracy')
print('The cross validated score for bagged KNN is:',result.mean())
```

<pre>
The accuracy for bagged KNN is: 0.832089552238806
The cross validated score for bagged KNN is: 0.8104244694132333
</pre>
**âœ” Bagged DecisionTree**  




```python
model = BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)
model.fit(train_X,train_Y)
prediction = model.predict(test_X)
print('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))

result = cross_val_score(model,X,Y,cv = 10,scoring = 'accuracy')
print('The cross validated score for bagged Decision Tree is:',result.mean())
```

<pre>
The accuracy for bagged Decision Tree is: 0.8208955223880597
The cross validated score for bagged Decision Tree is: 0.8171410736579275
</pre>
### **Boosting**

- ë¶„ë¥˜ê¸°ì˜ **ìˆœì°¨ í•™ìŠµ**ì„ ì‚¬ìš©í•˜ëŠ” ì•™ìƒë¸” ê¸°ìˆ 

  - ì•½í•œ ëª¨ë¸ì„ ë‹¨ê³„ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒ

-  ë¨¼ì € ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ í›ˆë ¨

  - ì¼ë¶€ ì¸ìŠ¤í„´ìŠ¤ëŠ” ë§ì§€ë§Œ ì¼ë¶€ ì¸ìŠ¤í„´ìŠ¤ëŠ” í‹€ë¦¼

- ì´í›„ ë‹¤ìŒ ë°˜ë³µì—ì„œ ì˜ëª» ì˜ˆì¸¡ëœ ì‚¬ë¡€ì— ë° ì§‘ì¤‘í•˜ê±°ë‚˜ ë¹„ì¤‘ì„ ë‘ì–´ ì˜ëª»ëœ ì˜ˆì¸¡ì„ ë°”ë¡œì¡ìœ¼ë ¤ ë…¸ë ¥

  - ì •í™•ë„ê°€ í•œê³„ì— ë‹¤ë‹¤ë¥¼ ë•Œê¹Œì§€ ìƒˆë¡œìš´ ë¶„ë¥˜ê¸°ë¥¼ ì¶”ê°€ì‹œí‚¤ë©° í•™ìŠµ


**âœ” AdaBoost(Adaptive Boosting)**  

- ì•½í•œ ë¶„ë¥˜ê¸°: ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬

  - ```base_estimator``` ì˜µì…˜ì—ì„œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥



```python
from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(n_estimators = 200,random_state = 0,learning_rate = 0.1)
result = cross_val_score(ada,X,Y,cv = 10,scoring = 'accuracy')
print('The cross validated score for AdaBoost is:',result.mean())
```

<pre>
The cross validated score for AdaBoost is: 0.8249188514357055
</pre>
**âœ” Stochastic Gradient Boosting**  

- ì•½í•œ ë¶„ë¥˜ê¸°: ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬



```python
from sklearn.ensemble import GradientBoostingClassifier

grad = GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)
result = cross_val_score(grad,X,Y,cv=10,scoring='accuracy')
print('The cross validated score for Gradient Boosting is:',result.mean())
```

<pre>
The cross validated score for Gradient Boosting is: 0.8115230961298376
</pre>
**âœ” XGBoost** 



```python
import xgboost as xg

xgboost = xg.XGBClassifier(n_estimators=900,learning_rate=0.1)
result = cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')
print('The cross validated score for XGBoost is:',result.mean())
```

<pre>
The cross validated score for XGBoost is: 0.8160299625468165
</pre>
- AdaBoostì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥




```python
### AdaBoost - hyper parameter tuning

n_estimators = list(range(100,1100,100))
learn_rate = [0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]
hyper = {'n_estimators':n_estimators,'learning_rate':learn_rate}

gd = GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)
gd.fit(X,Y)

print(gd.best_score_)
print(gd.best_estimator_)
```

<pre>
Fitting 5 folds for each of 120 candidates, totalling 600 fits
0.8293892411022534
AdaBoostClassifier(learning_rate=0.1, n_estimators=100)
</pre>
- learning_rate = 0.1, n_estimators = 100ì¼ ë•Œ ì •í™•ë„ê°€ 82.94% ì •ë„ë¡œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„


### **ìµœì  ëª¨í˜•ì— ëŒ€í•œ ì˜¤ì°¨ í–‰ë ¬**



```python
ada = AdaBoostClassifier(n_estimators=100,random_state=0,learning_rate=0.1)
result = cross_val_predict(ada,X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,result),cmap = 'winter',annot=True,fmt='2.0f')
plt.show()
```

<pre>
<Figure size 432x288 with 2 Axes>
</pre>
## **3-4. Feature ì¤‘ìš”ë„**



```python
f,ax = plt.subplots(2,2,figsize=(15,12))

### RandomForest
model = RandomForestClassifier(n_estimators=500,random_state=0) 
model.fit(X,Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])
ax[0,0].set_title('Feature Importance in Random Forests')

### AdaBoost
model=AdaBoostClassifier(n_estimators=100,learning_rate=0.1,random_state=0)
model.fit(X,Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')
ax[0,1].set_title('Feature Importance in AdaBoost')

### Gradient Boosting
model=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)
model.fit(X,Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')
ax[1,0].set_title('Feature Importance in Gradient Boosting')

### XGBoost
model=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)
model.fit(X,Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')
ax[1,1].set_title('Feature Importance in XgBoost')

plt.show()
```

<pre>
<Figure size 1080x864 with 4 Axes>
</pre>
**âœ” Observations**  

- ê³µí†µì ìœ¼ë¡œ ì¤‘ìš”í•˜ë‹¤ê³  ë‚˜íƒ€ë‚˜ëŠ” featureë“¤: Initial, Fare_cat, Pclass, Family_Size

- Sex ê¸°ëŠ¥ì€ ì¤‘ìš”í•˜ì§€ ì•Šì€ featureë¡œ ë³´ì„

  - RandomForestì—ì„œë§Œ Sexê°€ ì¤‘ìš”í•´ ë³´ì„

  - ê·¸ëŸ¬ë‚˜ ë§ì€ ë¶„ë¥˜ê¸°ì—ì„œ ë§¨ ìœ„ì— Initial featureê°€ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ë‘˜ ë‹¤ ì„±ë³„ì„ ì–¸ê¸‰


