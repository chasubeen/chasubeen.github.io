---
layout: single
title:  "[ECC DS 3주차] 개념 정리_분류 모델"
categories: ML
tags: [ECC, DS, Home Credit Default Risk] 
author_profile: false
---

<span style="background-color:#FFE6E6"> 📢 해당 포스트는 [ECC DS 3주차] 1. Start Here: A Gentle Introduction에 대한 추가적인 개념정리입니다. </span>  
[캐글 노트북 필사](https://chasubeen.github.io/ml/Home-Credit_Step1/)

# **1. 로지스틱 회귀(LogisticRegression)**
- 영국의 통계학자인 D.R.Cox가 1958년에 제안한 확률 모델
- 독립 변수의 **선형 결합**을 이용하여 사건의 발생 가능성을 예측하는 데 사용되는 통계 기법
  - 선형 회귀 방식을 **분류**에 적용한 알고리즘
  - 시그모이드 함수의 최적선을 찾고 해당 시그모이드 함수의 반환값을 **확률**로 간주해 확률에 따라 분류를 결정
  - cf> 시그모이드 함수
    - $$y=\frac{1}{1+{e}^{-x}}$$
    - 항상 0<y<1
    - x값이 커지면 1에 근사하며 x값이 작아지면 0에 근사함
    <img src = "https://user-images.githubusercontent.com/98953721/229339596-8f1884ff-f3c5-43cb-ad63-b3e7eecfa05e.png" width = 500 height = 250>

- **이진 분류**만 가능(2개의 클래스 판별만 가능)
  - 3개 이상의 클래스에 대한 판별을 진행하는 경우 다음과 같은 방법을 택함
    - one-vs-rest(OvR): K개의 클래스가 존재할 때, 1개의 클래스를 제외한 다른 클래스를 K개 만들어, 각각의 이진 분류에 대한 확률을 구하고, 총합을 통해 최종 클래스를 판별
    - one-vs-one(OvO) : 4개의 계절을 구분하는 클래스가 존재한다고 가정했을 때, Ovs1, Ovs2, Ovs3, ... , 2vs3까지 NX(N-1)/2개의 분류기를 만들어 가장 많이 양성으로 선택된 클래스를 판별
## **📌 주요 하이퍼 파라미터**
1. ```solver```
  - 최적화 문제에 사용할 알고리즘
  - {lbfgs, liblinear, newton-cg, newton-cholesky, sag, saga}, default = lbfgs
2. ```penalty```
  - 규제의 유형을 설정
  - {l2: L2 규제, l1: L1 규제, elastic: L1 규제 + L2 규제}, default = l2
3. ```C```
  - 규제 강도를 조절하는 **alpha** 값의 역수(C = 1/alpha)
  - C가 작을수록 규제 강도가 커짐 

# **2. 랜덤포레스트(Random Forest)**

## **2-1. 모델 앙상블(Model Ensemble)**
- 여러 개의 머신러닝 모델을 이용해 **최적**의 답을 찾아내는 기법
- 종류
  - ```보팅(Voting)```: 투표를 통해 결과를 도출
  - ```배깅(Bagging)```: 샘플 중복 생성을 통해 결과를 도출
  - ```부스팅(Boosting)```: 이전 오차를 보완하면서 가중치를 부여
  - ```스태킹(Stacking)```: 여러 모델을 기반으로 예측된 결과를 통해 meta 모델이 다시 한번 예측
- 단점>  
  - 하이퍼 파라미터의 수가 너무 많고, 따라서 튜닝을 위한 시간이 많이 소모됨
  - 튜닝 후 예측 성능이 크게 향상되는 경우가 많지는 않음   

## **2-2. 랜덤 포레스트(Random Forest)**
- 여러 개의 결정 트리 분류기가 전체 데이터에서 **배깅** 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤, 최종적으로 모든 분류기가 **보팅**을 통해 예측을 결정하는 방식
  - 개별적인 분류기의 기반 알고리즘은 **결정 트리**지만, 개별 트리가 학습하는 데이터 세트는 전체 데이터에서 일부가 중첩되게 샘플링되는 **부트스트래핑 분할** 방식
  
  <img src = "https://user-images.githubusercontent.com/98953721/229340649-f0264a6c-6147-4cde-a273-c3c4e36a4ae3.png" width = 500 height = 400>
  
  <img src = "https://user-images.githubusercontent.com/98953721/229340707-3f8a0696-7918-4ecc-a31a-8496928f9d94.png" width = 500 height = 200>
  
  - 배깅(Bagging)
    - Bootstrap Aggregating의 줄임말
    - 여러 개의 데이터셋을 중첩을 허용하여 샘플링 후 분할하는 방식
    - ex> 데이터 셋의 구성이 [1,2,3,4,5]라면..
    ```
    group1 = [1,2,3]
    group2 = [1,3,4]
    group3 = [2,3,5]
    ```

- 장점>
  - 굉장히 인기 있는 앙상블 모델
  - 사용성이 쉽고, 성능도 우수함

### **📌 주요 하이퍼 파라미터**    
1. ```n_estimators```
  - 결정 트리의 개수를 지정(default = 10)
  - 많이 성장할수록 좋은 성능을 기대할 수 있지만, 계속 증가시킨다고 성능이 무조건 향상되는 것은 아님
    - 오히려 학습 수행 시간이 오래 걸리는 문제를 발생
2. ```max_features```
  - 결정 트리의 max_features 파라미터와 동일한 기능이다만 default 값이 다름(default: 'sqrt')
    - 랜덤 포레스트의 트리를 분할하는 피처를 참조할 때, 전체 피처가 아닌 sqrt(피처)만큼 참조
3. ```max_depth```, ```min_samples_leaf```, ```min_samples_split```
  - 결정 트리에서 과적합을 개선하기 위해 사용되는 파라미터들
  - 랜덤 포레스트에서도 **동일**하게 적용됨

# **3. LGBM(Light Gradient Boosting Machine)**
- 부스팅(Boosting)
  - 약한 학습기를 순차적으로 학습하되, 이전 학습에 대하여 잘못 예측된 데이터에 가중치를 부여해 **오차를 보완**해 나가는 방식
  - 장점: 성능이 매우 우수(특히 LGBM, XGBoost)
  - 단점
    - 부스팅 알고리즘의 특성상 계속 약점(오분류/잔차)을 보완하려 함 -> 잘못된 레이블링이나 아웃라이어에 필요 이상으로 민감할 수 있음
    - 다른 앙상블 기법 대비 학습 시간이 오래 걸림
- LGBM의 XGBoost 대비 장점
  - 빠른 학습과 예측 수행 시간, 더 작은 메모리 사용량
  - XGBoost와 예측 성능은 비슷하지만 더 **다양한** 기능
  - 범주형 변수를 자동으로 변환, 최적 분할
  - 리프 중심 트리 분할(leaf wise/ 일반적인 트리 분할 방법처럼 트리의 균형을 맞춰서 분할) 대신 **최대 손실 값(max delta loss)**을 가지는 리프 노드를 지속적으로 분할하며 **비대칭적인** 규칙 트리 생성
<img src = "https://user-images.githubusercontent.com/98953721/229343247-448d2fcd-d81c-4841-b85c-8a5fe1e84511.png" width = 600 height = 200>

## **📌 주요 파라미터**
1. ```num_iterations```: 반복 수행하려는 트리의 개수를 지정(dedault = 100)
2. ```learning_rate```: 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값(dedault = 0.1)
3. ```max_depth```: 트리 기반 알고리즘의 max_depth(dedault = -1)
4. ```min_data_in_leaf```: 결정 트리의 min_samples_leaf와 같은 파라미터(dedault = 20)
5. ```num_leaves```: 하나의 트리가 가질 수 있는 최대 리프 개수(dedault = 31)
6. ```boosting```: 부스팅의 트리를 생성하는 알고리즘(dedault = gbdt)
   - gbdt : 일반적인 그래디언트 부스팅 결정 트리
   - rf : 랜덤 포레스트
7. ```bagging_fraction```: 과적합을 제어하기 위해서 데이터를 샘플링하는 비율(dedault = 1.0)
8. ```feature_fraction```: 개별 트리를 학습할 때마다 무작위로 선택하는 feature의 비율(dedault = 1.0)
9. ```lambda_l2```: L2 regulation 제어를 위한 값(dedault = 0.0)
10. ```lambda_l1```: L1 regulation 제어를 위한 값(dedault = 0.0)

## **📌 하이퍼 파라미터 튜닝 방안**
- ```num_leaves```의 개수를 높이면 정확도가 높아지지만, 반대로 트리의 깊이가 깊어지고 모델이 복잡도가 커져 과적합 영향도가 커짐
- ```min_data_in_leaf```는 ```num_leaves```와 학습 데이터의 크기에 따라 달라지지만, 보통 큰 값으로 설정하면 트리가 깊어지는 것을 방지함
- ```max_depth```는 깊이의 크기를 제한






