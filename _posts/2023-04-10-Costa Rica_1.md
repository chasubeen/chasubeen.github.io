---
layout: single
title:  "[ECC DS 4ì£¼ì°¨] 1. A Complete Introduction Walkthrough"
categories: ML
tags: [ECC, DS, Costa Rican Household Poverty Level Prediction] 
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>

<span style="background-color:#FFE6E6"> ğŸ“¢ ì»´í“¨í„° ë¦¬ì†ŒìŠ¤ ë¬¸ì œì™€ ì—ëŸ¬ í•´ê²°ì„ í•˜ì§€ ëª»í•´ ì‹¤í–‰ ê²°ê³¼ë¥¼ ëª¨ë‘ ì§€ìš´ ìƒíƒœì…ë‹ˆë‹¤. ê° ì…€ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•˜ë ¤ë©´ ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”. </span> 

- [Kaggle Kernel_ì›ë³¸](https://www.kaggle.com/code/willkoehrsen/a-complete-introduction-and-walkthrough/notebook)

# **0. ëŒ€íšŒ ì†Œê°œ**

- [Costa Rican Household Poverty Level Prediction](https://www.kaggle.com/competitions/costa-rican-household-poverty-prediction)

- **ğŸ“Œ ëª©ì **  

  - ê°œì¸ê³¼ ê°€êµ¬ íŠ¹ì„±ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ **ê°€êµ¬ì˜ ë¹ˆê³¤ ìˆ˜ì¤€ì„ ì˜ˆì¸¡**í•  ìˆ˜ ìˆëŠ” ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ê²ƒ



- **ğŸ“Œ ë…¸íŠ¸ë¶ ê°œìš”**

  - ë¬¸ì œ ì •ì˜

  - ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ EDA

  - ì—¬ëŸ¬ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ í…ŒìŠ¤íŠ¸/ì„ íƒ/ìµœì í™”

  - ëª¨ë¸ì˜ ì¶œë ¥ ê²€ì‚¬, ê²°ë¡  ë„ì¶œ

  - [feature engineering ìë™í™”](https://www.kaggle.com/willkoehrsen/featuretools-for-good)





## **0-1. ë°ì´í„° ì„¤ëª…**



- ë°ì´í„°ëŠ” train.csvì™€ test.csv ë‘ íŒŒì¼ë¡œ ì œê³µë¨

  - train ì„¸íŠ¸: 9557ê°œì˜ í–‰(row) * 143ê°œì˜ ì—´(column)

  - test ì„¸íŠ¸: 23856ê°œì˜ í–‰(row) * 142ê°œì˜ ì—´(column)

- ê° í–‰ì€ ```í•œ ëª…ì˜ ê°œì¸ ë˜ëŠ” í•œ ê°€êµ¬```ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê° ì—´ì€ ì´ë“¤ì˜ ê³ ìœ í•œ ```íŠ¹ì„±```ì„ ë‚˜íƒ€ëƒ„

---

- Target êµ¬ì„±(í´ë˜ìŠ¤ êµ¬ì„±)  

  - 4ê°€ì§€ ë¹ˆê³¤ ìˆ˜ì¤€

  ```

  1 = ê·¹ì‹¬í•œ ë¹ˆê³¤ ê°€êµ¬

  2 = ì ë‹¹í•œ ë¹ˆê³¤ ê°€êµ¬

  3 = ì·¨ì•½ ê°€êµ¬

  4 = ë¹„ì·¨ì•½ê°€êµ¬

  ```

- Columns

  - ì´ 143ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ êµ¬ì„±ë¨

  - [ì „ì²´ ì»¬ëŸ¼ ì„¤ëª…](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data)

  - ```ID```: ê° ê°œì¸ì˜ ê³ ìœ  ì‹ë³„ì -> í™œìš© x

  - ```idhogar```: ê° ê°€êµ¬ì˜ ê³ ìœ  ì‹ë³„ì -> ê°€êµ¬ë³„ë¡œ ê°œì¸ì„ ê·¸ë£¹í™”í•˜ëŠ” ë° ì‚¬ìš©

  - ```parentesco1```: ê°€ì¥ì¸ì§€ ì—¬ë¶€(ì¼ì¢…ì˜ flag ë³€ìˆ˜)


## **0-2. ëª©í‘œ**

- ê°€êµ¬ ìˆ˜ì¤€ì—ì„œ ë¹ˆê³¤ ì •ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ

  - ê°œì¸ ìˆ˜ì¤€ì— ëŒ€í•œ ë°ì´í„°ê°€ ì œê³µë˜ë©°, ê° ê°œì¸ì€ ê³ ìœ í•œ íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ê°€êµ¬ì— ëŒ€í•œ ì •ë³´ë„ ê°€ì§€ê³  ìˆìŒ

- ë°ì´í„° ì„¸íŠ¸ë¥¼ ê°€ê³µí•˜ê¸° ìœ„í•´ ê° ê°€ì •ì˜ ê°œë³„ ë°ì´í„° ì§‘ê³„ê°€ ìš”êµ¬ë¨

- test setì˜ ëª¨ë“  ê°œì¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰

  - **"ONLY the heads of household are used in scoring"**  

  - ê°€êµ¬ ë‹¨ìœ„ë¡œ ë¹ˆê³¤ì„ ì˜ˆì¸¡  



**â­ ì¤‘ìš”**  

- í•œ ê°€êµ¬ì˜ ëª¨ë“  êµ¬ì„±ì›ì´ train ë°ì´í„°ì—ì„œ ë™ì¼í•œ ë ˆì´ë¸”ì„ ê°€ì ¸ì•¼ í•¨  

  - ë§Œì•½ ë‹¤ë¥¸ ê²½ìš° ```parentesco1 == 1.0``` í–‰ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ê° ê°€êµ¬ì˜ ê°€ì¥ì— ëŒ€í•œ ë¼ë²¨ì„ ì‚¬ìš©

---

- ëª¨ë¸ í›ˆë ¨ ì‹œ **ê°€ì¥ì˜ ë¹ˆê³¤ ìˆ˜ì¤€**ì´ë¼ëŠ” ë¼ë²¨ì„ ê° ê°€êµ¬ì— ë¶™ì—¬ì„œ ê°€ì • ë‹¨ìœ„ë¡œ í›ˆë ¨ì‹œí‚¬ ì˜ˆì •

  - ì›ë³¸ ë°ì´í„°ì—ëŠ” ê°€êµ¬ ë° ê°œì¸ì˜ íŠ¹ì„±ì´ í˜¼í•©ë˜ì–´ ìˆìŒ â‡€ ê°œë³„ ë°ì´í„°ì˜ ê²½ìš° ê° ê°€êµ¬ì— ëŒ€í•´ ì´ë¥¼ ì§‘ê³„í•˜ëŠ” ë°©ë²•ì„ ì°¾ì•„ì•¼ í•¨

  - ê°œì¸ ì¤‘ ì¼ë¶€ëŠ” ê°€ì¥ì´ ì—†ëŠ” ê°€êµ¬ì— ì†í•¨ â‡€ í›ˆë ¨ ë°ì´í„°ë¡œ ì‚¬ìš© ë¶ˆê°€

  


## **0-3. í‰ê°€ ì§€í‘œ(metric) - Macro F1 Score**

- ê¶ê·¹ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ê°€êµ¬ì˜ ë¹ˆê³¤ ìˆ˜ì¤€(**ì •ìˆ˜**ë¡œ êµ¬ë¶„ë¨)ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ ì í•¨

- í‰ê°€ ì§€í‘œë¡œ ```Macro F1 Score```ì„ í™œìš©í•  ì˜ˆì •

  - ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· 



- **í‘œì¤€ F1 ì ìˆ˜**

  - [Reference](http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)

$$F_1 = \frac{2}{\tfrac{1}{\mathrm{ì¬í˜„ìœ¨}} + \tfrac{1}{\mathrm{ì •í™•ë„}}} = 2 \cdot \frac{\mathrm{ì •í™•ë„} \cdot \mathrm{ì¬í˜„ìœ¨}}{\mathrm{ì •í™•ë„} + \mathrm{ì¬í˜„ìœ¨}}$$

  - ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° ê° í´ë˜ìŠ¤ ë³„ F1 scoreì„ **í‰ê· **ë‚´ì–´ í™œìš©



- **Macro F1 score**

  - labelì˜ ë¶ˆê· í˜•ì„ ê³ ë ¤í•˜ì§€ ì•Šê³  ê° í´ë˜ìŠ¤ì˜ F1 ì ìˆ˜ë¥¼ í‰ê· 

    - ê° ë ˆì´ë¸”ì˜ ë°œìƒ ë¹ˆë„ëŠ” ë§¤í¬ë¡œë¥¼ ì‚¬ìš©í•  ë•Œ ê³„ì‚°ì— ë°˜ì˜ë˜ì§€ ì•ŠìŒ(ì‚¬ìš©í•˜ë ¤ë©´ ```weighted``` ì˜µì…˜ì„ í™œìš©)

  $$\text{Macro F1} = \frac{\text{F1_Class1} + \text{F1_Class2} + \text{F1_Class3} + \text{F1_Class4}}{4}$$

  - ì½”ë“œ

  ```

  from sklearn.metrics import f1_score

  f1_score(y_true, y_predicted, average = 'macro')

  ```





## **0-4. ë¡œë“œë§µ(ì „ë°˜ì ì¸ ì§„í–‰ process)**

1. ë¬¸ì œ ì´í•´

2. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)

3. íŠ¹ì„± ê³µí•™(Feature Engineering)

4. Baseline ML í•™ìŠµ ëª¨ë¸ ë¹„êµ

5. ì¢€ ë” ë³µì¡í•œ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ ì‹œë„

6. ì„ íƒí•œ ëª¨ë¸ ìµœì í™”

7. ì˜ˆì¸¡ ìˆ˜í–‰ / í™•ì¸

8. ê²°ë¡  ë„ì¶œ, ë‹¤ìŒ ë‹¨ê³„ ì œì‹œ


# **1. ì¤€ë¹„(Getting Started)**





## **1-1. ë¼ì´ë¸ŒëŸ¬ë¦¬ import**



```python
### ë°ì´í„° ë³€í˜•(ê°€ê³µ)
import pandas as pd
import numpy as np

### ì‹œê°í™”
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

plt.style.use('fivethirtyeight')
plt.rcParams['font.size'] = 18
plt.rcParams['patch.edgecolor'] = 'k'
```

## **1-2. ë°ì´í„° ì¤€ë¹„í•˜ê¸°**



```python
from google.colab import drive
drive.mount('/content/drive')
```


```python
pd.options.display.max_columns = 150 # ìµœëŒ€ 150 ì»¬ëŸ¼ë§Œ í‘œì‹œ

train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ECC 48á„€á…µ á„ƒá…¦á„€á…ªB/4á„Œá…®á„á…¡/Costa Rica/data/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ECC 48á„€á…µ á„ƒá…¦á„€á…ªB/4á„Œá…®á„á…¡/Costa Rica/data/test.csv')
```


```python
train.head()
```

- ì»¬ëŸ¼ë“¤ ê°„ì˜ ìˆœì„œëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨ëœë‹¤.


#**2. EDA(Exploratory Data Analysis) & ì „ì²˜ë¦¬**


## **2-1. ë°ì´í„° ì •ë³´ í™•ì¸**



```python
### í•™ìŠµìš© ë°ì´í„°

train.info()
```

- 130ê°œì˜ integerí˜• ì»¬ëŸ¼, 8ê°œì˜ floatí˜• ì»¬ëŸ¼ ë° 5ê°œì˜ object ì»¬ëŸ¼ì´ ì¡´ì¬

- integer ì»¬ëŸ¼ì˜ ê²½ìš°  0 ë˜ëŠ” 1ì„ ì‚¬ìš©í•˜ëŠ” bool ë³€ìˆ˜ ë˜ëŠ” ìˆœì„œí˜•(ordinal) ë³€ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ

- object ì»¬ëŸ¼ì˜ ê²½ìš° ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì— ì§ì ‘ ê³µê¸‰ë  ìˆ˜ ì—†ìŒ -> ì „ì²˜ë¦¬ í•„ìš”



```python
### í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°

test.info()
```

### **a) ì •ìˆ˜í˜•(Integer) Columns**

- ê° ì—´ì— ëŒ€í•´ ê³ ìœ í•œ ê°’ì˜ ìˆ˜ë¥¼ ì„¸ê³  ê·¸ ê²°ê³¼ë¥¼ **ë§‰ëŒ€ ê·¸ë˜í”„**ë¡œ í‘œì‹œ



```python
train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', 
                                                                             figsize = (8, 6),
                                                                             edgecolor = 'k', 
                                                                             linewidth = 2)
plt.xlabel('Number of Unique Values')
plt.ylabel('Count')
plt.title('Count of Unique Values in Integer Columns')
```

- ê³ ìœ í•œ ê°’ì´ 2ê°œë§Œ ìˆëŠ” ì»¬ëŸ¼ì€ **boolean(0 ë˜ëŠ” 1)**ì„ ë‚˜íƒ€ëƒ„

  - ëŒ€ë¶€ë¶„ì˜ boolean ì •ë³´ë“¤ì€ ëŒ€ë¶€ë¶„ ê°€êµ¬ ë‹¨ìœ„ë¡œ ì§‘ê³„ë˜ì–´ ìˆìŒ

  - ex> ```refrig``` ì»¬ëŸ¼

    - ê°€ì •ì— ëƒ‰ì¥ê³ ê°€ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ 0ê³¼ 1ë¡œ í‘œì‹œ

- ê°œì¸ ìˆ˜ì¤€ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë“¤ì€ ì§‘ê²¨ í•„ìš”


### **b) Float Columns**

- floatí˜• ë³€ìˆ˜ë“¤ì˜ ê²½ìš° ì£¼ë¡œ **ì—°ì†í˜•** ë³€ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ„

- ë¶„í¬ë„ plot(dist plot)ì„ í†µí•´ float ë³€ìˆ˜ë“¤ì˜ ë¶„í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŒ

  - ['OrderedDict'](https://pymotw.com/2/collections/ordereddict.html) ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ˆê³¤ ìˆ˜ì¤€ì„ ìƒ‰ìƒì— ë§¤í•‘



- ì•„ë˜ ê·¸ë˜í”„ëŠ” ```target``` ê°’ìœ¼ë¡œ í‘œì‹œëœ ```float``` ì—´ì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì¤Œ

  > ê°€êµ¬ì˜ ë¹ˆê³¤ ìˆ˜ì¤€ì— ë”°ë¼ ë³€ìˆ˜ë“¤ì˜ ë¶„í¬ì— ìœ ì˜í•œ ì°¨ì´ê°€ ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŒ



```python
from collections import OrderedDict # ìˆœì„œë¥¼ ê°€ì§€ëŠ” ë”•ì…”ë„ˆë¦¬

### ì‹œê°í™” format ì„¤ì •
plt.figure(figsize = (20, 16))
plt.style.use('fivethirtyeight')

### Color ì„¤ì •
colors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})
poverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 
                               3: 'vulnerable', 4: 'non vulnerable'})

### floatí˜• ë³€ìˆ˜ë“¤ ê°ê°ì— ëŒ€í•´...
for i, col in enumerate(train.select_dtypes('float')):
    ax = plt.subplot(4, 2, i + 1) # plotting ìœ„ì¹˜ ì„¤ì •
    # ê° ê°€êµ¬ë³„ ë¹ˆê³¤ ìˆ˜ì¤€ë³„ë¡œ
    for poverty_level, color in colors.items():
        # ê°ê° ë‹¤ë¥¸ ìƒ‰ìƒì˜ ì„ ìœ¼ë¡œ ì‹œê°í™”
        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(),
                    ax = ax, color = color, label = poverty_mapping[poverty_level])
        
    plt.title(f'{col.capitalize()} Distribution')
    plt.xlabel(f'{col}')
    plt.ylabel('Density')

plt.subplots_adjust(top = 2)
```

- ì‹œê°í™”ë¥¼ í†µí•´ ì–´ë–¤ ë³€ìˆ˜ê°€ ëª¨í˜•ê³¼ ê°€ì¥ ê´€ë ¨ì´ ìˆì„ì§€ë¥¼ ëŒ€ëµì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŒ

  - ex>

    - ```meaneduc```(ê°€êµ¬ ë‚´ ì„±ì¸ì˜ í‰ê·  êµìœ¡ì„ ë‚˜íƒ€ëƒ„)ì€ ë¹ˆê³¤ ìˆ˜ì¤€ê³¼ ê´€ë ¨ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì„

    - ```meaneduc```ì´ ë†’ì„ìˆ˜ë¡ ë¹ˆê³¤ ìˆ˜ì¤€ì´ ë‚®ì€ ê°€êµ¬ê°€ ì ìŒ



### **c) Object Columns**




```python
train.select_dtypes('object').head()
```

- ```id```, ```idhogar```: ë³€ìˆ˜ ì‹ë³„ì— í™œìš©

- ```dependency```: ì¢…ì†ë¥ , (19ì„¸ ë¯¸ë§Œ ë˜ëŠ” 64ì„¸ ì´ìƒ ê°€êµ¬ì› ìˆ˜)/(19ì„¸ ì´ìƒ 64ì„¸ ë¯¸ë§Œ ê°€êµ¬ì› ìˆ˜)

- ```edjeefe```: ë‚¨ì„± ê°€ì¥ì˜ ìˆ˜ë…„ê°„ êµìœ¡, ì—ìŠ¤ì½”ë¼ë¦¬(êµìœ¡ì—°ìˆ˜), ê°€ì¥ê³¼ ì„±ë³„ì„ ê¸°ë°˜ìœ¼ë¡œ yes = 1, no = 0ë¡œ í‘œì‹œ

- ```edjefa```: ì—¬ì„± ê°€ì¥ì˜ ìˆ˜ë…„ê°„ êµìœ¡, ì—ìŠ¤ì½”ë¼ë¦¬(êµìœ¡ì—°ìˆ˜), ê°€ì¥ê³¼ ì„±ë³„ì„ ê¸°ë°˜ìœ¼ë¡œ yes = 1 ë° no = 0



- ì„¸ ë³€ìˆ˜ì— ëŒ€í•´ yes = 1, no = 0ì˜ ê²½ìš° ë§¤í•‘ì„ í™œìš©í•˜ì—¬ ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•˜ê³  ë¶€ë™ ì†Œìˆ˜ì ìœ¼ë¡œ ë³€í™˜



```python
mapping = {"yes": 1, "no": 0}

### train, testì— ëŒ€í•´ ëª¨ë‘ ê°™ì€ ì‘ì—… ìˆ˜í–‰
for df in [train, test]:
    # mappingìœ¼ë¡œ ë°ì´í„°ë¥¼ ì ì ˆí•˜ê²Œ ë³€í™˜
    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)
    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)
    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)
```


```python
train[['dependency', 'edjefa', 'edjefe']].describe()
```

- ì œëŒ€ë¡œ ë³€í™˜ë˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.



```python
### ì‹œê°í™”

plt.figure(figsize = (16, 12))

### ê° ë³€ìˆ˜ë“¤ ë³„ë¡œ..
for i, col in enumerate(['dependency', 'edjefa', 'edjefe']):
    ax = plt.subplot(3, 1, i + 1)
    # ê° ê°€êµ¬ë³„ ë¹ˆê³¤ ìˆ˜ì¤€ë³„ë¡œ
    for poverty_level, color in colors.items():
        # ë¹ˆê³¤ ìˆ˜ì¤€ë³„ë¡œ ê°ê° ë‹¤ë¥¸ ìƒ‰ìœ¼ë¡œ í‘œì‹œ
        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), 
                    ax = ax, color = color, label = poverty_mapping[poverty_level])
        
    plt.title(f'{col.capitalize()} Distribution')
    plt.xlabel(f'{col}')
    plt.ylabel('Density')

plt.subplots_adjust(top = 2)
```

- í•´ë‹¹ ë³€ìˆ˜ë“¤ì´ ìˆ«ìë¡œ ì˜¬ë°”ë¥´ê²Œ í‘œí˜„ë¨

  - ML í•™ìŠµ ëª¨ë¸ì— ì…ë ¥ë  ìˆ˜ ìˆìŒ



```python
### test ë°ì´í„°ì˜ target ê°’ì„ ì¼ë‹¨ nullë¡œ ì´ˆê¸°í™”

test['Target'] = np.nan
data = train.append(test, ignore_index = True)
```

## **2-2. ë¼ë²¨(target) ë¶„í¬ í™•ì¸**

- êµ‰ì¥íˆ **ë¶ˆê· í˜•í•œ(imbalanced)** ë¶„í¬ë¥¼ ê°€ì§ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

- ì •ìˆ˜(1~4)ë¡œ ê° í´ë˜ìŠ¤ê°€ êµ¬ë¶„ë˜ì–´ ìˆìŒ

- ë¼ë²¨ì„ ì •í™•í•˜ê²Œ í™•ì¸í•˜ê¸° ìœ„í•´ ```parentesco1 == 1```ë¡œ í‘œì‹œëœ(ê° ê°€ì •ì˜ ê°€ì¥ì„ í‘œì‹œ) ì—´ë§Œ ê³ ë ¤




```python
# ê°€ì¥
heads = data.loc[data['parentesco1'] == 1].copy()

# í•™ìŠµì„ ìœ„í•œ ë¼ë²¨(target)
train_labels = data.loc[(data['Target'].notnull()) & (data['parentesco1'] == 1), ['Target', 'idhogar']]

# target ì§‘ê³„(ë¶„í¬ í™•ì¸)
label_counts = train_labels['Target'].value_counts().sort_index()

# ê° labelì˜ ë¶„í¬ì— ëŒ€í•œ ë§‰ëŒ€ ê·¸ë˜í”„
label_counts.plot.bar(figsize = (8, 6), 
                      color = colors.values(),
                      edgecolor = 'k', linewidth = 2)

# ì¶•, ì œëª© ì„¤ì •
plt.xlabel('Poverty Level'); plt.ylabel('Count'); 
plt.xticks([x - 1 for x in poverty_mapping.keys()], 
           list(poverty_mapping.values()), rotation = 60)
plt.title('Poverty Level Breakdown');

label_counts
```

- targetì˜ ë¶„í¬ê°€ ë§¤ìš° **ë¶ˆê· í˜•í•¨**

  - ```non_vernerable```ìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” ê°€êµ¬ì˜ ìˆ˜ê°€ ë‹¤ë¥¸ í´ë˜ìŠ¤ì— ë¹„í•´ í˜„ì €íˆ ë§ìŒ

  - ```extreme```ì€ ë§¤ìš° ì ìŒ

-  ë¶ˆê· í˜• í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° ML ëª¨ë¸ì´ í›¨ì”¬ ì ì€ ì˜ˆì œë¥¼ ë³´ê¸° ë•Œë¬¸ì— ì†Œìˆ˜ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆìŒ

  - ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Oversampling**ì„ ì ìš©í•  ìˆ˜ ìˆìŒ


## **2-3. ì˜ëª»ëœ label ì²˜ë¦¬**

- ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„° ê³¼í•™ í”„ë¡œì íŠ¸ì—ì„œ 80%ì˜ ì‹œê°„ì„ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ê³  ì´ìƒ ì§•í›„/ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ëŠ” ë° í• ì• 

- ì‚¬ëŒì˜ ì…ë ¥ ì˜¤ë¥˜, ì¸¡ì • ì˜¤ë¥˜ ë˜ëŠ” ì •í™•í•˜ì§€ë§Œ ëˆˆì— ë„ëŠ” ê·¹ë‹¨ê°’ ë“±

- ê°™ì€ ê°€êµ¬ì„ì—ë„ ê°œì¸ì€ ë¹ˆê³¤ ìˆ˜ì¤€ì´ ë‹¤ë¥¸, ì˜ëª»ëœ ë¼ë²¨ë“¤ì´ ì¡´ì¬í•¨

  - ê°€ì¥ì˜ ë¼ë²¨ì„ ì§„ì •í•œ(true) ë¼ë²¨ë¡œ í™œìš©

- ì˜ëª»ëœ labelì„ ì°¾ì•„ë‚´ê¸° ìœ„í•´ ê°€êµ¬ë³„ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•œ ë‹¤ìŒ,```target```ì˜ ê³ ìœ ê°’ì´ í•˜ë‚˜ë§Œ ìˆëŠ”ì§€ í™•ì¸



```python
# ê°€êµ¬ë³„ë¡œ ê·¸ë£¹í™”í•˜ê³  ê³ ìœ ê°’ì˜ ìˆ˜ íŒŒì•…
all_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)

# target ê°’ì´ ëª¨ë‘ ê°™ì§€ ì•Šì€ ê°€êµ¬
not_equal = all_equal[all_equal != True]
print('ê°€ì¡± êµ¬ì„±ì›ì´ ëª¨ë‘ ë™ì¼í•œ ëŒ€ìƒì´ ì•„ë‹Œ ê°€êµ¬ëŠ” {}ê°€êµ¬ì…ë‹ˆë‹¤.'.format(len(not_equal)))
```

- ì˜ˆì‹œë¥¼ í™•ì¸í•´ë³´ì.



```python
train[train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]
```

- ```parentesco1 == 1```ë¡œ í™•ì¸ ê²°ê³¼, í•´ë‹¹ ê°€êµ¬ì˜ ê²½ìš° ëª¨ë“  êµ¬ì„±ì›ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë ˆì´ë¸”ì€ 3(vulnerable)ì´ë‹¤.

- í•´ë‹¹ ê°€êµ¬ì˜ ëª¨ë“  êµ¬ì„±ì›ì—ì„œ ì˜¬ë°”ë¥¸ ë¹ˆê³¤ ìˆ˜ì¤€(target, label)ì„ ì¬í• ë‹¹


**cf> ê°€ì¥ì´ ì—†ëŠ” ê²½ìš°**




```python
households_leader = train.groupby('idhogar')['parentesco1'].sum()

# ê°€ì¥ì´ ì—†ëŠ” ê°€êµ¬
households_no_head = train.loc[train['idhogar'].isin(households_leader[households_leader == 0].index), :]

print('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))
```


```python
# ê°€ì¥ì´ ì—†ëŠ” ê°€êµ¬ë“¤ ì¤‘ êµ¬ì„±ì›ë“¤ ê°„ì˜ ë ˆì´ë¸”ì´ ë‹¤ë¥¸ ê°€êµ¬ ì°¾ê¸°

households_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)
print('{} Households with no head have different labels.'.format(sum(households_no_head_equal == False)))
```

- ë‹¤í–‰íˆ ì´ëŸ¬í•œ ë°ì´í„°ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.


**ì˜ëª»ëœ label ì²˜ë¦¬**



```python
### ë¼ë²¨ê°’ì´ ê°™ì§€ ì•Šì€ ê° ê°€êµ¬ë³„ë¡œ..
for household in not_equal.index:
    # ê°€ì¥ì„ í†µí•´ ì œëŒ€ë¡œ ëœ labelê°’ ì°¾ê¸°
    true_target = int(train[(train['idhogar'] == household) & (train['parentesco1'] == 1.0)]['Target'])
    
    # ì œëŒ€ë¡œ ëœ ê°’ìœ¼ë¡œ ì¬í• ë‹¹
    train.loc[train['idhogar'] == household, 'Target'] = true_target
    
    
# ê°€êµ¬ë³„ë¡œ ê·¸ë£¹í™”í•˜ê³  ê³ ìœ ê°’ì˜ ìˆ˜ íŒŒì•…
all_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)

# target ê°’ì´ í†µì¼ë˜ì§€ ì•Šì€ ê°€êµ¬
not_equal = all_equal[all_equal != True]
print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))
```

- ì œëŒ€ë¡œ ì²˜ë¦¬ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.


## **2-4. ê²°ì¸¡ì¹˜(Missing Value) ì²˜ë¦¬**

- ëˆ„ë½ëœ ê°’ì€ ML ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ì…ë ¥í•´ì•¼ í•˜ë©°, ë³€ìˆ˜ì— ë”°ë¼ ì…ë ¥í•˜ëŠ” ìµœì„ ì˜ ì „ëµì„ ìƒê°í•´ì•¼ í•©



```python
# ì»¬ëŸ¼(ë³€ìˆ˜)ë³„ë¡œ ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸
missing = pd.DataFrame(data.isnull().sum()).rename(columns = {0: 'total'})

# ê²°ì¸¡ì¹˜ì˜ ë¹„ìœ¨ í™•ì¸
missing['percent'] = missing['total'] / len(data)

missing.sort_values('percent', ascending = False).head(10).drop('Target') 
```

- ê²°ì¸¡ê°’ì˜ ë¹„ìœ¨ì´ ë†’ì€ ì„¸ ê°œì˜ ì»¬ëŸ¼ì„ ì²˜ë¦¬í•˜ì.

- ```v18q1```: ê°€ì¡±ë³„ë¡œ ì†Œìœ í•œ íƒœë¸”ë¦¿ ìˆ˜

  - ê°€êµ¬ ì°¨ì›ìœ¼ë¡œ ë³´ëŠ” ê²ƒì´ íƒ€ë‹¹í•¨

  - ê°€ì¥ì— ëŒ€í•´ì„œë§Œ í–‰ì„ ì„ íƒ

- ```v2a1```: ì›”ì„¸

- ```rez_esc```: ë’¤ë–¨ì–´ì§„ í•™ë…„ì˜ ì—° ìˆ˜



### **ğŸ“Œ Value Counts ì‹œê°í™” í•¨ìˆ˜**

- í•œ ì—´ì˜ ì¹´ìš´íŠ¸ ê°’ì„ ì‹œê°í™”

- ì„ íƒì ìœ¼ë¡œ **ê°€ì¥**ë§Œ í‘œì‹œ



```python
def plot_value_counts(df, col, heads_only = False):
    # ê°€ì¥ë§Œ ì„ íƒ
    if heads_only:
        df = df.loc[df['parentesco1'] == 1].copy()
        
    plt.figure(figsize = (8, 6))
    df[col].value_counts().sort_index().plot.bar(color = 'blue',
                                                 edgecolor = 'k',
                                                 linewidth = 2)
    plt.xlabel(f'{col}')
    plt.ylabel('Count')
    plt.title(f'{col} Value Counts')
    
    plt.show()
```

### **a) v18q1**



```python
plot_value_counts(heads, 'v18q1')
```

- ì¼ë‹¨ì€ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì†Œìœ í•  ìˆ˜ ìˆëŠ” íƒœë¸”ë¦¿ì˜ ìˆ˜ëŠ” 1ì¸ ê²ƒ ê°™ìŒ

  - í•˜ì§€ë§Œ, ê²°ì¸¡ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ìƒê°í•  í•„ìš”ê°€ ìˆìŒ

  - í•´ë‹¹ ë²”ì£¼ê°€ ```NaN```ì¸ ê°€êµ¬ëŠ” íƒœë¸”ë¦¿ì„ ì†Œìœ í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŒ

- ```v18q``` ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™” í•œ ë‹¤ìŒ ```v18q1```ì— ëŒ€í•œ null ê°’ì˜ ìˆ˜ë¥¼ ê³„ì‚°

  -  null ê°’ì´ ê°€ì¡±ì´ íƒœë¸”ë¦¿ì„ ì†Œìœ í•˜ì§€ ì•ŠìŒì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì¸ì§€ë¥¼ í™•ì¸ ê°€ëŠ¥ 



```python
heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())
```

- ```v18q1```ì—ì„œ ê²°ì¸¡ì¹˜ë¥¼ ê°€ì§€ëŠ” ê°€êµ¬ì˜ ê²½ìš° íƒœë¸”ë¦¿ì„ ì†Œìœ í•˜ì§€ ì•Šì€ ê°€êµ¬ë¼ëŠ” ì ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

  - í•´ë‹¹ ê°’ì˜ ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì±„ìš¸ ìˆ˜ ìˆìŒ



```python
### ê²°ì¸¡ì¹˜ ì²˜ë¦¬

data['v18q1'] = data['v18q1'].fillna(0)
```

### **b) v2a1**


- ì›”ì„¸ ë‚©ë¶€ì˜ ëˆ„ë½ëœ ê°’ì„ ì‚´í´ë³´ëŠ” ê²ƒ ì™¸ì—ë„ ì£¼íƒì˜ ì†Œìœ /ì„ëŒ€ ìƒíƒœë¥¼ ë³´ì—¬ì£¼ëŠ” ì—´ì— ìˆëŠ” ```tipovivi```ì˜ ë¶„í¬ë¥¼ ì‚´í´ë³´ëŠ” ê²ƒë„ í¥ë¯¸ë¡œìš¸ ê²ƒì„

  - ì›”ì„¸ ì§€ë¶ˆì— ëŒ€í•´ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì£¼íƒë“¤ì˜ ì†Œìœ  í˜„í™©ì„ íŒŒì•…í•˜ì.



```python
# ì£¼íƒ ì†Œìœ ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜
own_variables = [x for x in data if x.startswith('tipo')]


# ì£¼íƒ ì„ëŒ€ë£Œ ì§€ê¸‰(ê²°ì¸¡ì¹˜ O) vs ì£¼íƒ ì†Œìœ (Plottinh)
data.loc[data['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),
                                                              color = 'green',
                                                              edgecolor = 'k', 
                                                              linewidth = 2)
plt.xticks([0, 1, 2, 3, 4],
           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],
          rotation = 60)
plt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);
```

- ì£¼íƒ ì†Œìœ  ë³€ìˆ˜ ì„¤ëª…:

```

tipovivi1 = 1: ì „ì•¡ì„ ì§€ë¶ˆí•œ ë³¸ì¸ ì†Œìœ ì˜ ì§‘

tipovivi2 = 1: ì†Œìœ , í• ë¶€ë¡œ ì§€ë¶ˆ

tipovivi3 = 1: ì„ëŒ€ ì£¼íƒ

tipovivi4 = 1: ë¶ˆí™•ì‹¤

tipovivi5 = 1: ê¸°íƒ€(ëŒ€ì—¬)

```

- ëŒ€ë¶€ë¶„ ì›”ì„¸ë¥¼ ë‚´ì§€ ì•ŠëŠ” ê°€êµ¬ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ìì‹ ì˜ ì§‘ì„ ì†Œìœ í•˜ê³  ìˆìŒ

- ì¼ë¶€ ìƒí™©ì—ì„œëŠ” ê²°ì¸¡ì¹˜ê°€ ë°œìƒí•œ ì´ìœ ë¥¼ ì•Œ ìˆ˜ ì—†ìŒ



---



- ì†Œìœ í•˜ê³  ìˆê³  ì›”ì„¸ê°€ ëˆ„ë½ëœ ì£¼íƒì˜ ê²½ìš° ì„ëŒ€ë£Œ ì§€ê¸‰ì•¡ì„ 0ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŒ

- ë‹¤ë¥¸ ì£¼íƒì˜ ê²½ìš° ê²°ì¸¡ê°’ì„ ëƒ…ë‘˜ ìˆœ ìˆì§€ë§Œ í•´ë‹¹ ê°€êµ¬ì— ê²°ì¸¡ê°’ì´ ìˆìŒì„ ë‚˜íƒ€ë‚´ëŠ” í”Œë˜ê·¸(ë¶€ìš¸) ì—´ì„ ì¶”ê°€



```python
# ì§‘ì„ ì†Œìœ í•œ ê°€êµ¬ì˜ ê²½ìš° ì›”ì„¸ ë‚©ë¶€ì•¡ ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ í‘œê¸°
data.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0

# ëˆ„ë½ëœ ì„ëŒ€ë£Œ ì§€ê¸‰ì„ í‘œê¸°í•˜ëŠ” ì»¬ëŸ¼ ìƒì„±
data['v2a1-missing'] = data['v2a1'].isnull()

data['v2a1-missing'].value_counts()
```

### **c) rez_esc**

- ê²°ì¸¡ì¹˜ì˜ ê²½ìš° ê°€êµ¬ì—ì„œ í˜„ì¬ í•™êµì— ë‹¤ë‹ˆëŠ” ìë…€ê°€ ì—†ì„ ìˆ˜ ìˆìŒ

  - í•´ë‹¹ ì—´ì— ê²°ì¸¡ê°’ì´ ìˆëŠ” ì‚¬ëŒì˜ ë‚˜ì´ì™€ ê²°ì¸¡ê°’ì´ ì—†ëŠ” ì‚¬ëŒì˜ ë‚˜ì´ë¥¼ ì°¾ì•„ ì´ë¥¼ í™•ì¸í•˜ì.



```python
### ê²°ì¸¡ì¹˜ê°€ ì•„ë‹Œ ì‚¬ëŒë“¤ì˜ ë‚˜ì´

data.loc[data['rez_esc'].notnull()]['age'].describe()
```

- ê²°ì¸¡ì¹˜ê°€ ê°€ì¥ ë§ì€ ë‚˜ì´ëŠ” **17ì„¸**ì„

  - ì´ê²ƒë³´ë‹¤ ë‚˜ì´ê°€ ë§ì€ ì‚¬ëŒì´ë¼ë©´, ìš°ë¦¬ëŠ” ë‹¨ìˆœíˆ ì´ ì‚¬ëŒë“¤ì´ í•™êµì— ë‹¤ë‹ˆì§€ ì•ŠëŠ”ë‹¤ê³  ê°€ì •í•  ìˆ˜ë„ ìˆìŒ

  



```python
### ë°ì´í„°ê°€ ê²°ì¸¡ì¸ ì‚¬ëŒë“¤ì˜ ë‚˜ì´

data.loc[data['rez_esc'].isnull()]['age'].describe()
```

- ëŒ€íšŒ ì •ë³´ì— ì˜í•˜ë©´ í•´ë‹¹ ë³€ìˆ˜ëŠ” **7ì„¸ì—ì„œ 19ì„¸ ì‚¬ì´**ì˜ ê°œì¸ì—ê²Œë§Œ ì •ì˜ë¨

  - í•´ë‹¹ ë²”ìœ„ë³´ë‹¤ ë‚˜ì´ê°€ ì–´ë¦° ì‚¬ëŒì´ë‚˜ ë‚˜ì´ê°€ ë§ì€ ì‚¬ëŒì€ ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ë¨

  - ë‹¤ë¥¸ ì‚¬ëŒì˜ ê²½ìš° ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³ , bool flagë¥¼ ì¶”ê°€



```python
# 19ì„¸ ì´ìƒì´ê±°ë‚˜ 7ì„¸ ë¯¸ë§Œì¸ ì‚¬ëŒ -> 0ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
data.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0

# 7ì„¸ì—ì„œ 19ì„¸ ì‚¬ì´ì¸ ì‚¬ëŒ ì¤‘ ê²°ì¸¡ì¹˜ì¸ ê²½ìš° flag ë³€ìˆ˜ë¡œ í‘œì‹œ
data['rez_esc-missing'] = data['rez_esc'].isnull()
```

- ```rez_esc```ì— ì´ìƒì¹˜ê°€ í•˜ë‚˜ ì¡´ì¬í•¨

  - ëŒ€íšŒ ì„¤ëª…ì— ì˜í•˜ë©´, í•´ë‹¹ ë³€ìˆ˜ì˜ ìµœëŒ“ê°’ì€ **5**

  - 5ë³´ë‹¤ í° ê°’ë“¤ì„ 5ë¡œ ì¬í• ë‹¹



```python
### ì´ìƒì¹˜ ì²˜ë¦¬

data.loc[data['rez_esc'] > 5, 'rez_esc'] = 5
```

## **2-5. ë²”ì£¼í˜• ë³€ìˆ˜ ì‹œê°í™”**

- ë‘ ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ì„œë¡œ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ì‹ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ **ì‚°ì ë„**,**ëˆ„ì ë§‰ëŒ€ê·¸ë¦¼**, **ìƒìê·¸ë¦¼** ë“± ì—¬ëŸ¬ ê°€ì§€ í‘œì‹œ ì˜µì…˜ì´ ìˆìŒ

- ê°ê°ì˜ x ê°’ì— ëŒ€í•œ yê°’ì˜ ë°±ë¶„ìœ¨ì„ ì ì˜ í¬ê¸°ë¡œ í‘œí˜„í•˜ëŠ” ì‚°ì ë„ ê·¸ë¦¼ìœ¼ë¡œ ì‹œê°í™” ì§„í–‰



```python
### ì‚°ì ë„ ì‹œê°í™”ë¥¼ ìœ„í•œ í•¨ìˆ˜ ì •ì˜

def plot_categoricals(x, y, data, annotate = True):
    """
    - ë‘ ë²”ì£¼í˜•ì˜ ì¹´ìš´íŠ¸ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    - size: ê° ê·¸ë£¹ì˜ ì¹´ìš´íŠ¸ ìˆ˜
    - percentage: yì˜ ì£¼ì–´ì§„ ê°’ì— ëŒ€í•œ ê²ƒ
    """
    
    # Raw counts 
    raw_counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = False))
    raw_counts = raw_counts.rename(columns = {x: 'raw_count'})
    
    # x ë° yì˜ ê° ê·¸ë£¹ì— ëŒ€í•œ ì¹´ìš´íŠ¸ ê³„ì‚°
    counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = True)) # ì •ê·œí™” ìˆ˜í–‰
    
    # ì—´ ì´ë¦„ ë³€ê²½ ë° ì¸ë±ìŠ¤ ì¬ì„¤ì •
    counts = counts.rename(columns = {x: 'normalized_count'}).reset_index()
    counts['percent'] = 100 * counts['normalized_count']
    
    # Add the raw count
    counts['raw_count'] = list(raw_counts['raw_count'])
    
    # ë°±ë¶„ìœ¨ë¡œ í¬ê¸°ê°€ ì¡°ì •ëœ ì‚°ì ë„
    plt.figure(figsize = (14, 10))
    plt.scatter(counts[x], counts[y], edgecolor = 'k', color = 'lightgreen',
                s = 100 * np.sqrt(counts['raw_count']), marker = 'o',
                alpha = 0.6, linewidth = 1.5)
    
    if annotate:
        # í…ìŠ¤íŠ¸ë¡œ í”Œë¡¯ì— ì£¼ì„ ë‹¬ê¸°
        for i, row in counts.iterrows():
            plt.annotate(xy = (row[x] - (1 / counts[x].nunique()), 
                               row[y] - (0.15 / counts[y].nunique())),
                         color = 'navy',
                         text = f"{round(row['percent'], 1)}%")
        
    # ì¶• ì„¤ì •
    plt.yticks(counts[y].unique())
    plt.xticks(counts[x].unique())
    
    # ì œê³±ê·¼ ì˜ì—­ì—ì„œ ìµœì†Œ ë° ìµœëŒ€ë¥¼ ê· ë“±í•œ ê³µê°„ìœ¼ë¡œ ë³€í™˜
    sqr_min = int(np.sqrt(raw_counts['raw_count'].min()))
    sqr_max = int(np.sqrt(raw_counts['raw_count'].max()))
    
    # 5 sizes for legend
    msizes = list(range(sqr_min, sqr_max,
                        int(( sqr_max - sqr_min) / 5)))
    markers = []
    
    # Markers for legend
    for size in msizes:
        markers.append(plt.scatter([], [], s = 100 * size, 
                                   label = f'{int(round(np.square(size) / 100) * 100)}', 
                                   color = 'lightgreen',
                                   alpha = 0.6, edgecolor = 'k', linewidth = 1.5))
        
    # Legend and formatting
    plt.legend(handles = markers, title = 'Counts',
               labelspacing = 3, handletextpad = 2,
               fontsize = 16,
               loc = (1.10, 0.19))
    
    plt.annotate(f'* Size represents raw count while % is for a given y value.',
                 xy = (0, 1), xycoords = 'figure points', size = 10)
    
    # Adjust axes limits
    plt.xlim((counts[x].min() - (6 / counts[x].nunique()), 
              counts[x].max() + (6 / counts[x].nunique())))
    plt.ylim((counts[y].min() - (4 / counts[y].nunique()), 
              counts[y].max() + (4 / counts[y].nunique())))
    plt.grid(None)
    plt.xlabel(f"{x}"); plt.ylabel(f"{y}"); plt.title(f"{y} vs {x}");
```


```python
plot_categoricals('rez_esc', 'Target', data)
```

- ë§ˆì»¤ì˜ í¬ê¸°: raw count

- í•´ì„

  = ì§€ì •ëœ y ê°’ì„ ì„ íƒí•œ ë‹¤ìŒ í–‰ ì „ì²´ë¥¼ ì½ê¸°

  - ì˜ˆì‹œ: ë¹ˆê³¤ ìˆ˜ì¤€ì´ 1ì¸ ê²½ìš°, 93%ì˜ ê°œì¸ì´ 1ë…„ ì´ìƒ ë’¤ì²˜ì§€ì§€ ì•Šê³  ì´ 800ëª… ì •ë„ì˜ ê°œì¸ì´ ìˆìœ¼ë©°, ì•½ 0.4%ì˜ ê°œì¸ì´ 5ë…„ ë’¤ì³ì ¸ ìˆìœ¼ë©°, ì´ ë²”ì£¼ì— ì†í•˜ëŠ” ì´ 50ëª… ì •ë„ì˜ ê°œì¸ì´ ìˆìŒ

- í•´ë‹¹ ê·¸ë¦¼ì€ ì „ì²´ ì¹´ìš´íŠ¸ì™€ ë²”ì£¼ ë‚´ ë¹„ìœ¨ì„ ëª¨ë‘ í‘œì‹œ



```python
plot_categoricals('escolari', 'Target', data, annotate = False)
```

- ê²°ì¸¡ì¹˜ ì¤‘ ë‚¨ì€ ê°’ì€ **ì¤‘ê°„ê°’**ìœ¼ë¡œ ì±„ìš°ì.


- target ê°’ ë¶„í¬ë¥¼ ì‹œê°í™”í•˜ì—¬ ê²°ì¸¡ê°’ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ



```python
plot_value_counts(data[(data['rez_esc-missing'] == 1)], 'Target')
```

- í•´ë‹¹ ë¶„í¬ëŠ” ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ë¶„í¬ì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì„



```python
plot_value_counts(data[(data['v2a1-missing'] == 1)], 'Target')
```

- target = 2(moderate)ì˜ ë¹„ìœ¨ì´ ë†’ìŒ

  - ë” ë§ì€ ë¹ˆê³¤ì˜ ì§€í‘œê°€ ë  ìˆ˜ ìˆìŒ


# **3. íŠ¹ì„± ê³µí•™(Feature Engineering)**

- í•™ìŠµì„ ìœ„í•´ì„œëŠ” ê° ê°€êµ¬ì— ëŒ€í•´ **ìš”ì•½ëœ** ëª¨ë“  ì •ë³´ê°€ í•„ìš”í•¨

  - ê°€êµ¬ ë‚´ì˜ ê°œì¸ì„ ```groupby()```í•˜ê³  ê°œë³„ ë³€ìˆ˜ì˜ ```agg()```ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ì˜ë¯¸

- ì´í›„ íŠ¹ì„± ê³µí•™ì„ **ìë™í™”**ì‹œí‚¤ëŠ” ë°©ë²•ë„ ìˆìŒ


## **3-1. ì»¬ëŸ¼(ë³€ìˆ˜) ì •ì˜í•˜ê¸°**

- [data descriptions](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data) ë¥¼ í†µí•´ ê°œì¸ ìˆ˜ì¤€ê³¼ ê°€êµ¬ ìˆ˜ì¤€ì— ìˆëŠ” ì—´ì„ ì •ì˜í•´ì•¼ í•¨

  - ë³€ìˆ˜ ìì²´ë¥¼ ê²€í† 

- ì¼ë¶€ ë³€ìˆ˜ëŠ” ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ë³€ìˆ˜ë¥¼ ì •ì˜

  - ê° ìˆ˜ì¤€ì—ì„œ ì •ì˜ëœ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ í•„ìš”ì— ë”°ë¼ ë³€ìˆ˜ë¥¼ ì§‘ê³„í•  ìˆ˜ ìˆìŒ

- ì§„í–‰ í”„ë¡œì„¸ìŠ¤

1. ë³€ìˆ˜ë¥¼ ê°€êµ¬ ìˆ˜ì¤€ê³¼ ê°œì¸ ìˆ˜ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°

2. ê°œì¸ ìˆ˜ì¤€ì˜ ë°ì´í„°ì— ì í•©í•œ ì§‘ê³„ ì°¾ê¸°

  * ìˆœì„œí˜•(ordinal) ë³€ìˆ˜: í†µê³„ì  ì§‘ê³„ í™œìš©

  * ë…¼ë¦¬í˜•(bool)ë³€ìˆ˜: ì§‘ê³„í•  ìˆ˜ëŠ” ìˆì§€ë§Œ, í†µê³„ì¹˜ì˜ ì¢…ë¥˜ëŠ” ì ìŒ

3. ê°œì¸ ìˆ˜ì¤€ì˜ ì§‘ê³„ë¥¼ ê°€êµ¬ ìˆ˜ì¤€ ë°ì´í„°ì— ê²°í•©





### **ğŸ“Œ ë³€ìˆ˜ë“¤ì˜ ë²”ì£¼ ì •ì˜í•˜ê¸°**

- ë³€ìˆ˜ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ ë²”ì£¼ê°€ ìˆìŒ

1. Individual Variables: ê°œì¸ë³„ íŠ¹ì„±

  - boolean: yes or no(0 ë˜ëŠ” 1)

  - ordered discrete: ìˆœì„œê°€ ìˆëŠ” ì •ìˆ˜

2. Household variables

  - boolean: Yes or No

  - ordered discrete: ìˆœì„œê°€ ìˆëŠ” ì •ìˆ˜

  - ì—°ì†í˜• ìˆ˜ì¹˜

3. Squared Variables: ë°ì´í„°ì˜ (ë³€ìˆ˜)^2ì—ì„œ íŒŒìƒ

4. Id variables: ë°ì´í„°ë¥¼ ì‹ë³„ìš©, í”¼ì³ë¡œ ì‚¬ìš© x




```python
### Id variables

id_ = ['Id', 'idhogar', 'Target']
```


```python
### Individual Variables

ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 
            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', 
            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', 
            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', 
            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', 
            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 
            'instlevel9', 'mobilephone', 'rez_esc-missing']

ind_ordered = ['rez_esc', 'escolari', 'age']
```


```python
### Household variables

hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', 
           'paredpreb','pisocemento', 'pareddes', 'paredmad',
           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', 
           'pisonatur', 'pisonotiene', 'pisomadera',
           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', 
           'abastaguadentro', 'abastaguafuera', 'abastaguano',
            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', 
           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',
           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', 
           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 
           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',
           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 
           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', 
           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',
           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']

hh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', 
              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',
              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']

hh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']
```


```python
sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 
        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']
```

- ì¤‘ë³µ ì—¬ë¶€/ ë¹ ì§„ ë³€ìˆ˜ í™•ì¸



```python
x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_

from collections import Counter

print('There are no repeats: ', np.all(np.array(list(Counter(x).values())) == 1))
print('We covered every variable: ', len(x) == data.shape[1]) # ì—´ì˜ ê°œìˆ˜ì™€ ë™ì¼í•œì§€ í™•ì¸
```

### **âº Squared Variables**  

-  ì„ í˜• ëª¨í˜•ì´ **ë¹„ì„ í˜• ê´€ê³„**ë¥¼ í•™ìŠµí•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ë³€ìˆ˜ê°€ í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ì˜ ì¼ë¶€ë¡œ ì œê³±ë˜ê±°ë‚˜ ë³€í™˜ë˜ëŠ” ê²½ìš°ê°€ ì¡´ì¬

  - ë” ë³µì¡í•œ ëª¨ë¸ì„ ì‚¬ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ squared featureë“¤ì´ **ì¤‘ë³µë¨**

  - ì œê³±ë˜ì§€ ì•Šì€ featureë“¤ê³¼ **ë†’ì€** ìƒê´€ê´€ê³„ -> ê´€ë ¨ ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€í•˜ê³  í•™ìŠµì„ ëŠë¦¬ê²Œ í•¨ìœ¼ë¡œì¨ ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒ



- ex> ```SQBage``` vs ```age```



```python
sns.lmplot(x = 'age', y = 'SQBage', data = data, fit_reg = False);
plt.title('Squared Age versus Age');
```

- ë‘ ë³€ìˆ˜ì˜ ìƒê´€ê³„ìˆ˜ê°€ ë§¤ìš° í¬ë‹¤.

  - ë°ì´í„°ì— ë‘ ë³€ìˆ˜ ëª¨ë‘ë“¤ ì €ì¥í•  í•„ìš”ê°€ x



```python
# ì œê³±í•œ ë³€ìˆ˜ ì œê±°í•˜ê¸°
data = data.drop(columns = sqr_)
data.shape
```

### **âº Id Variables**

- ë°ì´í„° ì‹ë³„ì— í•„ìš” -> ìœ ì§€



### **âº Household Variables**



```python
heads = data.loc[data['parentesco1'] == 1, :]
heads = heads[id_ + hh_bool + hh_cont + hh_ordered]
heads.shape
```

- ëŒ€ë¶€ë¶„ì˜ ê°€êµ¬ ìˆ˜ì¤€ ë³€ìˆ˜ì˜ ê²½ìš° ê·¸ëŒ€ë¡œ ìœ ì§€í•  ìˆ˜ë„ ìˆìŒ

- ì¼ë¶€ ì¤‘ë³µ ë³€ìˆ˜ë¥¼ ì œê±°í•˜ê³  ê¸°ì¡´ ë°ì´í„°ì—ì„œ íŒŒìƒëœ ê¸°ëŠ¥ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŒ


**ì¤‘ë³µëœ Household Variables**

- ìƒê´€ ê´€ê³„ê°€ ë„ˆë¬´ **ë†’ì€** ë³€ìˆ˜ê°€ ìˆìœ¼ë©´ ìƒê´€ ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì œê±°í•  ìˆ˜ ìˆìŒ




```python
# ìƒê´€ê³„ìˆ˜ í–‰ë ¬
corr_matrix = heads.corr()

# ìƒì‚¼ê°í–‰ë ¬ë§Œ ë‚¨ê¸°ê¸°
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# ìƒê´€ ê´€ê³„ê°€ 0.95ë³´ë‹¤ í° feeature columnì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]

to_drop
```


```python
corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]
```


```python
sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],
            annot=True, cmap = plt.cm.autumn_r, fmt='.3f');
```

- ì§‘ì˜ í¬ê¸°ì™€ ê´€ë ¨ëœ ëª‡ ê°€ì§€ ë³€ìˆ˜ê°€ ìˆìŒ

  - ```r4t3```: ê°€êµ¬ ë‚´ ì´ ì¸ì›ìˆ˜

  - ```tamhog```: ê°€êµ¬ í¬ê¸°

  - ```tamviv```: ê°€êµ¬ì— ì‚¬ëŠ” ì‚¬ëŒë“¤ì˜ ìˆ˜

  - ```hhsize```: ê°€êµ¬ í¬ê¸°

  - ```hogar_total```: ê°€êµ¬ êµ¬ì„±ì› ìˆ˜

- ì´ ë³€ìˆ˜ë“¤ì€ ëª¨ë‘ ì„œë¡œ **ë†’ì€** ìƒê´€ ê´€ê³„ë¥¼ ê°€ì§

  - ```hhsize```ëŠ” ```tamhog```ì™€ ```hogar_total```ê³¼ ì™„ë²½í•œ **ì–‘ì˜ ìƒê´€ê´€ê³„**ë¥¼ ê°€ì§

  - ```rt4t3```ê³¼ ```hhsize```ëŠ” **ê±°ì˜ ì™„ë²½í•œ ìƒê´€ê´€ê³„**ë¥¼ ê°€ì§

  > ë‘ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ë¥¼ ì œê±°í•  ìˆ˜ ìˆìŒ 



```python
heads = heads.drop(columns = ['tamhog', 'hogar_total', 'r4t3'])
```

---

- **hhsize vs tamviv** 



```python
sns.lmplot(x = 'tamviv', y = 'hhsize', data = data, fit_reg = False);
plt.title('Household size vs number of persons living in the household');
```

- ê°€ì¡±ë³´ë‹¤ ê°€êµ¬ì— ë” ë§ì€ ì‚¬ëŒë“¤ì´ ì‚´ê³  ìˆìŒ..?  

~(ë¬´ìŠ¨ ì˜ë¯¸ì¸ì§€..)~



```python
heads['hhsize-diff'] = heads['tamviv'] - heads['hhsize']
plot_categoricals('hhsize-diff', 'Target', heads)
```

- ëŒ€ë¶€ë¶„ì˜ ê°€êµ¬ëŠ” ì°¨ì´ê°€ ì—†ì§€ë§Œ ê°€êµ¬ êµ¬ì„±ì›ë³´ë‹¤ ê°€êµ¬ì— ì‚´ê³  ìˆëŠ” ì‚¬ëŒì˜ ìˆ˜ê°€ ë§ì€ ê²½ìš°ê°€ ì ì§€ë§Œ ì¡´ì¬í•¨


---

- **coopele ë³€ìˆ˜**



```python
corr_matrix.loc[corr_matrix['coopele'].abs() > 0.9, corr_matrix['coopele'].abs() > 0.9]
```

- ```coopele```: ê°€ì •ì˜ ì „ê¸°ê°€ ì–´ë””ì—ì„œ ì˜¤ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„

- ë„¤ ê°€ì§€ ì„ íƒì§€ê°€ ìˆëŠ”ë°, ì´ ë‘ ê°€ì§€ ì„ íƒì§€ ì¤‘ í•˜ë‚˜ê°€ ì—†ëŠ” ê°€ì •ì€ ì „ê¸°ê°€ ì—†ê±°ë‚˜(```noelec```) ê°œì¸ ë°œì „ì†Œì—ì„œ ê³µê¸‰ë°›ìŒ(```planpri```)


```

    0: No electricity

    1: Electricity from cooperative

    2: Electricity from CNFL, ICA, ESPH/JASEC

    3: Electricity from private plant

```

- ì •ë ¬ëœ ë³€ìˆ˜ì—ëŠ” ê³ ìœ í•œ ìˆœì„œê°€ ìˆìœ¼ë©°, **ë„ë©”ì¸ ì§€ì‹**ì— ê¸°ë°˜í•˜ì—¬ ì„ íƒ

- ìƒˆë¡œìš´ ìˆœì„œí˜• ë³€ìˆ˜ë¥¼ ìƒì„±í•œ ë‹¤ìŒ, ê¸°ì¡´ì˜ ë³€ìˆ˜ë“¤ì„ ì‚­ì œí•´ë„ ok

- ê²°ì¸¡ì¹˜ì¸ ë°ì´í„°ì˜ ê²½ìš° ```NaN```ì„ ì…ë ¥í•˜ê³  boolean ì»¬ëŸ¼ìœ¼ë¡œ í•´ë‹¹ ë³€ìˆ˜ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ìŒì„ í‘œì‹œ



```python
elec = []

# ê°’ í• ë‹¹(mapping)
for i, row in heads.iterrows():
    if row['noelec'] == 1:
        elec.append(0)
    elif row['coopele'] == 1:
        elec.append(1)
    elif row['public'] == 1:
        elec.append(2)
    elif row['planpri'] == 1:
        elec.append(3)
    else:
        elec.append(np.nan)
        
# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
heads['elec'] = elec
heads['elec-missing'] = heads['elec'].isnull()

# ê¸°ì¡´ ì»¬ëŸ¼ ì œê±°
# heads = heads.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])
```


```python
plot_categoricals('elec', 'Target', heads)
```

- targetì˜ ëª¨ë“  ê°’ì— ëŒ€í•´ ê°€ì¥ ì¼ë°˜ì ì¸ ì „ê¸° ê³µê¸‰ì›ì´ ë‚˜ì—´ëœ ê³µê¸‰ì—…ì²´ ì¤‘ í•˜ë‚˜ì„ì„ ì•Œ ìˆ˜ ìˆìŒ


---

- **area2 ë³€ìˆ˜**


- ì§‘ì´ ì‹œê³¨ ì§€ì—­ì— ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸

  - ì§‘ì´ ë„ì‹œ ì§€ì—­ì— ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì—´(area1)ì´ ìˆê¸° ë•Œë¬¸ì— ì¤‘ë³µë©

> í•´ë‹¹ ì—´ ì‚­ì œ



```python
heads = heads.drop(columns = 'area2')

heads.groupby('area1')['Target'].value_counts(normalize = True)
```

- ë„ì‹œ ì§€ì—­ì˜ ê°€êµ¬(value = 1)ëŠ” ë†ì´Œ ì§€ì—­ì˜ ê°€êµ¬(value = 0)ë³´ë‹¤ ë¹ˆê³¤ ìˆ˜ì¤€ì´ **ë‚®ì„** ê°€ëŠ¥ì„±ì´ ë” ë†’ì€ ê²ƒìœ¼ë¡œ íŒë‹¨ë¨


### **âº Ordinal Variables**

- ì§‘ì˜ ë²½, ì§€ë¶•, ë°”ë‹¥ì—ëŠ” ê°ê° 3ê°œì˜ columnì´ ì¡´ì¬í•¨

  - 'bad', 'regular', 'good'

- ë³€ìˆ˜ë¥¼ boolean í˜•ìœ¼ë¡œ ëƒ…ë‘˜ ìˆ˜ë„ ìˆì§€ë§Œ, **bad < regular < good**ì´ë¼ëŠ” ëª…í™•í•œ ìˆœì„œê°€ ì¡´ì¬í•¨

  - ìˆœì„œí˜• ë³€ìˆ˜ë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ ë” ì¢‹ì•„ ë³´ì„

  - ```np.argmax()```ë¥¼ í†µí•´ ê° ê°€êµ¬ì— ëŒ€í•´ 0ì´ ì•„ë‹Œ ì—´ì„ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆìŒ



```python
### Wall ordinal variable

heads['walls'] = np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]),
                           axis = 1)

# heads = heads.drop(columns = ['epared1', 'epared2', 'epared3'])
plot_categoricals('walls', 'Target', heads)
```


```python
### Roof ordinal variable

heads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]),
                           axis = 1)
heads = heads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])
```


```python
### Floor ordinal variable

heads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]),
                           axis = 1)
# heads = heads.drop(columns = ['eviv1', 'eviv2', 'eviv3'])
```

## **3-2. ë³€ìˆ˜(feature) êµ¬ì„±í•˜ê¸°**

- ë³€ìˆ˜ë¥¼ ìˆœì„œí˜• í”¼ì²˜ì— ë§¤í•‘í•˜ëŠ” ê²ƒ ì™¸ì—ë„ ê¸°ì¡´ ë°ì´í„°ì—ì„œ ì™„ì „íˆ ìƒˆë¡œìš´ í”¼ì²˜ë¥¼ ìƒì„±í•  ìˆ˜ë„ ìˆìŒ

  - ì˜ˆì‹œ> ì´ì „ì˜ ì„¸ ê°€ì§€ íŠ¹ì§•(wall, roof, floor)ì„ í•©ì‚°í•˜ì—¬ ì§‘ êµ¬ì¡°ì˜ ì „ë°˜ì ì¸ í’ˆì§ˆì„ ì¸¡ì •í•  ìˆ˜ ìˆìŒ 


- **walls + roof + floor**



```python
# ìƒˆë¡œìš´ feature ìƒì„±í•˜ê¸°

heads['walls+roof+floor'] = heads['walls'] + heads['roof'] + heads['floor']

plot_categoricals('walls+roof+floor', 'Target', heads, annotate = False)
```

- ìƒˆë¡œìš´ featureëŠ” **target = 4(the lowest poverty level)**ì—ì„œ  ```house quality``` ë³€ìˆ˜ì˜ ê°’ì´ ë” ë†’ì€ ê²½í–¥ì´ ìˆëŠ” ê²ƒì²˜ëŸ¼ ë³´ì„



```python
counts = pd.DataFrame(heads.groupby(['walls+roof+floor'])['Target'].value_counts(normalize = True)).rename(columns = {'Target': 'Normalized Count'}).reset_index()
counts.head()
```

- **warning**


- ì§‘ì˜ ì§ˆì— ëŒ€í•œ ê²½ê³ 

- í™”ì¥ì‹¤, ì „ê¸°, ë°”ë‹¥, ìˆ˜ë„, ì²œì¥ì´ ì—†ëŠ” ê²½ìš° ê°ê° **-1ì **ì˜ ë§ˆì´ë„ˆìŠ¤ ê°’



```python
# No toilet, no electricity, no floor, no water service, no ceiling

heads['warning'] = 1 * (heads['sanitario1'] + 
                         (heads['elec'] == 0) + 
                         heads['pisonotiene'] + 
                         heads['abastaguano'] + 
                         (heads['cielorazo'] == 0))
```


```python
### seabornì˜ violin plotìœ¼ë¡œ ì‹œê°í™”

plt.figure(figsize = (10, 6))
sns.violinplot(x = 'warning', y = 'Target', data = heads)
plt.title('Target vs Warning Variable');
```

- ë°”ì´ì˜¬ë¦° í”Œë¡¯ì€ ëŒ€ìƒì´ ì‹¤ì œë³´ë‹¤ ë” ì‘ê³  ë” í° ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ëŠ” íš¨ê³¼ë¡œ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ í‰í™œ(smoothing)í•˜ê¸° ë•Œë¬¸ì— ì—¬ê¸°ì„œëŠ” ì í•©í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ íŒë‹¨ë¨

  - í•˜ì§€ë§Œ, ê²½ê³  ì‹ í˜¸ê°€ ì—†ëŠ” ê°€êµ¬ ê·¸ë£¹ì— ë¹ˆê³¤ ìˆ˜ì¤€ì´ ê°€ì¥ ë‚®ì€ ê°€êµ¬ë“¤ì´ ì§‘ì¤‘ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ



```python
plot_categoricals('warning', 'Target', data = heads)
```

- **bonus**


- ëƒ‰ì¥ê³ , ì»´í“¨í„°, íƒœë¸”ë¦¿, í…”ë ˆë¹„ì „ì„ ê°€ì§€ê³  ìˆìœ¼ë©´ ì ìˆ˜ê°€ **ì˜¬ë¼ê°€ëŠ”** ë³€ìˆ˜



```python
# Owns a refrigerator, computer, tablet, and television

heads['bonus'] = 1 * (heads['refrig'] + 
                      heads['computer'] + 
                      (heads['v18q1'] > 0) + 
                      heads['television'])

sns.violinplot(x = 'bonus', y = 'Target', 
               data = heads, figsize = (10, 6))

plt.title('Target vs Bonus Variable');
```

## **3-3. Per Capita Features**

- ê°€êµ¬ì› ë³„ë¡œ íŠ¹ì • ì¸¡ì •ê°’ì˜ ìˆ˜ë¥¼ ê³„ì‚°

- íŠ¹ì •ê°’ / ê°€êµ¬ êµ¬ì„±ì› ìˆ˜



```python
heads['phones-per-capita'] = heads['qmobilephone'] / heads['tamviv']
heads['tablets-per-capita'] = heads['v18q1'] / heads['tamviv']
heads['rooms-per-capita'] = heads['rooms'] / heads['tamviv']
heads['rent-per-capita'] = heads['v2a1'] / heads['tamviv']
```

## **3-4. Household Variables ì‚´í´ë³´ê¸°**

- ê´€ê³„ë¥¼ ì •ëŸ‰í™”



### **a) ìƒê´€ê´€ê³„ í™•ì¸í•˜ê¸°**

- ë‘ ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì¸¡ì •í•˜ëŠ” ë°ëŠ” ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ì´ ìˆìŒ

---

**1. Pearson Correlation**

- -1ë¶€í„° 1ê¹Œì§€ ë‘ ë³€ìˆ˜ ì‚¬ì´ì˜ **ì„ í˜• ê´€ê³„** ì¸¡ì •

- ì¦ê°€ ì¶”ì„¸ê°€ ì •í™•í•˜ê²Œ **ì„ í˜•**ì¸ ê²½ìš°ì—ë§Œ í•˜ë‚˜ê°€ ë  ìˆ˜ ìˆìŒ



**2. Spearman Correlation**

- -1ì—ì„œ 1ê¹Œì§€ ë‘ ë³€ìˆ˜ì˜ ë‹¨ì¡°ë¡œìš´ ê´€ê³„ë¥¼ ì¸¡ì •

- í•œ ë³€ìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ê´€ê³„ê°€ ì„ í˜•ì ì´ì§€ ì•Šë”ë¼ë„ ë‹¤ë¥¸ ë³€ìˆ˜ë„ ì¦ê°€í•˜ëŠ” ê²½ìš° ìƒê´€ê³„ìˆ˜ê°€ 1ì„

- ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜ ê³„ì‚°ì—ëŠ” ê´€ê³„ì˜ ì¤‘ìš”ì„± ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ```p-value```ë„ í•¨ê»˜ ì‚°ì¶œë¨

  - ```p-value```ê°€ 0.05 ë¯¸ë§Œì´ë©´ ì¼ë°˜ì ìœ¼ë¡œ ìœ ì˜í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼ë¨

---

- ìƒê´€ê³„ìˆ˜ í•´ì„



```

- 0.00 ~ 0.19: "ë§¤ìš° ì•½í•œ ìƒê´€ê´€ê³„"

- 0.20 ~ 0.39: "ì•½í•œ ìƒê´€ê´€ê³„"

- 0.40 ~ 0.59: "ì–´ëŠ ì •ë„ì˜ ìƒê´€ê´€ê³„"

- 0.60 ~ 0.79: "ê°•í•œ ìƒê´€ê´€ê³„"

- 0.80 ~ 1.0: "ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„"

```



- ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ë‘ ìƒê´€ê´€ê³„ëŠ” ë¹„ìŠ·í•¨

- **ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê´€ê³„**ëŠ” ì¢…ì¢… **ìˆœì„œí˜•** ë³€ìˆ˜ì— ëŒ€í•´ ë” ë‚˜ì€ ë°©ë²•ì´ë¼ê³  íŒë‹¨ë¨

  - ì‹¤ì œ ì„¸ê³„ì—ì„œ ëŒ€ë¶€ë¶„ì˜ ê´€ê³„ëŠ” ì„ í˜•ì ì´ì§€ ì•ŠìŒ

  - **Pearson ìƒê´€ê´€ê³„**ëŠ” ë‘ ë³€ìˆ˜ê°€ ì–¼ë§ˆë‚˜ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•œ ê·¼ì‚¬ì¹˜ì¼ ìˆœ ìˆì§€ë§Œ, ì •í™•í•˜ì§€ ì•Šê³  ê°€ì¥ ì¢‹ì€ ë¹„êµ ë°©ë²• ë˜í•œ ì•„ë‹˜



```python
from scipy.stats import spearmanr
```


```python
### ìƒê´€ê³„ìˆ˜ ì‹œê°í™”

def plot_corrs(x, y):
    
    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    spr = spearmanr(x, y).correlation
    pcr = np.corrcoef(x, y)[0, 1]
    
    # ì‚°ì ë„ plot
    data = pd.DataFrame({'x': x, 'y': y})
    plt.figure( figsize = (6, 4))
    sns.regplot(data = data, x = 'x', y = 'y', fit_reg = False)
    plt.title(f'Spearman: {round(spr, 2)}; Pearson: {round(pcr, 2)}')
```

---

- **Example**



```python
x = np.array(range(100))
y = x ** 2

plot_corrs(x, y)
```


```python
x = np.array([1, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 7, 8, 8, 9, 9, 9])
y = np.array([1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 3, 3, 2, 4, 2, 2, 4])

plot_corrs(x, y)
```


```python
x = np.array(range(-19, 20))
y = 2 * np.sin(x)

plot_corrs(x, y)
```

---

#### **ğŸ“Œ Pearson ìƒê´€ ê´€ê³„**



```python
### í•™ìŠµ(train) ë°ì´í„°ë§Œ ì‚¬ìš©
train_heads = heads.loc[heads['Target'].notnull(), :].copy()

pcorrs = pd.DataFrame(train_heads.corr()['Target'].sort_values()).rename(columns = {'Target': 'pcorr'}).reset_index()
pcorrs = pcorrs.rename(columns = {'index': 'feature'})

print('Most negatively correlated variables:')
print(pcorrs.head())

print('\nMost positively correlated variables:')
print(pcorrs.dropna().tail())
```

- **ìŒì˜ ìƒê´€ê´€ê³„**ì˜ ê²½ìš° ë³€ìˆ˜ì˜ ê°’ì´ ì¦ê°€í• ìˆ˜ë¡ targetê°’ì´ ê°ì†Œí•¨ì„ ì˜ë¯¸

  - ë¹ˆê³¤ ì‹¬ê°ë„ê°€ **ì¦ê°€**í•¨ì„ ì˜ë¯¸

- ```warning```ì´ ì¦ê°€í•¨ì— ë”°ë¼ ë¹ˆê³¤ ìˆ˜ì¤€ë„ ì¦ê°€

  - ì´ëŠ” ì§‘ì— ëŒ€í•œ ì ì¬ì ì¸ ë‚˜ìœ ì§•í›„ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ê²ƒì´ê¸° ë•Œë¬¸ì— íƒ€ë‹¹í•¨

- ```hogar_nin```

  - ì´ëŠ” ê°€ì¡± ë‚´ 0~19ëª…ì˜ ì•„ì´ë“¤ì˜ ìˆ«ìë¡œ, ë” ì–´ë¦° ì•„ì´ë“¤ì´ ë” ë†’ì€ ìˆ˜ì¤€ì˜ ë¹ˆê³¤ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ê°€ì¡±ì—ê²Œ ìŠ¤íŠ¸ë ˆìŠ¤ì˜ ì¬ì •ì ì¸ ì›ì¸ì´ ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸

  - ì‚¬íšŒ ê²½ì œì  ì§€ìœ„ê°€ ë‚®ì€ ê°€ì •ë“¤ì€ ê·¸ë“¤ ì¤‘ í•œ ëª…ì´ ì„±ê³µí•  ìˆ˜ ìˆê¸°ë¥¼ ë°”ë¼ëŠ” ë§ˆìŒìœ¼ë¡œ ë” ë§ì€ ì•„ì´ë“¤ì„ ê°€ì§€ëŠ” ê²½í–¥ì´ ìˆìŒ

  > ê°€ì¡± ê·œëª¨ì™€ ë¹ˆê³¤ ì‚¬ì´ì—ëŠ” ì‹¤ì§ˆì ì¸ ì—°ê´€ì„±ì´ ìˆë‹¤. 

---

- **ì–‘ì˜ ìƒê´€ê´€ê³„**ì˜ ê²½ìš°, ë³€ìˆ˜ì˜ ê°’ì´ ì¦ê°€í• ìˆ˜ë¡ Targetê°’ì´ ì¦ê°€í•¨ì„ ì˜ë¯¸

  - ë¹ˆê³¤ ì‹¬ê°ë„ê°€ **ê°ì†Œ**í•¨ì„ ì˜ë¯¸

- ```meaneduc```

  - ê°€êµ¬ ë‚´ ì„±ì¸ì˜ í‰ê·  êµìœ¡ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ê°€êµ¬ ìˆ˜ì¤€ì˜ ë³€ìˆ˜

  - targetê³¼ ê°€ì¥ **ë†’ì€** ì–‘ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì„

  > êµìœ¡ì˜ ë‚®ì€ ìˆ˜ì¤€ì€ ì¼ë°˜ì ìœ¼ë¡œ ë¹ˆê³¤ì˜ ë‚®ì€ ìˆ˜ì¤€ê³¼ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤. 



- ë³€ìˆ˜ì™€ target ê°„ì— **ì•½í•œ** ìƒê´€ê´€ê³„ê°€ ì¡´ì¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ


#### **ğŸ“Œ ìŠ¤í”¼ì–´ë§Œ ìƒê´€ ê´€ê³„**



```python
import warnings
warnings.filterwarnings('ignore', category = RuntimeWarning)

feats = [] # features
scorr = [] # scores
pvalues = [] # p-value

# ê° ì»¬ëŸ¼(ë³€ìˆ˜)ë³„ë¡œ
for c in heads:
    # ìˆ«ìí˜• ë³€ìˆ˜ë“¤ì— ëŒ€í•´..
    if heads[c].dtype != 'object':
        feats.append(c)
        
        # ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        scorr.append(spearmanr(train_heads[c], train_heads['Target']).correlation)
        pvalues.append(spearmanr(train_heads[c], train_heads['Target']).pvalue)

scorrs = pd.DataFrame({'feature': feats, 'scorr': scorr, 'pvalue': pvalues}).sort_values('scorr')
```

- p-valueê°€ **0.05 ë¯¸ë§Œ**ì´ë©´ ì¼ë°˜ì ìœ¼ë¡œ ìœ ì˜í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼ë¨

  - ìš°ë¦¬ëŠ” ë‹¤ì¤‘ ë¹„êµë¥¼ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— p-valueë¥¼ ë¹„êµ íšŸìˆ˜ë¡œ ë‚˜ëˆ„ê³ ì í•¨

  > Bonferroni ìˆ˜ì •ì´ë¼ í•¨



```python
print('Most negative Spearman correlations:')
print(scorrs.head())
print()
print('\nMost positive Spearman correlations:')
print(scorrs.dropna().tail())
```

- ìƒê´€ê´€ê³„ë¥¼ ì¸¡ì •í•˜ëŠ” ë‘ ê³„ìˆ˜ ëª¨ë‘ ë¹„ìŠ·í•œ ê²°ê³¼ë¥¼ ë³´ì´ê³  ìˆë‹¤.



```python
corrs = pcorrs.merge(scorrs, on = 'feature')
corrs['diff'] = corrs['pcorr'] - corrs['scorr'] # p-value - ìƒê´€ê³„ìˆ˜

corrs.sort_values('diff').head()
```


```python
corrs.sort_values('diff').dropna().tail()
```

- ```dependency``` ë³€ìˆ˜ê°€ ê°€ì¥ í° ì°¨ì´ë¥¼ ë³´ì„




```python
### ì‹œê°í™”
# ë‘ ë³€ìˆ˜ ëª¨ë‘ ì´ì‚°í˜• ë³€ìˆ˜ì´ê¸°ì—, plotì— ì•½ê°„ì˜ jitterë¥¼ ì¶”ê°€í•¨

sns.lmplot(x = 'dependency', y = 'Target', fit_reg = True, data = train_heads, 
           x_jitter = 0.05, y_jitter = 0.05)
plt.title('Target vs Dependency')
```

- ì•½í•œ **ìŒì˜ ìƒê´€ê´€ê³„**ë¥¼ ë³´ì„

  - ```dependency```ê°€ ì¦ê°€í• ìˆ˜ë¡ ```target``` ê°’ì´ ê°ì†Œí•¨

  - ```dependency```ê°€ (dependent)/(non-dependent)ë¥¼ ë‚˜íƒ€ëƒ„ì„ ì˜ë¯¸

  - í•´ë‹¹ ê°’ì´ ì¦ê°€í•˜ë©´, ë¹ˆê³¤ì˜ ì‹¬ê°ì„±ì´ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ

> (ë³´í†µ ì¼ì„ í•˜ì§€ ì•ŠëŠ”) ì˜ì¡´ì ì¸ ê°€ì¡± êµ¬ì„±ì›ì€ ë¹„ì˜ì¡´ì ì¸ ê°€ì¡± êµ¬ì„±ì›ì˜ ì§€ì›ì„ ë°›ì•„ì•¼ í•¨ => ë” ë†’ì€ ìˆ˜ì¤€ì˜ ë¹ˆê³¤ìœ¼ë¡œ ì´ì–´ì§




```python
sns.lmplot(x = 'rooms-per-capita', y = 'Target', fit_reg = True, data = train_heads, 
           x_jitter = 0.05, y_jitter = 0.05)
plt.title('Target vs Rooms Per Capita')
```

- ì•½í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì„


#### **ğŸ“Œìƒê´€ê³„ìˆ˜ heatmap** 

- ìƒê´€ê³„ìˆ˜ ì‹œê°í™”



```python
# Household Variables
variables = ['Target', 'dependency', 'warning', 'walls+roof+floor', 'meaneduc',
             'floor', 'r4m1', 'overcrowding']

# ìƒê´€ê³„ìˆ˜ ê³„ì‚°
corr_mat = train_heads[variables].corr().round(2)

# heatmap ìƒì„±
plt.rcParams['font.size'] = 18
plt.figure(figsize = (12, 12))

sns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, 
            cmap = plt.cm.RdYlGn_r, annot = True);
```

- targetê³¼ **ì•½í•œ ìƒê´€ê´€ê³„**ë¥¼ ê°€ì§€ëŠ” ë³€ìˆ˜ë“¤ì´ ìƒë‹¹ìˆ˜ ì¡´ì¬í•¨ì„ ë³´ì—¬ì¤Œ

- ì¼ë¶€ ë³€ìˆ˜ë“¤ì˜ ê²½ìš° ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ë„ê°€ ë†’ìŒ

  - ```floor``` vs ```walls+roof+floor```

  - ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity) ë¬¸ì œ


### **b) ë³€ìˆ˜ ì‹œê°í™”**

- ìœ„ìª½ ì‚¼ê°í˜•ì—ëŠ” ì‚°ì ë„(scatterplot), ëŒ€ê°ì„ ì—ëŠ” ì»¤ë„ ë°€ë„ plot(KDE), ì•„ë˜ìª½ ì‚¼ê°í˜•ì—ëŠ” 2ì°¨ì› KDE ê·¸ë¦¼ì„ í‘œì‹œí•´ë³´ì!



```python
import warnings
warnings.filterwarnings('ignore')

# ì‹œê°í™” í•  ë³€ìˆ˜ ì„ íƒ
plot_data = train_heads[['Target', 'dependency', 'walls+roof+floor',
                         'meaneduc', 'overcrowding']]

# ì˜ì—­ ë‚˜ëˆ„ê¸° -> pairgrid
grid = sns.PairGrid(data = plot_data, diag_sharey = False,
                    hue = 'Target', hue_order = [4, 3, 2, 1], 
                    vars = [x for x in list(plot_data.columns) if x != 'Target'])

# Upper: scatter plot
grid.map_upper(plt.scatter, alpha = 0.8, s = 20)

# Diagonal: kdeplot
grid.map_diag(sns.kdeplot)

# Bottom: 2ì°¨ì› kdeplot
grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r)
grid = grid.add_legend()

plt.suptitle('Feature Plots Colored By Target', size = 32, y = 1.05)
```


```python
household_feats = list(heads.columns) # ê°€êµ¬ ìˆ˜ì¤€ì—ì„œì˜ ë³€ìˆ˜ë“¤ì„ ìµœì¢…ì ìœ¼ë¡œ ì €ì¥
```

## **3-5. Individual Level Variables**

- ë‘ ê°€ì§€ ìœ í˜•ì´ ì¡´ì¬í•¨

  - booleaní˜• ë³€ìˆ˜(1 or 0)

  - ìˆœì„œí˜•(ordinal) ë³€ìˆ˜(ì˜ë¯¸ ìˆëŠ” ìˆœì„œê°€ ì§€ì •ëœ ê°œë³„ ê°’)



```python
ind = data[id_ + ind_bool + ind_ordered]
ind.shape
```

### **a) ì¤‘ë³µ ë³€ìˆ˜ ì œê±°í•˜ê¸°**

- ìƒê´€ ê³„ìˆ˜ì˜ ì ˆëŒ“ê°’ì´ **0.95**ë³´ë‹¤ í° ë³€ìˆ˜ì— ì£¼ëª©í•˜ì.



```python
# ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ìƒì„±
corr_matrix = ind.corr()

# ìƒì‚¼ê°í–‰ë ¬ë§Œ ë‚¨ê¸°ê¸°
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))

# ìƒê´€ê³„ìˆ˜ê°€ 0.95 ì´ìƒì¸ ë³€ìˆ˜ë“¤
to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]

to_drop
```

- ì´ëŠ” ë‹¨ìˆœíˆ ë‚¨ì„±ì˜ ë°˜ëŒ€ë¥¼ ì˜ë¯¸

  - ë‚¨ì„± flag ë³€ìˆ˜ë¥¼ ì œê±°



```python
ind = ind.drop(columns = 'male')
```

### **b) ìˆœì„œí˜• ë³€ìˆ˜ ìƒì„±í•˜ê¸°**

- ê¸°ì¡´ì˜ ë³€ìˆ˜ë“¤ì„ ìˆœì„œí˜• ë³€ìˆ˜ë¡œ ë§¤í•‘ ê°€ëŠ¥

  - ê°œì¸ì˜ êµìœ¡ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ```instlevel_``` ë³€ìˆ˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ 1(êµìœ¡ ìˆ˜ì¤€ì´ x)ë¶€í„° 9(ëŒ€í•™ì›)ê¹Œì§€ mapping



```python
ind[[c for c in ind if c.startswith('instl')]].head()
```


```python
ind['inst'] = np.argmax(np.array(ind[[c for c in ind if c.startswith('instl')]]), axis = 1)

plot_categoricals('inst', 'Target', ind, annotate = False);
```

- ë” ë†’ì€ ìˆ˜ì¤€ì˜ êµìœ¡ì„ ë°›ì„ìˆ˜ë¡ ëœ ê·¹ë‹¨ì ì¸ ìˆ˜ì¤€ì˜ ë¹ˆê³¤ì— í•´ë‹¹í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì„



```python
plt.figure(figsize = (10, 8))
sns.violinplot(x = 'Target', y = 'inst', data = ind)
plt.title('Education Distribution by Target');
```


```python
# Drop the education columns
# ind = ind.drop(columns = [c for c in ind if c.startswith('instlevel')])
ind.shape
```

### **c) ìƒˆë¡œìš´ ë³€ìˆ˜ ìƒì„±í•˜ê¸°**

- ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª‡ ê°€ì§€ ë³€ìˆ˜ë“¤ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ



```python
ind['escolari/age'] = ind['escolari'] / ind['age']

plt.figure(figsize = (10, 8))
sns.violinplot(x = 'Target', y = 'escolari/age', data = ind)
```


```python
ind['inst/age'] = ind['inst'] / ind['age']
ind['tech'] = ind['v18q'] + ind['mobilephone']
ind['tech'].describe()
```

## **3-6. ë³€ìˆ˜ ì§‘ê³„**

- ê°œë³„ ë°ì´í„°ë¥¼ ê°€êµ¬ ë°ì´í„°ì— í†µí•©í•˜ê¸° ìœ„í•´ì„œëŠ” ê°€êµ¬ë³„ ì§‘ê³„ê°€ í•„ìš”

  - ê°€ì¡± IDì¸ ```idhogar```ë¡œ ê·¸ë£¹í™” í›„ ë°ì´í„°ë¥¼ ```agg```ë¡œ ê·¸ë£¹í™”



```python
### ì§‘ê³„(aggregation)ë¥¼ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜
range_ = lambda x: x.max() - x.min() # í•œ ì¤„ì§œë¦¬ í•¨ìˆ˜ëŠ” ì£¼ë¡œ lambda ì‹ìœ¼ë¡œ êµ¬í˜„
range_.__name__ = 'range_'

# ê·¸ë£¹í™” & ì§‘ê³„
ind_agg = ind.drop(columns = ['Target','Id']).groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std', range_])
ind_agg.head()
```

- ë³€ìˆ˜ê°€ 30ê°œì—ì„œ 180ê°œê°€ ë˜ì—ˆë‹¤..



```python
### ë³€ìˆ˜ ì´ë¦„ ì¬ì •ì˜

new_col = []
for c in ind_agg.columns.levels[0]:
    for stat in ind_agg.columns.levels[1]:
        new_col.append(f'{c}-{stat}')
        
ind_agg.columns = new_col
ind_agg.head()
```


```python
ind_agg.iloc[:, [0, 1, 2, 3, 6, 7, 8, 9]].head()
```

## **3-7. ë³€ìˆ˜ ì„ íƒ**

- ìƒê´€ ê´€ê³„ê°€ **0.95ë³´ë‹¤ í°** ë³€ìˆ˜ë“¤ì˜ ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì œê±°



```python
# Create correlation matrix
corr_matrix = ind_agg.corr()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find index of feature columns with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]

print(f'There are {len(to_drop)} correlated columns to remove.')
```

- ë³€ìˆ˜ë¥¼ ì œê±°í•˜ê³ , ```head``` ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ ìµœì¢… ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±



```python
ind_agg = ind_agg.drop(columns = to_drop)
ind_feats = list(ind_agg.columns)

# Merge on the household id
final = heads.merge(ind_agg, on = 'idhogar', how = 'left')

print('Final features shape: ', final.shape)
```


```python
final.head()
```

## **3-8. ìµœì¢…ì ì¸ ë°ì´í„° íƒìƒ‰**



### **a) ìƒê´€ê³„ìˆ˜ í™•ì¸**



```python
corrs = final.corr()['Target']
```


```python
corrs.sort_values().head()
```


```python
corrs.sort_values().dropna().tail()
```

- ìƒì„±ëœ ë³€ìˆ˜ ì¤‘ ì¼ë¶€ê°€ target ë³€ìˆ˜ì™€ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

- í•´ë‹¹ ë³€ìˆ˜ê°€ ì‹¤ì œë¡œ ìœ ìš©í•œì§€ëŠ” ëª¨ë¸ë§ ë‹¨ê³„ì—ì„œ íŒë‹¨í•  ì˜ˆì •



### **b) escolari ë³€ìˆ˜**



```python
plot_categoricals('escolari-max', 'Target', final, annotate = False)
```


```python
plt.figure(figsize = (10, 6))
sns.violinplot(x = 'Target', y = 'escolari-max', data = final)
plt.title('Max Schooling by Target')
```


```python
plt.figure(figsize = (10, 6))
sns.boxplot(x = 'Target', y = 'escolari-max', data = final)
plt.title('Max Schooling by Target')
```

### **c) meaneduc ë³€ìˆ˜**



```python
plt.figure(figsize = (10, 6))
sns.boxplot(x = 'Target', y = 'meaneduc', data = final)
plt.xticks([0, 1, 2, 3], poverty_mapping.values())
plt.title('Average Schooling by Target')
```

### **d) Overcrowing ë³€ìˆ˜**



```python
plt.figure(figsize = (10, 6))
sns.boxplot(x = 'Target', y = 'overcrowding', data = final)
plt.xticks([0, 1, 2, 3], poverty_mapping.values())
plt.title('Overcrowding by Target')
```

### **e) ê°€ì¥ì˜ ì„±ë³„**



```python
head_gender = ind.loc[ind['parentesco1'] == 1, ['idhogar', 'female']]
final = final.merge(head_gender, on = 'idhogar', how = 'left').rename(columns = {'female': 'female-head'})
```


```python
final.groupby('female-head')['Target'].value_counts(normalize=True)
```

- ê°€ì¥ì´ **ì—¬ì„±**ì¸ ê°€êµ¬ëŠ” ë¹ˆê³¤ ìˆ˜ì¤€ì•„ ì‹¬ê°í•  ê°€ëŠ¥ì„±ì´ ì•½ê°„ ë” **ë†’ì€** ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.



```python
sns.violinplot(x = 'female-head', y = 'Target', data = final)
plt.title('Target by Female Head of Household');
```

**ê°€ì¥ì˜ ì„±ë³„ì— ë”°ë¥¸ í‰ê·  êµìœ¡ ìˆ˜ì¤€ ì°¨ì´**



```python
plt.figure(figsize = (8, 8))
sns.boxplot(x = 'Target', y = 'meaneduc', hue = 'female-head', data = final)
plt.title('Average Education by Target and Female Head of Household', size = 16)
```

- ì—¬ì„± ê°€ì¥ì„ ë‘” ê°€êµ¬ì¼ìˆ˜ë¡ êµìœ¡ ìˆ˜ì¤€ì´ ë†’ì€ ê²ƒìœ¼ë¡œ ë³´ì„

- ê·¸ëŸ¬ë‚˜ ì „ì²´ì ìœ¼ë¡œ ì—¬ì„±ì´ ê°€ì¥ì¸ ê°€êµ¬ëŠ” ì‹¬ê°í•œ ë¹ˆê³¤ì„ ê²ªì„ ê°€ëŠ¥ì„±ì´ ë” ë†’ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ



```python
final.groupby('female-head')['meaneduc'].agg(['mean', 'count'])
```

- ì „ì²´ì ìœ¼ë¡œ ì—¬ì„± ê°€ì¥ì´ ìˆëŠ” ê°€êµ¬ì˜ í‰ê·  êµìœ¡ ìˆ˜ì¤€ì€ ë‚¨ì„± ê°€ì¥ì´ ìˆëŠ” ê°€êµ¬ë³´ë‹¤ ì•½ê°„ ë†’ìŒ


# **4. Baseline Model**

- ëª¨ë“  ë°ì´í„°(train/test)ëŠ” ê° ê°€êµ¬ì— ëŒ€í•´ ì§‘ê³„ë˜ë¯€ë¡œ ëª¨ë¸ì— ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

- sklearnì˜ ```RandomForest```ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ë§ ê¸°ì¤€ ì°¾ê¸°

- ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•´ train ë°ì´í„°ì— ```10-fold êµì°¨ ê²€ì¦```ì„ í™œìš©

  - ë°ì´í„°ë¥¼ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì§‘í•©ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ëª¨ë¸ì„ ì´ **10ë²ˆ** í›ˆë ¨

- êµì°¨ ê²€ì¦ì˜ í‰ê·  ì„±ëŠ¥ê³¼ í‘œì¤€ í¸ì°¨ë¥¼ ì¡°ì‚¬í•˜ì—¬ fold ê°„ì— ì ìˆ˜ê°€ ì–¼ë§ˆë‚˜ ë³€í•˜ëŠ”ì§€ í™•ì¸

  - ```Fl Macro Score```ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ í‰ê°€



```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, make_scorer
from sklearn.model_selection import cross_val_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline

# êµì°¨ ê²€ì¦ì„ ìœ„í•´ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ë§Œë“¤ê¸°
scorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')
```


```python
# í•™ìŠµì„ ìœ„í•œ label(target) ê°’
train_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))

# train/ test set ì¤€ë¹„
train_set = final[final['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])
test_set = final[final['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])

# ì œì¶œìš© ì–‘ì‹ ë§Œë“¤ê¸°
submission_base = test[['Id', 'idhogar']].copy()
```

## **4-1. Pipelining**


- ì—¬ëŸ¬ ëª¨í˜•ì„ ë¹„êµí•˜ê¸° ìœ„í•´ featureë“¤ ê°„ì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •

  - ê° ì»¬ëŸ¼(ë³€ìˆ˜)ì˜ ë²”ìœ„ë¥¼ ```0ê³¼ 1 ì‚¬ì´```ë¡œ ì œí•œ

  - ë§ì€ ì•™ìƒë¸” ëª¨ë¸ì˜ ê²½ìš° ë¶ˆí•„ìš”í•˜ì§€ë§Œ, **KNearest Neighbors** ë˜ëŠ” **Support Vector Machine**ê³¼ ê°™ì´ ê±°ë¦¬ ë©”íŠ¸ë¦­ì— ì˜ì¡´í•˜ëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš° ```feature scaling```ì´ì ˆëŒ€ì ìœ¼ë¡œ í•„ìš”

  - ê²°ì¸¡ì¹˜ì˜ ê²½ìš° featureì˜ **ì¤‘ì•™ê°’**ìœ¼ë¡œ ëŒ€ì²´

- ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³  featureë“¤ì„ í•œ ë²ˆì— scaling í•˜ê¸° ìœ„í•´ ```Pipeline```ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ

  - train ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì í•©í•˜ê³  train ë° test ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©



```python
features = list(train_set.columns)

pipeline = Pipeline([('imputer', SimpleImputer(strategy = 'median')), 
                      ('scaler', MinMaxScaler())])

# ë°ì´í„¸ë¥¼ ì•Œë§ê²Œ ë³€í™˜(ì „ì²˜ë¦¬)
train_set = pipeline.fit_transform(train_set)
test_set = pipeline.transform(test_set)
```

- ë°ì´í„°ì—ëŠ” ê²°ì¸¡ê°’ì´ ì—†ìœ¼ë©°, 0ê³¼ 1 ì‚¬ì´ ë²”ìœ„ë¡œ scaling ë¨

  - scikit-Learn ëª¨ë¸ì—ì„œ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ



```python
### ëª¨ë¸ í•™ìŠµ

model = RandomForestClassifier(n_estimators = 100, random_state = 10, n_jobs = -1)
# 10 fold cross validation
cv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)

print(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')
```

- í˜„ì¬ëŠ” ì„±ëŠ¥ì´ ê·¸ë‹¤ì§€ ì¢‹ì§€ëŠ” ì•ŠìŒ


## **4-2. í”¼ì³ ì¤‘ìš”ë„ í™•ì¸**

- **íŠ¸ë¦¬ ê¸°ë°˜** ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì˜ ê¸°ëŠ¥ ìœ ìš©ì„±ì— ëŒ€í•œ ìƒëŒ€ì  ìˆœìœ„ë¥¼ ë³´ì—¬ì£¼ëŠ” ```ê¸°ëŠ¥ ì¤‘ìš”ë„(feature importances)```ë¥¼ ì‚´í´ë³¼ ìˆ˜ ìˆìŒ

  - ë¶„í• ì„ ìœ„í•´ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•œ ë…¸ë“œì˜ ë¶ˆìˆœë¬¼ ê°ì†Œì˜ í•©ì„ ì˜ë¯¸

  - ìƒëŒ€ ì ìˆ˜ì— ì´ˆì 

- ```feature importances```ë¥¼ ë³´ê¸° ìœ„í•´ì„  **ì „ì²´** train ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ì„ trainì‹œì¼œì•¼ í•¨

- êµì°¨ ê²€ì¦ì˜ ê²½ìš° feature importanceë¥¼ ë°˜í™˜í•˜ì§€ x



```python
model.fit(train_set, train_labels) # ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ í•™ìŠµ

# Feature importances í™•ì¸
feature_importances = pd.DataFrame({'feature': features, 'importance': model.feature_importances_})
feature_importances.head()
```

**ğŸ“Œ ê¸°ëŠ¥ ì¤‘ìš”ë„ ì‹œê°í™” í•¨ìˆ˜**  

- ì¤‘ìš”í•œ ìˆœì„œëŒ€ë¡œ nê°œì˜ í”¼ì³ ì‹œê°í™”

- ì„ê³„ê°’(threshold)ì´ ì§€ì •ëœ ê²½ìš° **ëˆ„ì  ì¤‘ìš”ë„**ë¥¼ í‘œì‹œí•˜ê³ , ëˆ„ì  ì¤‘ìš”ë„ê°€ ì„ê³„ê°’ì— ë„ë‹¬í•˜ëŠ” ë° í•„ìš”í•œ í”¼ì³ ìˆ˜ë¥¼ print

- íŠ¸ë¦¬ ê¸°ë°˜ ê¸°ëŠ¥ ì¤‘ìš”ë„ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë„ë¡ ì„¤ê³„ë¨

---

- Arguments>  

  - ```df(dataframe)```

    - í”¼ì²˜ ì¤‘ìš”ë„ì˜ ë°ì´í„° í”„ë ˆì„

    - ì—´ì€ **ê¸°ëŠ¥** ë° **ì¤‘ìš”ë„**ì—¬ì•¼ í•¨

  - ```n(int)```

    - ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ í‘œì‹œí•  í”¼ì³ ìˆ˜

    - default = 15

  - ```threshold(float)```

    - ëˆ„ì  ì¤‘ìš”ë„ plotì˜ ì„ê³„ê°’

    - ì„ê³„ê°’ì´ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ plotì´ ìƒì„±ë˜ì§€ x

    - default: None

- Returns>  

  - ```df(dataframe)```

    - ì •ê·œí™”ëœ ì»¬ëŸ¼(í•©ê³„ = 1)ê³¼ ëˆ„ì  ì¤‘ìš”ë„ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ í”¼ì³ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„° í”„ë ˆì„

---

**(Note)**  

- ì´ ê²½ìš° ì •ê·œí™”ëŠ” í•©ì´ 1ì´ë¼ëŠ” ê²ƒì„ ì˜ë¯¸

- ëˆ„ì  ì¤‘ìš”ë„ëŠ” ê°€ì¥ ì¤‘ìš”í•˜ì§€ ì•Šì€ featureë¶€í„° ê°€ì¥ ì¤‘ìš”í•˜ì§€ ì•Šì€ featureë¥¼ í•©ì‚°í•˜ì—¬ ê³„ì‚°

- ```threshold = 0.9```: ëˆ„ì  ì¤‘ìš”ë„ì˜ 90%ì— ë„ë‹¬í•˜ëŠ” ë° í•„ìš”í•œ ê°€ì¥ ì¤‘ìš”í•œ featureë¥¼ í‘œì‹œ



```python
def plot_feature_importances(df, n = 10, threshold = None):

    plt.style.use('fivethirtyeight')
    
    # ê°€ì¥ ì¤‘ìš”í•œ ê¸°ëŠ¥ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    df = df.sort_values('importance', ascending = False).reset_index(drop = True)
    
    # í”¼ì³ ì¤‘ìš”ë„ë¥¼ ì •ê·œí™”(0 ~ 1 ì‚¬ì´)í•˜ì—¬ ëˆ„ì  ì¤‘ìš”ë„ ê³„ì‚°
    df['importance_normalized'] = df['importance'] / df['importance'].sum()
    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])
    
    plt.rcParams['font.size'] = 12
    
    # ì¤‘ìš”í•œ ìˆœì„œëŒ€ë¡œ nê°œ featureì— ëŒ€í•œ barhplot
    df.loc[:n, :].plot.barh(y = 'importance_normalized', 
                            x = 'feature', color = 'darkgreen', 
                            edgecolor = 'k', figsize = (12, 8),
                            legend = False, linewidth = 2)

    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); 
    plt.title(f'{n} Most Important Features', size = 18)
    plt.gca().invert_yaxis()
    
    ### ------------------------------------------------------------------------
    
    # ì„ê³„ê°’ì— ë‹¤ë‹¤ëë‹¤ë©´
    if threshold:
        # ëˆ„ì  ì¤‘ìš”ë„ plot
        plt.figure(figsize = (8, 6))
        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')
        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16)
        plt.title('Cumulative Feature Importance', size = 18)
        
        # ëˆ„ì  ì¤‘ìš”ë„ì— ë„ë‹¬í•˜ê¸° ìœ„í•œ feature ìˆ˜
        # ì¸ë±ìŠ¤(-> ì‹¤ì œ ìˆ«ìì— ëŒ€í•´ 1ì„ ì¶”ê°€í•´ì•¼ í•¨)
        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))
        
        # ìˆ˜ì§ ì„  ì¶”ê°€
        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')
        plt.show()
        
        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, 
                                                                                  100 * threshold))
    
    return df
```


```python
norm_fi = plot_feature_importances(feature_importances, threshold=0.95)
```

- ê°€ì¥ ì¤‘ìš”í•œ ë³€ìˆ˜ëŠ” ```meaneduc(ê°€êµ¬ ë‚´ í‰ê·  êµìœ¡ëŸ‰)```ì´ê³ , ë‹¤ìŒì´ ```age-max(ê°€êµ¬ ë‚´ í•œ ê°œì¸ì˜ ìµœëŒ€ êµìœ¡ëŸ‰)```ì„

  - ì´ ë‘ ë³€ìˆ˜ëŠ” ìƒê´€ì„±ì´ **ë†’ì€** ë³€ìˆ˜ë“¤ì„

  - ë”°ë¼ì„œ, ë°ì´í„°ì—ì„œ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ë¥¼ ì œê±°í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸

- ë‹¤ë¥¸ ì¤‘ìš”í•œ featureë“¤ì€ ìš°ë¦¬ê°€ ë§Œë“  ë³€ìˆ˜ì™€ ë°ì´í„°ì— ì´ë¯¸ ì¡´ì¬í–ˆë˜ ë³€ìˆ˜ë“¤ê³¼ì˜ ì¡°í•©ìœ¼ë¡œ ìƒì„±ëœ ë³€ìˆ˜ë“¤ì„



- 90%ì˜ ì¤‘ìš”ë„ë¥¼ ìœ„í•´ì„œëŠ” ëŒ€ëµ 180ê°œ ì •ë„ì˜ featureë“¤ë§Œ ì¡´ì¬í•´ë„ ok

  - ì¼ë¶€ featureë“¤ì„ ì œê±°í•´ë„ ë¬´ë°©í•¨

- í”¼ì²˜ ì¤‘ìš”ë„ì€ í”¼ì²˜ê°€ ì–´ëŠ ë°©í–¥ìœ¼ë¡œ ì¤‘ìš”í•œì§€ë¥¼ ì•Œë ¤ì£¼ì§€ëŠ” ì•ŠìŒ

  - ì˜ˆë¥¼ ë“¤ì–´, êµìœ¡ì„ ë§ì´ ë°›ì„ìˆ˜ë¡ ëœ ì‹¬ê°í•œ ë¹ˆê³¤ìœ¼ë¡œ ì´ì–´ì§€ëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ì§€ëŠ” ëª»í•¨

  - ê´€ë ¨ì´ ìˆì„ ê²ƒìœ¼ë¡œ ê°„ì£¼ë˜ëŠ” ëª¨ë¸ë§Œ ì•Œë ¤ì¤Œ



```python
### ì»¤ë„ ë°€ë„ í•¨ìˆ˜ ì‹œê°í™”
# "variable" ë³„ë¡œ "target" ê°’ì˜ ë¶„í¬ë¥¼ í‘œì‹œ

def kde_target(df, variable):
    
    colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}

    plt.figure(figsize = (12, 8))
    
    df = df[df['Target'].notnull()]
    
    for level in df['Target'].unique():
        subset = df[df['Target'] == level].copy()
        sns.kdeplot(subset[variable].dropna(), 
                    label = f'Poverty Level: {level}', 
                    color = colors[int(subset['Target'].unique())])

    plt.xlabel(variable); plt.ylabel('Density');
    plt.title('{} Distribution'.format(variable.capitalize()));
```


```python
kde_target(final, 'meaneduc')
```


```python
kde_target(final, 'escolari/age-range_')
```

## **4-2. ëª¨ë¸ ì„ íƒ**

- ì´ë¯¸ **RandomForestClassifier**ëŠ” ì‹œë„

  - Macro F1-score: 0.35

- ê¸°ê³„ í•™ìŠµì—ì„œëŠ” ì–´ë–¤ ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„° ì„¸íŠ¸ì— ê°€ì¥ ì˜ ì‘ë™í•˜ëŠ”ì§€ ë¯¸ë¦¬ ì•Œ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ìŒ

  - ê° ìƒí™©ë§ˆë‹¤ ë‹¤ë¦„..

  - ì–´ë–¤ ê²ƒì´ ìµœì ì¸ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ì„œëŠ” ì—¬ëŸ¬ ê°€ì§€ ëª¨ë¸ì„ ì‹œë„í•´ ë³´ì•„ì•¼ í•¨



![algorithm_comparison](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/algorithm_comparison.png)






```python
# Model imports

from sklearn.svm import LinearSVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import ExtraTreesClassifier
```


```python
import warnings 
from sklearn.exceptions import ConvergenceWarning

# Filter out warnings from models
warnings.filterwarnings('ignore', category = ConvergenceWarning)
warnings.filterwarnings('ignore', category = DeprecationWarning)
warnings.filterwarnings('ignore', category = UserWarning)

# ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ data frame
model_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])
```

**ğŸ“Œ ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ í•¨ìˆ˜**  

- ```RandomForestClassifier``` ì™¸ì— 8ê°œì˜ ë‹¤ë¥¸ Scikit - Learn ëª¨ë¸ ì‚¬ìš©

- ê²°ê³¼ë¥¼ ì €ì¥í•  ë°ì´í„° í”„ë ˆì„ì„ ë§Œë“¤ê³ , í•¨ìˆ˜ëŠ” ê° ëª¨ë¸ì˜ ë°ì´í„° í”„ë ˆì„ì— í–‰ì„ ì¶”ê°€í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰

- ê° ëª¨ë¸ì— ëŒ€í•´ **10-fold** êµì°¨ ê²€ì¦ ìˆ˜í–‰



```python
def cv_model(train, train_labels, model, name, model_results = None):
    
    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)
    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')
    
    if model_results is not None:
        # ê²°ê³¼ ì €ì¥
        model_results = model_results.append(pd.DataFrame({'model': name, 
                                                           'cv_mean': cv_scores.mean(), 
                                                            'cv_std': cv_scores.std()},
                                                           index = [0]),ignore_index = True)

        return model_results
```


```python
### 1. Linear SVC

model_results = cv_model(train_set, train_labels, 
                         LinearSVC(), 'LSVC', model_results)
```

- ì„±ëŠ¥ì´ ê·¸ë‹¥ ì¢‹ì§€ ì•ŠìŒ

  - ëª©ë¡ì—ì„œ ì‚­ì œí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ ì¤‘ í•˜ë‚˜



```python
### 2. Gaussian Naive Bayes

model_results = cv_model(train_set, train_labels, 
                         GaussianNB(), 'GNB', model_results)
```

- ë§¤ìš° ë‚˜ìœ ì„±ëŠ¥..




```python
### 3. Multi-Layer Perceptron

model_results = cv_model(train_set, train_labels, 
                         MLPClassifier(hidden_layer_sizes=(32, 64, 128, 64, 32)),
                         'MLP', model_results)
```

- ê´œì°®ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŒ

  - í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆëŠ” ê²½ìš° í•´ë‹¹ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

  - í•˜ì§€ë§Œ ì œí•œëœ ì–‘ì˜ ë°ì´í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ì„  ìˆ˜ì‹­ë§Œ ê°œì˜ ì˜ˆì œë¥¼ í•„ìš”ë¡œ í•˜ê¸° ë•Œë¬¸ì— ì‹ ê²½ë§ì— ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŒ



```python
### 4. LinearDiscriminantAnalysis

model_results = cv_model(train_set, train_labels, 
                          LinearDiscriminantAnalysis(), 
                          'LDA', model_results)
```

- UserWarningì„ filteringí•˜ì§€ ì•Šê³  LDAë¥¼ ì§„í–‰ ì‹œ ì—ëŸ¬ ë°œìƒ

  - ```Variables are collinear.``` : ê³µì„ ì„± ë¬¸ì œ

- ì„ í˜• ë³€ìˆ˜ë¥¼ ì œê±°í•œ í›„ í•´ë‹¹ ëª¨ë¸ì„ ë‹¤ì‹œ ì‹œë„í•´ ë³¼ ìˆ˜ ìˆìŒ



```python
### 5. Ridge

model_results = cv_model(train_set, train_labels, 
                         RidgeClassifierCV(), 'RIDGE', model_results)
```

- **ì„ í˜•** ëª¨í˜•(ridge ëª¨ë¸ í¬í•¨)ì€ ë†€ë¼ìš¸ ì •ë„ë¡œ ì˜ ì‘ë™í•¨

  - ë‹¨ìˆœí•œ ëª¨ë¸ì´ ì´ ë¬¸ì œì—ì„œëŠ” ë” ë„ì›€ì´ ë  ìˆ˜ë„ ìˆìŒì„ ì˜ë¯¸



```python
### 6. K-Neighbors

for n in [5, 10, 20]:
    print(f'\nKNN with {n} neighbors\n')
    model_results = cv_model(train_set, train_labels, 
                             KNeighborsClassifier(n_neighbors = n),
                             f'knn-{n}', model_results)
```


```python
### 7. ExtraTreeClassifier

model_results = cv_model(train_set, train_labels, 
                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),
                         'EXT', model_results)
```

## **4-3. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ**

- ëª¨ë¸ë§ ê²°ê³¼ê°€ ì €ì¥ëœ dfë¥¼ í™œìš©í•´ ì–´ë–¤ ëª¨ë¸ì´ ê°€ì¥ íš¨ê³¼ì ì¸ì§€ ì‹œê°í™”í•˜ëŠ” ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆìŒ



```python
# ì²˜ìŒìœ¼ë¡œ ìˆ˜í–‰í•œ RandomForestClassifierì˜ ê²°ê³¼ë„ ì €ì¥í•´ì£¼ê¸°

model_results = cv_model(train_set, train_labels,
                          RandomForestClassifier(100, random_state=10),
                              'RF', model_results)
```


```python
### ëª¨ë¸ë§ ê²°ê³¼ ì‹œê°í™”

model_results.set_index('model', inplace = True)
model_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),
                                  yerr = list(model_results['cv_std']),
                                  edgecolor = 'k', linewidth = 2)
plt.title('Model F1 Score Results')
plt.ylabel('Mean F1 Score (with error bar)')

model_results.reset_index(inplace = True)
```

- ```RandomForestClassifier```ëŠ” ê°€ì¥ ê°„ë‹¨í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê¸°ì—, ê°€ì¥ ë¨¼ì € ì‹œë„í•´ ë³¼ ìˆ˜ ìˆëŠ” ëª¨í˜•ì„

- ì•„ì§ baseline modelë“¤ì˜ ê²½ìš° í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì§€ ì•Šì•„ ëª¨ë¸ ê°„ ë¹„êµê°€ ì™„ë²½í•˜ì§€ëŠ” ì•Šì§€ë§Œ, **íŠ¸ë¦¬ ê¸°ë°˜** ì•™ìƒë¸” ë°©ë²•(```Gradient Boosting Machine``` í¬í•¨)ì´ **êµ¬ì¡°í™”ëœ** ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë§¤ìš° ì˜ ìˆ˜í–‰ëœë‹¤ëŠ” ë‹¤ë¥¸ ë§ì€ Kaggle competitionì˜ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ê³  ìˆìŒ





**```í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¡°ì • íš¨ê³¼```**  

![hyperparameter_improvement](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/hyperparameter_improvement.png)



- ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì •í™•ë„ í–¥ìƒì€ **10% ë¯¸ë§Œ**

  - ìµœì•…ì˜ ëª¨ë¸ì€ íŠœë‹ì„ í†µí•´ ê°‘ìê¸° ìµœê³ ì˜ ëª¨ë¸ì´ ë˜ì§€ëŠ” ì•Šì„ ê²ƒì„


- ì¼ë‹¨ì€ ê·¸ëƒ¥ **RandomForestClassifier**ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰


## **4-4. ì œì¶œ íŒŒì¼ ìƒì„±**

- ì œì¶œí•˜ê¸° ìœ„í•´ì„œëŠ” test ë°ì´í„°ê°€ í•„ìš”í•¨

  - í˜„ì¬ test dataê°€ train dataì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ format ë˜ì–´ ìˆìŒ

- ê°€êµ¬ë³„ë¡œ ì˜ˆì¸¡ì„ í•˜ê³  ìˆì§€ë§Œ ì‹¤ì œë¡œëŠ” ê°œì¸ë‹¹ í•œ ì¤„(Idë¡œ ì‹ë³„)ë§Œ í•„ìš”

  - **ê°€ì¥(household)**ì— ëŒ€í•œ ì˜ˆì¸¡ë§Œ ì ìˆ˜ê°€ ë§¤ê²¨ì§



- í…ŒìŠ¤íŠ¸ ì œì¶œ í˜•ì‹



```

Id,Target

ID_2f6873615,1

ID_1c78846d2,2

ID_e5442cf6a,3

ID_a8db26a79,4

ID_a62966799,4 

```



- ```submission_base```ëŠ” ê° ê°œì¸ì— ëŒ€í•œ **ì˜ˆì¸¡**ì´ ìˆì–´ì•¼ í•¨

  - ```submission_base```ëŠ” ëª¨ë“  ê°œì¸ì„ test setì— í¬í•¨

- ```test_ids```: ê°€ì¥ì˜ ```idhogar```ë§Œ í¬í•¨



- ì˜ˆì¸¡ ì‹œì—ëŠ” ê° ê°€êµ¬ì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡í•œ ë‹¤ìŒ **ì˜ˆì¸¡** ë°ì´í„° í”„ë ˆì„ì„ ê°€êµ¬ ID(```idhogar```)ì˜ ëª¨ë“  ê°œì¸ê³¼ ë³‘í•©

  - ```target```ì´ ê°€êµ¬ì› ëª¨ë‘ì—ê²Œ **ë™ì¼í•œ** ê°’ìœ¼ë¡œ ì„¤ì •ë¨

  - ê°€ì¥ì´ ì—†ëŠ” ê°€êµ¬ì˜ ê²½ìš° ì ìˆ˜ê°€ ë§¤ê²¨ì§€ì§€ ì•Šìœ¼ë¯€ë¡œ ì´ëŸ¬í•œ ê²½ìš°ì—ëŠ” ì˜ˆì¸¡ì¹˜ë¥¼ **4(non-vulnerable)**ë¡œ ì„¤ì •



```python
test_ids = list(final.loc[final['Target'].isnull(), 'idhogar'])
```

**ğŸ“Œ ì˜ˆì¸¡ì„ ìœ„í•œ í•¨ìˆ˜**  

- ëª¨ë¸, train ì„¸íŠ¸, train label ë° test ì„¸íŠ¸ë¥¼ ê°€ì ¸ì™€ ë‹¤ìŒ ì‘ì—…ë“¤ì„ ìˆ˜í–‰

  - ```fit()```ì„ í™œìš©í•˜ì—¬ train ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ì„ í•™ìŠµ

  - ```predict()```ë¥¼ í™œìš©í•˜ì—¬ test ë°ì´í„°ë¡œ ì˜ˆì¸¡

  - ì´ë¥¼ ì €ì¥í•˜ì—¬ ì œì¶œ íŒŒì¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ **submission** ë°ì´í„° í”„ë ˆì„ ìƒì„±



```python
def submit(model, train, train_labels, test, test_ids):
    
    model.fit(train, train_labels) # í•™ìŠµ
    predictions = model.predict(test) # ì˜ˆì¸¡
    predictions = pd.DataFrame({'idhogar': test_ids,
                               'Target': predictions})

    # ì œì¶œìš© df ë§Œë“¤ê¸°
    submission = submission_base.merge(predictions, 
                                       on = 'idhogar',
                                       how = 'left').drop(columns = ['idhogar'])
    
    # ê°€ì¥ x -> 4ë¡œ ì±„ìš°ê¸°
    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)

    return submission 
```


```python
### RandomForestë¡œ ì˜ˆì¸¡

rf_submission = submit(RandomForestClassifier(n_estimators = 100, 
                                              random_state=10, n_jobs = -1), 
                         train_set, train_labels, test_set, test_ids)

rf_submission.to_csv('rf_submission.csv', index = False)
```

- ì˜ˆì¸¡ ì„±ëŠ¥: **0.370**


## **4-5. ë³€ìˆ˜ ì„ íƒ**

- ëª¨ë¸ ì„±ëŠ¥ì„ **í–¥ìƒ**ì‹œí‚¤ëŠ” í•œ ê°€ì§€ ë°©ë²•

  - ëª¨ë¸ì— ê°€ì¥ **ìœ ìš©í•œ** ê¸°ëŠ¥ë§Œ ìœ ì§€í•˜ë ¤ê³  í•˜ëŠ” í”„ë¡œì„¸ìŠ¤

- í•´ë‹¹ ë…¸íŠ¸ë¶ì—ì„œëŠ” featureë¥¼ ì„ íƒí•˜ëŠ” ê²½ìš° ë¨¼ì € ìƒê´€ ê´€ê³„ê°€ **0.95ë³´ë‹¤ í°** ì—´ì„ ì œê±°í•œ ë‹¤ìŒ,  scikit-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì¤‘ë³µì ì¸ feature ì œê±°ë¥¼ ì ìš©



### **a) ìƒê´€ë„ê°€ ë†’ì€ ë³€ìˆ˜ ì œê±°**

- ìƒê´€ê³„ìˆ˜ê°€ **0.95** ì´ìƒì¸ ë³€ìˆ˜ë“¤ì„ ì œê±°



```python
### ìƒê´€ê³„ìˆ˜ê°€ 0.95 ì´ìƒì¸ ì»¬ëŸ¼ í™•ì¸

train_set = pd.DataFrame(train_set, columns = features)

# ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ìƒì„±
corr_matrix = train_set.corr()

# ìƒì‚¼ê°í–‰ë ¬ë§Œ ì„ íƒ
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# ìƒê´€ê³„ìˆ˜ê°€ 0.95 ì´ìƒì¸ ì»¬ëŸ¼ë§Œ ì„ íƒ
to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]

to_drop
```


```python
# í•´ë‹¹ ì»¬ëŸ¼ drop

train_set = train_set.drop(columns = to_drop)
train_set.shape
```


```python
# test dataì—ì„œë„ í•´ë‹¹ featureë“¤ì„ ì œê±°

test_set = pd.DataFrame(test_set, columns = features)
train_set, test_set = train_set.align(test_set, axis = 1, join = 'inner')

features = list(train_set.columns)
```

### **b) RandomForestë¥¼ í™œìš©í•˜ì—¬ ì¤‘ë³µ ë³€ìˆ˜ ì œê±°**

- ```sklearn.RFECV```

  - êµì°¨ ê²€ì¦ì„ í†µí•œ ì¤‘ë³µì ì¸ feature ì œê±°ë¥¼ ì˜ë¯¸

  - ë°˜ë³µì ì¸ ë°©ì‹ìœ¼ë¡œ í”¼ì²˜ ì¤‘ìš”ë„ê°€ ìˆëŠ” ëª¨ë¸ì„ ì‚¬ìš©

  - ê° ë°˜ë³µë§ˆë‹¤ í”¼ì²˜ì˜ ì¼ë¶€ ë˜ëŠ” ì„¤ì •ëœ ê°œìˆ˜ì˜ í”¼ì²˜ë¥¼ ì œê±°

  - êµì°¨ ê²€ì¦ ì ìˆ˜ê°€ ë” ì´ìƒ í–¥ìƒë˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ì‘ì—…ì„ ê³„ì† ë°˜ë³µí•¨



- **selector** ê°ì²´ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ëª¨ë¸, ê° ë°˜ë³µë§ˆë‹¤ ì œê±°í•  featureì˜ ìˆ˜, êµì°¨ ê²€ì¦ ì‹œì˜ fold ìˆ˜, ì‚¬ìš©ì ì§€ì • ì ìˆ˜ ê³„ì‚°ê¸° ë° ì„ íƒì„ ì•ˆë‚´í•˜ëŠ” ê¸°íƒ€ parameterë“¤ì„ ì„¤ì •



```python
from sklearn.feature_selection import RFECV

# ë³€ìˆ˜ ì„ íƒì„ ìœ„í•œ ëª¨ë¸ ê°ì²´ ìƒì„±
estimator = RandomForestClassifier(random_state = 10, n_estimators = 100, n_jobs = -1)

# selector ê°ì²´ ìƒì„±
selector = RFECV(estimator, step = 1, cv = 3, scoring = scorer, n_jobs = -1)
```


```python
### í•™ìŠµ

selector.fit(train_set, train_labels)
```


```python
### ê²°ê³¼ ì‹œê°í™”

plt.plot(selector.cv_results_.keys())

plt.xlabel('Number of Features'); plt.ylabel('Macro F1 Score'); plt.title('Feature Selection Scores');
selector.n_features_
```

- ìµœëŒ€ **96ê°œ**ì˜ ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ë©´ ì ìˆ˜ê°€ í–¥ìƒëœë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

  - selectorì— ë”°ë¥´ë©´ ì´ê²ƒì´ ìµœì ì˜ feature ê°œìˆ˜ì„

- ê° featureì˜ ìˆœìœ„ëŠ” í›ˆë ¨ëœ selector ê°ì²´ë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆìŒ

  - ì—¬ëŸ¬ ë²ˆì˜ ë°˜ë³µì— ê±¸ì²˜ í‰ê· í™”ëœ ê¸°ëŠ¥ ì¤‘ìš”ë„ë¥¼ í‘œì‹œ

  - ìˆœìœ„ê°€ ë™ì¼í•  ìˆ˜ ìˆìœ¼ë©°, ìˆœìœ„ê°€ 1ì¸ featureë§Œ ìœ ì§€ë¨



```python
rankings = pd.DataFrame({'feature': list(train_set.columns), 'rank': list(selector.ranking_)}).sort_values('rank')
rankings.head(10)
```

**ìµœì¢… ë³€ìˆ˜ ì„ íƒ & êµì°¨ ê²€ì¦ ìˆ˜í–‰**



```python
train_selected = selector.transform(train_set)
test_selected = selector.transform(test_set)
```


```python
# dfë¡œ ì¬ê°€ê³µ

selected_features = train_set.columns[np.where(selector.ranking_==1)]
train_selected = pd.DataFrame(train_selected, columns = selected_features)
test_selected = pd.DataFrame(test_selected, columns = selected_features)
```


```python
model_results = cv_model(train_selected, train_labels, model, 'RF-SEL', model_results)
```


```python
# ê²°ê³¼ ì‹œê°í™”

model_results.set_index('model', inplace = True)
model_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),
                                  yerr = list(model_results['cv_std']),
                                 edgecolor = 'k', linewidth = 2)
plt.title('Model F1 Score Results');
plt.ylabel('Mean F1 Score (with error bar)');
model_results.reset_index(inplace = True)
```

- feature selection í•œ ëª¨ë¸ì´ êµì°¨ ê²€ì¦ì—ì„œ ì•½ê°„ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„



# **5. ëª¨ë¸ ì—…ë°ì´íŠ¸**





## **5-1. Light Gradient Boosting Machine**

- Kaggle ëŒ€íšŒì—ì„œ ì£¼ë¡œ ë°ì´í„°ê¸° êµ¬ì¡°í™”ë˜ì–´ ìˆê³ (í…Œì´ë¸” í˜•íƒœ), ë°ì´í„° ì…‹ì´ ê·¸ë¦¬ í¬ì§€ ì•Šì€ ê²½ìš°(ê´€ì¸¡ì¹˜ê°€ ë°±ë§Œ ê°œ ë¯¸ë§Œ) ```GBM(Gradient Boosting Machine)```ì´ ê²½ìŸì—ì„œ ë†’ì€ ë¹„ìœ¨ë¡œ ìŠ¹ë¦¬í•¨

- Gradient Boosting Machineì„ ìœ„í•œ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™”ëŠ” ì£¼ë¡œ **ëª¨ë¸ ìµœì í™”**ë¥¼ í†µí•´ ìˆ˜í–‰ë¨

  - ì´ì „ì— ì˜ ì‘ë™í•˜ì˜€ë˜ ê°’ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•¨

- ```n_estimators```ë¥¼ ì¼ë‹¨ 10000ìœ¼ë¡œ ì„¤ì •í•˜ì˜€ì§€ë§Œ, í•´ë‹¹ ìˆ«ìì— ë„ë‹¬í•˜ì§€ëŠ” ëª»í•¨

  - ìš°ë¦¬ëŠ” ```early_stopping_rounds```ë¥¼ ì„¤ì •í•¨

    - êµì°¨ ê²€ì¦ metricì´ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ train estimatorë¥¼ ì¢…ë£Œì‹œí‚´

    - ```display```ëŠ” ```%%capture```ì™€ ê²°í•©í•˜ì—¬ train ì¤‘ ì‚¬ìš©ì ì§€ì • ì •ë³´ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì‚¬ìš©ë¨


### **ğŸ“Œ ì¡°ê¸° ì¢…ë£Œ(Early Stopping)ë¥¼ í†µí•œ estimatorì˜ ê°œìˆ˜ ì •í•˜ê¸°**

- estimatorì˜ ìˆ˜(```n_estimators``` ë˜ëŠ” ```num_boost_rounds```ë¼ê³  í•˜ëŠ” ì•™ìƒë¸”ì˜ ì˜ì‚¬ ê²°ì • **íŠ¸ë¦¬ ìˆ˜**)ë¥¼ ì„ íƒí•˜ê¸° ìœ„í•´ 5-fold êµì°¨ ê²€ì¦ì„ í™œìš©í•˜ì—¬ ì¡°ê¸° ì¤‘ì§€ ìˆ˜í–‰

  - Macro F1-scoreë¡œ ì¸¡ì •í•œ ì„±ëŠ¥ì´ 100íšŒì˜ train ë¼ìš´ë“œ ë™ì•ˆ ì¦ê°€í•˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ì¶”ì •ì¹˜ë¥¼ ê³„ì† ì¶”ê°€í•  ìˆ˜ ìˆìŒ

  - í•´ë‹¹ metricì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì‚¬ìš©ì ì§€ì • metricì„ ì •ì˜í•´ì•¼ í•¨



```python
def macro_f1_score(labels, predictions):
    predictions = predictions.reshape(len(np.unique(labels)), -1).argmax(axis = 0)
    
    metric_value = f1_score(labels, predictions, average = 'macro')
    
    return 'macro_f1', metric_value, True
```

### **ğŸ“Œ Stratified K-Foldë¥¼ ìœ„í•œ í•¨ìˆ˜**

-  **Stratified K-fold êµì°¨ ê²€ì¦** ë° ì¡°ê¸° ì •ì§€ë¥¼ í†µí•´ ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹ ì„ êµìœ¡

  - êµìœ¡ ë°ì´í„°ì— **ê³¼ì í•©**ë˜ëŠ” ê²ƒì„ ë°©ì§€

- êµì°¨ ê²€ì¦ì„ í†µí•´ í•™ìŠµì„ ìˆ˜í–‰í•˜ê³  ê° foldì— ëŒ€í•œ í™•ë¥ ë¡œ ì˜ˆì¸¡ì„ ê¸°ë¡

- ê° foldì˜ ì˜ˆì¸¡ê°’ì„ ë°˜í™˜í•œ ë‹¤ìŒ ì œì¶œë¬¼ì„ ë°˜í™˜í•˜ì—¬ ê²°ê³¼ í™•ì¸



```python
from sklearn.model_selection import StratifiedKFold
import lightgbm as lgb
from IPython.display import display
```


```python
def model_gbm(features, labels, test_features, test_ids, 
              nfolds = 5, return_preds = False, hyp = None):
    
    feature_names = list(features.columns) # ë³€ìˆ˜ë“¤ì„ ì €ì¥

    ### ì‚¬ìš©ì ì§€ì • hyper parameterì— ëŒ€í•œ ì˜µì…˜
    # ì‚¬ìš©ì ì§€ì • hyper parameterê°€ ìˆëŠ” ê²½ìš°
    if hyp is not None:
        # early sroppingì„ ì‚¬ìš©í•˜ë¯€ë¡œ estimator ìˆ˜ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ
        if 'n_estimators' in hyp:
            del hyp['n_estimators']
        params = hyp
    else:
        # ê¸°ë³¸ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •
        params = {'boosting_type': 'dart', 
                  'colsample_bytree': 0.88, 
                  'learning_rate': 0.028, 
                   'min_child_samples': 10, 
                   'num_leaves': 36, 'reg_alpha': 0.76, 
                   'reg_lambda': 0.43, 
                   'subsample_for_bin': 40000, 
                   'subsample': 0.54, 
                   'class_weight': 'balanced'}
    
    # ëª¨ë¸ ê°ì²´ ìƒì„±
    model = lgb.LGBMClassifier(**params, objective = 'multiclass', 
                               n_jobs = -1, n_estimators = 10000,
                               random_state = 10)
  
    # Stratified k-Fold êµì°¨ ê²€ì¦
    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)
    
    # ê° foldë§ˆë‹¤ ì˜ˆì¸¡í•œ ê²°ê³¼ë¥¼ ì €ì¥
    predictions = pd.DataFrame()
    importances = np.zeros(len(feature_names))
    
    # ì¸ë±ì‹±ì„ ìœ„í•´ arrayë¡œ ë³€í™˜
    features = np.array(features)
    test_features = np.array(test_features)
    labels = np.array(labels).reshape((-1 ))
    
    valid_scores = [] # ê²€ì¦ ì ìˆ˜
    
    ### ê° fold ë§ˆë‹¤
    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):
        # ê° foldë§ˆë‹¤ ì˜ˆì¸¡ ìˆ˜í–‰ -> ê²°ê³¼ ì €ì¥
        fold_predictions = pd.DataFrame()
        
        # train data / valid data
        X_train = features[train_indices]
        y_train = labels[train_indices]
        X_valid = features[valid_indices]
        y_valid = labels[valid_indices]
        
        # í•™ìŠµ(early stopping ì ìš©)
        model.fit(X_train, y_train, early_stopping_rounds = 100, 
                  eval_metric = macro_f1_score,
                  eval_set = [(X_train, y_train), (X_valid, y_valid)],
                  eval_names = ['train', 'valid'],
                  verbose = 200)
        
        # ê²€ì¦ ì ìˆ˜ ì €ì¥
        valid_scores.append(model.best_score_['valid']['macro_f1'])
        
        # "í™•ë¥ "ì„ í†µí•œ ì˜ˆì¸¡ ìˆ˜í–‰
        fold_probabilitites = model.predict_proba(test_features)
        
        # ê°œë³„ ì»¬ëŸ¼ìœ¼ë¡œ ì˜ˆì¸¡ê°’ ì €ì¥
        for j in range(4):
            fold_predictions[(j + 1)] = fold_probabilitites[:, j]
            
        # ì˜ˆì¸¡ì„ ìœ„í•´ í•„ìš”í•œ ì •ë³´ ì¶”ê°€
        fold_predictions['idhogar'] = test_ids
        fold_predictions['fold'] = (i+1)
        
        # ì˜ˆì¸¡ì„ ê¸°ì¡´ ì˜ˆì¸¡ì— ìƒˆ í–‰ìœ¼ë¡œ ì¶”ê°€
        predictions = predictions.append(fold_predictions)
        
        # ê° fold ë³„ í”¼ì²˜ ì¤‘ìš”ë„
        importances += model.feature_importances_ / nfolds   
        
        # foldì— ëŒ€í•œ ì •ë³´ í‘œì‹œ
        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')

    # í”¼ì³ ì¤‘ìš”ë„ë¥¼ dfë¡œ ì €ì¥
    feature_importances = pd.DataFrame({'feature': feature_names,
                                        'importance': importances})
    
    # ê²€ì¦ ì ìˆ˜ì— ëŒ€í•œ ì •ë³´ í‘œì‹œ
    valid_scores = np.array(valid_scores)
    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')
    
    # ì˜ˆì¸¡ì´ í‰ê· ì„ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´
    if return_preds:
        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)
        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)
        
        return predictions, feature_importances
    
    # fold ë³„ ì˜ˆì¸¡ì„ í‰ê· 
    predictions = predictions.groupby('idhogar', as_index = False).mean()
    
    # í´ë˜ìŠ¤ ë° ê´€ë ¨ í™•ë¥  ì°¾ê¸°
    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)
    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)
    predictions = predictions.drop(columns = ['fold'])
    
    # ê° ê°œì²´ì— ëŒ€í•´ "í•˜ë‚˜"ì˜ ì˜ˆì¸¡ê°’ì„ ê°–ë„ë¡ ê¸°ì¡´ì˜ ê°’ê³¼ ë³‘í•©
    submission = submission_base.merge(predictions[['idhogar', 'Target']], 
                                       on = 'idhogar', how = 'left').drop(columns = ['idhogar'])
        
    # ê²°ì¸¡ì¹˜ -> class = 4ë¡œ  ì±„ìš°ê¸°
    # ì ìˆ˜ ê³„ì‚°ì´ ë˜ì§€ x
    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)
    
    return submission, feature_importances, valid_scores
```

### **a) ì¡°ê¸° ì¤‘ì§€ ë…¸íŠ¸ë¥¼ ì‚¬ìš©í•œ êµì°¨ ê²€ì¦**

- train ì„¸íŠ¸ì—ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²• ì¤‘ í•˜ë‚˜

  - ê²€ì¦ ì ìˆ˜ê°€ ê°œì„ ë˜ì§€ ì•ŠëŠ” ê²ƒì´ ë¶„ëª…í•´ì§€ë©´ ëª¨ë¸ ë³µì¡ì„±ì„ ëŠ˜ë¦´ ìˆ˜ ì—†ê¸° ë•Œë¬¸

- í•´ë‹¹ ê³¼ì •ì„ ì—¬ëŸ¬ foldì—ì„œ ë°˜ë³µí•˜ë©´ ë‹¨ì¼ foldë¥¼ ì‚¬ìš©í•  ë•Œ ë°œìƒë  ìˆ˜ ìˆëŠ” í¸í–¥(bias)ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë¨

- ë˜í•œ, ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥

- Gradient Boosting Machineì—ì„œ eatimatorì˜ ìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ê°€ì¥ **ì¢‹ì€** ë°©ë²•



```python
%%capture --no-display

predictions, gbm_fi = model_gbm(train_set, train_labels, test_set, test_ids, return_preds=True)
```

- LGBMì„ í™œìš©í•˜ì—¬ êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•œ ê²°ê³¼ ì„±ëŠ¥ì´ ë§ì´ ë†’ì•„ì§



```python
### ì˜ˆì¸¡ê°’ í™•ì¸

predictions.head()
```

- ê° foldì— ëŒ€í•´ **1, 2, 3, 4**ì—´ì€ ê° **target**ì— ëŒ€í•œ ```í™•ë¥ ```ì„ ë‚˜íƒ€ëƒ„

  - **target**ë¡œ **confidence**ê°€ ìµœëŒ€ì¹˜ì¸ ê²ƒì„ ì„ íƒ

- 5ê°œì˜ fold ëª¨ë‘ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ê°€ì§€ê³  ìˆìŒ

  - ë‹¤ë¥¸ foldì— ëŒ€í•œ ê° **target**ì— ëŒ€í•œ ì‹ ë¢°ë„ í‘œì‹œ ê°€ëŠ¥



```python
plt.rcParams['font.size'] = 18

### Kdeplot
g = sns.FacetGrid(predictions, row = 'fold', hue = 'Target', aspect = 4)
g.map(sns.kdeplot, 'confidence');
g.add_legend();

plt.suptitle('Distribution of Confidence by Fold and Target', y = 1.05);
```

- ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì‹ ë¢°ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ

  - **í´ë˜ìŠ¤ ë¶ˆê· í˜•**ê³¼ **ë†’ì€ ë³´ê¸‰ë¥ **ë¡œ ì¸í•´ ```Target = 4```ì— ëŒ€í•œ ì‹ ë¢°ë„ê°€ ë†’ì€ ê²ƒìœ¼ë¡œ ë³´ì„




```python
### violinplot

plt.figure(figsize = (24, 12))
sns.violinplot(x = 'Target', y = 'confidence', hue = 'fold', data = predictions)
```

- **ë¶ˆê· í˜• í´ë˜ìŠ¤**ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

  - ëª¨ë¸ì´ ê°ê°ì˜ í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆìŒ

  - ì´í›„ ì˜ˆì¸¡ì¹˜ë¥¼ ë³´ê³  í˜¼ë€ì„ ì•¼ê¸°í•˜ëŠ” ìœ„ì¹˜ë¥¼ íƒìƒ‰í•  ìˆ˜ ìˆìŒ

---

- ê° ê°€êµ¬ë³„ ì˜ˆì¸¡ ìˆ˜í–‰ ì‹œ ê° foldì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì„ ```í‰ê· ```í•¨

  - ê°ê°ì˜ ëª¨ë¸ì€ ì•½ê°„ì”© ë‹¤ë¥¸ ë°ì´í„° foldì— ëŒ€í•´ í•™ìŠµë¨ -> ì—¬ëŸ¬ ëª¨ë¸ì„ ì‚¬ìš©í•¨

- Gradient Boosting Machineì€ ```ì•™ìƒë¸” ëª¨ë¸```ì´ë©°, ì—¬ëŸ¬ gbm ëª¨ë¸ì„ í™œì˜í•˜ì—¬ ```meta-ensemble```ë¡œ í™œìš©




```python
# fold ë³„ ì˜ˆì¸¡ì„ í‰ê· 
predictions = predictions.groupby('idhogar', as_index = False).mean()

# í´ë˜ìŠ¤ ë° ê´€ë ¨ í™•ë¥  ì°¾ê¸°
predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)
predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)
predictions = predictions.drop(columns = ['fold'])

# ê° targetì— ëŒ€í•œ ì‹ ë¢°ë„ plotting
plt.figure(figsize = (10, 6))
sns.boxplot(x = 'Target', y = 'confidence', data = predictions)
plt.title('Confidence by Target')

plt.figure(figsize = (10, 6))
sns.violinplot(x = 'Target', y = 'confidence', data = predictions)
plt.title('Confidence by Target')
```

-  5ê°œì˜ foldì— ëŒ€í•œ **í‰ê· ** ì˜ˆì¸¡ì„ ì·¨í•˜ë©°, ì‚¬ì‹¤ìƒ 5ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ê²°í•©í•˜ëŠ” ê²ƒê³¼ ë™ì¼

  - ê° ëª¨ë¸ì€ ì•½ê°„ì”© ë‹¤ë¥¸ ë°ì´í„° ë¶€ë¶„ ì§‘í•©ì— ëŒ€í•´ì„œ í•™ìŠµë¨



```python
%%capture
submission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, 
                                             test_set, test_ids, return_preds=False)

submission.to_csv('gbm_baseline.csv')
```


```python
### feature ì¤‘ìš”ë„ í™•ì¸

_ = plot_feature_importances(gbm_fi, threshold = 0.95)
```

- gbmì—ì„œ ì¤‘ìš”í•˜ê²Œ ì‘ìš©í•˜ëŠ” featureë“¤ì€ ì£¼ë¡œ ```age(ë‚˜ì´)```ì—ì„œ íŒŒìƒëœ featureë“¤ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

- ```education``` ë³€ìˆ˜ ë˜í•œ ì¤‘ìš”í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¨


### **b) ì„ íƒëœ ë³€ìˆ˜ë“¤ë§Œ ì ìš©í•˜ê¸°**

- ì¤‘ë³µ feature ì œê±° ì‘ì—…ì„ í†µí•´ ì„ íƒëœ featureë“¤ì„ í™œìš©



```python
%%capture --no-display

### ì„ íƒëœ ë³€ìˆ˜ë“¤ë§Œ ê°€ì§€ê³  êµì°¨ ê²€ì¦ ìˆ˜í–‰
submission, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, 
                                                               test_selected, test_ids)
```


```python
### ê²°ê³¼ ì €ì¥

model_results = model_results.append(pd.DataFrame({'model': ["GBM", "GBM_SEL"], 
                                                   'cv_mean': [valid_scores.mean(), valid_scores_selected.mean()],
                                                   'cv_std':  [valid_scores.std(), valid_scores_selected.std()]}),
                                                sort = True)
```


```python
### ê²°ê³¼ ì‹œê°í™”

model_results.set_index('model', inplace = True)
model_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),
                                  yerr = list(model_results['cv_std']),
                                 edgecolor = 'k', linewidth = 2)
plt.title('Model F1 Score Results')
plt.ylabel('Mean F1 Score (with error bar)')
model_results.reset_index(inplace = True)
```

- ```10-fold êµì°¨ ê²€ì¦```ì„ ì ìš©í•´ë³´ì.



```python
%%capture

### 1. ì „ì²´ featureì— ëŒ€í•´..
# 5-folds -> 10-folds
submission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, test_set, test_ids, 
                                             nfolds = 10, return_preds = False)
```


```python
submission.to_csv('gbm_10fold.csv', index = False)
```


```python
%%capture

### 2. ì„ íƒëœ featureì— ëŒ€í•´ì„œë§Œ
submission, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, 
                                                               test_selected, test_ids, nfolds = 10)
```


```python
submission.to_csv('gmb_10fold_selected.csv', index = False)
```


```python
### ê²°ê³¼ ì €ì¥

model_results = model_results.append(pd.DataFrame({'model': ["GBM_10Fold", "GBM_10Fold_SEL"], 
                                                   'cv_mean': [valid_scores.mean(), valid_scores_selected.mean()],
                                                   'cv_std':  [valid_scores.std(), valid_scores_selected.std()]}),
                                    sort = True)
```


```python
### ê²°ê³¼ ì‹œê°í™”

model_results.set_index('model', inplace = True)
model_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6), 
                                  edgecolor = 'k', linewidth = 2,
                                  yerr = list(model_results['cv_std']))
plt.title('Model F1 Score Results')
plt.ylabel('Mean F1 Score (with error bar)')
model_results.reset_index(inplace = True)
```

- ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì€ ```ì„ íƒëœ feature```ë“¤ë¡œ ```10-fold``` êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•œ ëª¨ë¸ì„

- **ìµœì í™”**ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ë” ê°œì„ ì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒì´ë¼ ê¸°ëŒ€ë¨



```python
print(f"There are {gbm_fi_selected[gbm_fi_selected['importance'] == 0].shape[0]} features with no importance.")
```

- ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  featureë“¤ì€ Gradient Boosting Machineì—ì„œ ì–´ëŠ ì •ë„ ì¤‘ìš”í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.


# **6. ëª¨ë¸ ìµœì í™”(Model Optimization)**

- êµì°¨ ê²€ì¦ì„ í†µí•´ ```í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •```í•˜ì—¬ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ì´ëŒì–´ ë‚´ëŠ” í”„ë¡œì„¸ìŠ¤



- ëª¨ë¸ ìµœì í™” Options



```

1. ìˆ˜ë™(Manual)

2. GridSearch

3. RandomSearch

4. ìë™í™” ê¸°ë²•

```



- 4ì˜ ê²½ìš° ```Tree Parzen Estimator```ì™€ í•¨ê»˜ ```Bayesian Optimization```ì˜ ìˆ˜ì • ë²„ì „ì„ ì‚¬ìš©í•˜ëŠ” ```Hyperopt```ë¥¼ í¬í•¨í•œ ë‹¤ìˆ˜ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‰½ê²Œ êµ¬í˜„ ê°€ëŠ¥ -> ì´ë¥¼ í™œìš©


## **6-1. Hyperoptì„ í†µí•œ ëª¨ë¸ íŠœë‹**

- ```ë² ì´ì§€ì•ˆ ìµœì í™”```ì—ëŠ” 4ê°€ì§€ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë¨



```

1. ëª©ì  í•¨ìˆ˜: ìµœëŒ€í™”(ë˜ëŠ” ìµœì†Œí™”)í•˜ê³  ì‹¶ì€ ê²ƒ

2. ë„ë©”ì¸ ì˜ì—­: ê²€ìƒ‰í•  ì˜ì—­

3. ë‹¤ìŒ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„ íƒì„ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜: ê³¼ê±° ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ê°’ì„ ì œì•ˆ

4. ê²°ê³¼ ì €ì¥

```




```python
from hyperopt import hp, tpe, Trials, fmin, STATUS_OK
from hyperopt.pyll.stochastic import sample
```


```python
import csv
import ast
from timeit import default_timer as timer
```

### **a) ëª©ì  í•¨ìˆ˜(Objective Function)**

- ëª¨ë¸ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ê°€ ì‚¬ìš©ë˜ê³  ê´€ë ¨ validation ì ìˆ˜ê°€ ë°˜í™˜ë¨

- Hyperoptì€ ìµœì†Œí™” í•  ì ìˆ˜ë¥¼ ìš”êµ¬í•¨

  - ```1 - Macro F1 score```ë¥¼ ë¦¬í„´

- hyper parameterì— ëŒ€í•œ ì—¬ëŸ¬ **ì„¸ë¶€ ì‚¬í•­**ë“¤ì„ ì„¤ì •í•˜ëŠ” ë‹¨ê³„



```python
def objective(hyperparameters, nfolds=5):

    global ITERATION
    ITERATION += 1
    
    # í•˜ìœ„ ìƒ˜í”Œ 
    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)
    subsample_freq = hyperparameters['boosting_type'].get('subsample_freq', 0)
    
    boosting_type = hyperparameters['boosting_type']['boosting_type']
    
    if boosting_type == 'dart':
        hyperparameters['drop_rate'] = hyperparameters['boosting_type']['drop_rate']
    
    # Subsample and subsample frequency to top level keys
    hyperparameters['subsample'] = subsample
    hyperparameters['subsample_freq'] = subsample_freq
    hyperparameters['boosting_type'] = boosting_type
    
    # Whether or not to use limit maximum depth
    if not hyperparameters['limit_max_depth']:
        hyperparameters['max_depth'] = -1
    
    # Make sure parameters that need to be integers are integers
    for parameter_name in ['max_depth', 'num_leaves', 'subsample_for_bin', 
                           'min_child_samples', 'subsample_freq']:
        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])

    if 'n_estimators' in hyperparameters:
        del hyperparameters['n_estimators']
    
    # Using stratified kfold cross validation
    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)
    
    # Convert to arrays for indexing
    features = np.array(train_selected)
    labels = np.array(train_labels).reshape((-1 ))
    
    valid_scores = []
    best_estimators = []
    run_times = []
    
    model = lgb.LGBMClassifier(**hyperparameters, class_weight = 'balanced',
                               n_jobs=-1, metric = 'None',
                               n_estimators=10000)
    
    # Iterate through the folds
    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):
        
        # Training and validation data
        X_train = features[train_indices]
        X_valid = features[valid_indices]
        y_train = labels[train_indices]
        y_valid = labels[valid_indices]
        
        start = timer()
        # Train with early stopping
        model.fit(X_train, y_train, early_stopping_rounds = 100, 
                  eval_metric = macro_f1_score, 
                  eval_set = [(X_train, y_train), (X_valid, y_valid)],
                  eval_names = ['train', 'valid'],
                  verbose = 400)
        end = timer()
        
        # Record the validation fold score
        valid_scores.append(model.best_score_['valid']['macro_f1'])
        best_estimators.append(model.best_iteration_)
        
        run_times.append(end - start)
    
    score = np.mean(valid_scores)
    score_std = np.std(valid_scores)
    loss = 1 - score
    
    run_time = np.mean(run_times)
    run_time_std = np.std(run_times)
    
    estimators = int(np.mean(best_estimators))
    hyperparameters['n_estimators'] = estimators
    
    # Write to the csv file ('a' means append)
    of_connection = open(OUT_FILE, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, hyperparameters, ITERATION, run_time, score, score_std])
    of_connection.close()
    
    # Display progress
    if ITERATION % PROGRESS == 0:
        display(f'Iteration: {ITERATION}, Current Score: {round(score, 4)}.')
    
    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,
            'time': run_time, 'time_std': run_time_std, 'status': STATUS_OK, 
            'score': score, 'score_std': score_std}
```

### **b) ë„ë©”ì¸ ì˜ì—­(Search Space)**

- ```domain```: ê²€ìƒ‰í•  ì „ì²´ ê°’ì˜ ë²”ìœ„

- ```boosting_type```ì´ ```goss```ì¸ ê²½ìš° subsample ë¹„ìœ¨ì„ ë°˜ë“œì‹œ **1.0**ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•¨




```python
space = {
    'boosting_type': hp.choice('boosting_type', 
                              [{'boosting_type': 'gbdt', 
                                'subsample': hp.uniform('gdbt_subsample', 0.5, 1),
                                'subsample_freq': hp.quniform('gbdt_subsample_freq', 1, 10, 1)}, 
                               
                               {'boosting_type': 'dart', 
                                 'subsample': hp.uniform('dart_subsample', 0.5, 1),
                                 'subsample_freq': hp.quniform('dart_subsample_freq', 1, 10, 1),
                                 'drop_rate': hp.uniform('dart_drop_rate', 0.1, 0.5)},
                               
                                {'boosting_type': 'goss',
                                 'subsample': 1.0,
                                 'subsample_freq': 0}]),
         
    'limit_max_depth': hp.choice('limit_max_depth', [True, False]),
    'max_depth': hp.quniform('max_depth', 1, 40, 1),
    'num_leaves': hp.quniform('num_leaves', 3, 50, 1),
    'learning_rate': hp.loguniform('learning_rate', 
                                   np.log(0.025), 
                                   np.log(0.25)),
    'subsample_for_bin': hp.quniform('subsample_for_bin', 2000, 100000, 2000),
    'min_child_samples': hp.quniform('min_child_samples', 5, 80, 5),
    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),
    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),
    'colsample_bytree': hp.uniform('colsample_by_tree', 0.5, 1.0)
}
```


```python
sample(space)
```

### **c) ì•Œê³ ë¦¬ì¦˜**

- ë‹¤ìŒ ê°’ì„ ì„ íƒí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì€ ```Tree Parzen Estimator```ë¡œ, ```Bayes rule```ì„ ì‚¬ìš©í•˜ì—¬ ëª©ì  í•¨ìˆ˜ì˜ ëŒ€ì²´ ëª¨ë¸ì„ êµ¬ì„±í•¨

- objective functionì„ ìµœëŒ€í™” í•˜ëŠ” ëŒ€ì‹  ëŒ€ì²´ ëª¨ë¸ì˜ ```Expected Improvement (EI)```ë¥¼ ìµœëŒ€í™”




```python
algo = tpe.suggest
```

### **d) ê²°ê³¼ ì €ì¥**

- ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ ë°©ë²•ì„ í™œìš©



```

1. Trials object: ëª©ì  í•¨ìˆ˜ì—ì„œ ë°˜í™˜ëœ ëª¨ë“  ê²ƒì„ ì €ì¥

2. ë°˜ë³µí•  ë•Œë§ˆë‹¤ CSV íŒŒì¼ì— ì“°ê¸°

```




```python
# ê²°ê³¼ ì €ì¥í•˜ê¸°
trials = Trials()

# íŒŒì¼ ì—´ê¸°, ì—°ê²°í•˜ê¸°
OUT_FILE = 'optimization.csv'
of_connection = open(OUT_FILE, 'w')
writer = csv.writer(of_connection)

MAX_EVALS = 100
PROGRESS = 10
N_FOLDS = 5
ITERATION = 0

# ì»¬ëŸ¼ëª… ì‘ì„±
headers = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score', 'std']
writer.writerow(headers)

of_connection.close()
```


```python
%%capture --no-display
display("Running Optimization for {} Trials.".format(MAX_EVALS))

# ìµœì í™” ìˆ˜í–‰
best = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,
            max_evals = MAX_EVALS)
```

- í•™ìŠµì„ ì¬ê°œí•˜ê¸° ìœ„í•´ì„œ ë™ì¼í•œ ```trials``` ê°ì²´ë¥¼ ì „ë‹¬í•˜ê³  ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ë¨

- ë‚˜ì¤‘ì— í™œìš©í•˜ê¸° ìœ„í•´ trialsë¥¼ ```json```ìœ¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŒ



```python
import json

# trial ê²°ê³¼ ì €ì¥
with open('trials.json', 'w') as f:
    f.write(json.dumps(str(trials)))
```

## **6-2. ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°**




```python
### ìµœì í™” ëœ ëª¨ë¸ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°

results = pd.read_csv(OUT_FILE).sort_values('loss', ascending = True).reset_index()
results.head()
```


```python
### ê²°ê³¼ ì‹œê°í™”

plt.figure(figsize = (8, 6))
sns.regplot('iteration', 'score', data = results)
plt.title("Optimization Scores")
plt.xticks(list(range(1, results['iteration'].max() + 1, 3)))
```


```python
### ìµœì  parameter ì„ íƒ

best_hyp = ast.literal_eval(results.loc[0, 'hyperparameters'])
best_hyp
```


```python
%%capture

### ì„ íƒëœ featureë“¤ë¡œë§Œ ëª¨ë¸ë§
submission, gbm_fi, valid_scores = model_gbm(train_selected, train_labels, 
                                             test_selected, test_ids, 
                                             nfolds = 10, return_preds=False)

model_results = model_results.append(pd.DataFrame({'model': ["GBM_OPT_10Fold_SEL"], 
                                                   'cv_mean': [valid_scores.mean()],
                                                   'cv_std':  [valid_scores.std()]}),
                                    sort = True).sort_values('cv_mean', ascending = False)
```


```python
%%capture

### ì „ì²´ featureë¡œ ëª¨ë¸ë§
submission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, 
                                             test_set, test_ids, 
                                             nfolds = 10, return_preds=False)

model_results = model_results.append(pd.DataFrame({'model': ["GBM_OPT_10Fold"], 
                                                   'cv_mean': [valid_scores.mean()],
                                                   'cv_std':  [valid_scores.std()]}),
                                    sort = True).sort_values('cv_mean', ascending = False)
```


```python
model_results.head()
```


```python
### ì œì¼ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ë¡œ ì˜ˆì¸¡ í›„ ê²°ê³¼ ì €ì¥

submission.to_csv('gbm_opt_10fold_selected.csv', index = False)
```

- ì´ ì‹œì ì—ì„œ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ìµœì í™”ë¥¼ ê³„ì†í•˜ê±°ë‚˜, ë” ë§ì€ ê¸°ëŠ¥ ì—”ì§€ë‹ˆì–´ë§/ ì¶”ê°€ì ì¸ ëª¨ë¸ ìŠ¤íƒ ë˜ëŠ” ì•™ìƒë¸”ì„ ì‹œë„í•˜ê±°ë‚˜, ì°¨ì› ì¶•ì†Œ ë˜ëŠ” ì˜¤ë²„ìƒ˜í”Œë§ê³¼ ê°™ì€ ë” ì‹¤í—˜ì ì¸ ë°©ë²•ì„ ê²€í† í•  ìˆ˜ ìˆìŒ

  - ì˜ˆì¸¡ê°’ì„ ê²€í† í•˜ì—¬ ëª¨ë¸ì´ ì–´ë””ì„œ ì˜¤ë¥˜ë¥¼ ë²”í•˜ê³  ìˆëŠ”ì§€ë¥¼ í™•ì¸í•  ì˜ˆì •






```python
_ = plot_feature_importances(gbm_fi)
```

# **7. ì˜ˆì¸¡ í™•ì¸**

- test ë°ì´í„°ì—ì„œ ì˜ˆì¸¡ëœ labelì˜ ë¶„í¬ë¥¼ ì‹œê°í™”í•˜ì—¬ í™•ì¸í•  ìˆ˜ ìˆìŒ

  - train ë°ì´í„°ì™€ ë™ì¼í•œ ë¶„í¬ë¥¼ ë³´ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒë¨

  - ê°€êµ¬ë³„ ì˜ˆì¸¡ì— ê´€ì‹¬ì´ ìˆê¸°ì—, ê° ê°€êµ¬ì— ëŒ€í•œ ì˜ˆì¸¡ë§Œ í™•ì¸í•˜ì—¬ train ë°ì´í„°ì˜ ì˜ˆì¸¡ê³¼ ë¹„êµ



----

- ë‹¤ìŒ íˆìŠ¤í† ê·¸ë¨ì€ ì ˆëŒ€ ì¹´ìš´íŠ¸ ëŒ€ì‹  ìƒëŒ€ì ì¸ ë¹ˆë„ë¥¼ í‘œì‹œí•˜ëŠ” **ì •ê·œí™”** ëœ ê°’

  - ì›ë³¸ì˜ ë°ì´í„° ìˆ˜ì™€ validation ë°ì´í„°ì—ì„œì˜ ë°ì´í„° ìˆ˜ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸



```python
preds = submission_base.merge(submission, on = 'Id', how = 'left')
preds = pd.DataFrame(preds.groupby('idhogar')['Target'].mean())

# train dataì—ì„œì˜ labelì˜ ë¶„í¬ ì‹œê°í™”
fig, axes = plt.subplots(1, 2, sharey = True, figsize = (12, 6))
heads['Target'].sort_index().plot.hist(normed = True,
                                       edgecolor = r'k',
                                       linewidth = 2,
                                       ax = axes[0])
axes[0].set_xticks([1, 2, 3, 4])
axes[0].set_xticklabels(poverty_mapping.values(), rotation = 60)
axes[0].set_title('Train Label Distribution')

# Plot the predicted labels
preds['Target'].sort_index().plot.hist(normed = True, 
                                       edgecolor = 'k',
                                       linewidth = 2,
                                       ax = axes[1])
axes[1].set_xticks([1, 2, 3, 4])
axes[1].set_xticklabels(poverty_mapping.values(), rotation = 60)
plt.subplots_adjust()
plt.title('Predicted Label Distribution')
```


```python
heads['Target'].value_counts()
```


```python
preds['Target'].value_counts()
```

- ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆì§€ë§Œ train labelì˜ ë¶„í¬ì— ê°€ê¹Œì›€

  - ```target = 4```ê°€ ì•„ë‹Œ ```target = 3```ì´ ê³¼ë„í•˜ê²Œ í‘œí˜„ë¨

- ë¶ˆê· í˜• ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì†Œìˆ˜ì˜ í´ë˜ìŠ¤ë¥¼ [oversampling](http://contrib.scikit-learn.org/imbalanced-learn/stable/over_sampling.html)í•˜ëŠ” ë°©ë²•ì´ ìˆìŒ

  - ```imbalanced learn``` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì‰½ê²Œ êµ¬í˜„ ê°€ëŠ¥


## **7-1. ê²€ì¦(Validation)**

- testìš© ì˜ˆì¸¡ì„ í†µí•´ labelì˜ ë¶„í¬ë¥¼ train ë°ì´í„°ì—ì„œì˜ ë¶„í¬ì™€ ë¹„êµí•  ìˆ˜ ìˆìŒ

- ì˜ˆì¸¡ì„ ì‹¤ì œ ê°’ê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ì„œëŠ” train ë°ì´í„°ë¥¼ ë³„ë„ì˜ validation ì„¸íŠ¸ë¡œ ë¶„í• í•´ì•¼ í•¨

  - **1000ê°œ**ì˜ ë°ì´í„°ë¥¼ ê²€ì¦ì— í™œìš©

  - ì´í›„ ```confusion matrix```ë¥¼ í†µí•´ ì˜¤ë¶„ë¥˜ íƒì§€



```python
from sklearn.model_selection import train_test_split

# ë°ì´í„° ë¶„í• 
X_train, X_valid, y_train, y_valid = train_test_split(train_selected,
                                                      train_labels,
                                                      test_size = 1000,
                                                      random_state = 10)

# ëª¨ë¸ ìƒì„±/í•™ìŠµ
model = lgb.LGBMClassifier(**best_hyp, 
                           class_weight = 'balanced',
                           random_state = 10)
model.fit(X_train, y_train);
```


```python
# ê²€ì¦ ìˆ˜í–‰
valid_preds = model.predict_proba(X_valid)
preds_df = pd.DataFrame(valid_preds, columns = [1, 2, 3, 4])

# ì˜ˆì¸¡ê°’ìœ¼ë¡œ ë³€í™˜
preds_df['prediction'] = preds_df[[1, 2, 3, 4]].idxmax(axis = 1)
preds_df['confidence'] = preds_df[[1, 2, 3, 4]].max(axis = 1)

preds_df.head()
```


```python
print('F1 score:', round(f1_score(y_valid, preds_df['prediction'], average = 'macro'), 5))
```

**ğŸ“Œ ì˜¤ì°¨ í–‰ë ¬(confusion matrix)**  

- ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´ë¥¼ ë³´ì—¬ì¤Œìœ¼ë¡œì¨ ëª¨í˜•ì´ í˜¼ë™í•˜ëŠ” ì§€ì ì„ íŒŒì•…í•  ìˆ˜ ìˆìŒ




```python
from sklearn.metrics import confusion_matrix
import itertools

### ì˜¤ì°¨ í–‰ë ¬ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def plot_confusion_matrix(cm, classes,
                          normalize = False,
                          title = 'Confusion matrix',
                          cmap = plt.cm.Oranges):
  
    ### ì •ê·œí™”
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    print(cm)

    plt.figure(figsize = (10, 10))
    plt.imshow(cm, interpolation='nearest', cmap = cmap)
    plt.title(title, size = 24)
    plt.colorbar(aspect = 4)
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation = 45, size = 14)
    plt.yticks(tick_marks, classes, size = 14)

    fmt = '.2f' if normalize else 'd' # formatting
    thresh = cm.max() / 2. # ì„ê³„ê°’ ì„¤ì •
    
    ### Labeling 
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,
                 horizontalalignment = "center",
                 color = "white" if cm[i, j] > thresh else "black")
        
    plt.grid(None)
    plt.tight_layout()
    plt.ylabel('True label', size = 18)
    plt.xlabel('Predicted label', size = 18)
```


```python
cm = confusion_matrix(y_valid, preds_df['prediction']) # ì˜¤ì°¨ í–‰ë ¬

plot_confusion_matrix(cm, classes = ['Extreme', 'Moderate', 'Vulnerable', 'Non-Vulnerable'],
                      title = 'Poverty Confusion Matrix')
```

- ```Confusion Matrix í•´ì„```

  - ëŒ€ê°ì„ :(ì˜ˆì¸¡ ê°’) == (ì‹¤ì œ ê°’)

  - ë‚˜ë¨¸ì§€: ì˜ëª»ëœ ê°’

- í˜„ì¬ ëª¨ë¸ì€ ```poverty = extreme```ì¸ 25ê°œì˜ ê´€ì¸¡ì¹˜ë¥¼ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•œ ë°˜ë©´, ë‚˜ë¨¸ì§€ 26ê°œì˜ ê´€ì¸¡ì¹˜ì˜ ê²½ìš° ```poverty = moderate```ë¼ê³  ì˜ëª» ì˜ˆì¸¡í•¨

- ì „ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì€ ```poverty = non-vulnerable```ì¸ ê°€êµ¬ë¥¼ ì‹ë³„í•˜ëŠ” ë°ë§Œ ë§¤ìš° ì •í™•í•˜ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

- ì‹¤ì œ ë ˆì´ë¸”ì— ëŒ€í•œ ì˜¤ì°¨ í–‰ë ¬ì„ **ì •ê·œí™”**í•˜ì—¬ ê° í´ë˜ìŠ¤ì—ì„œ ì˜ˆì¸¡ëœ ì‹¤ì œ ë ˆì´ë¸”ì˜ ë°±ë¶„ìœ¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ



```python
plot_confusion_matrix(cm, normalize = True,
                      classes = ['Extreme', 'Moderate', 'Vulnerable', 'Non-Vulnerable'],
                      title = 'Poverty Confusion Matrix')
```

- ```poverty = non-vulnerable```ì™¸ì˜ í´ë˜ìŠ¤ëŠ” ì˜ êµ¬ë¶„í•˜ì§€ ëª»í•˜ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

- ```poverty = vulnerable``` ì¤‘ ì•½ 35%ë§Œ ì •í™•í•˜ê²Œ ì‹ë³„í•˜ê³ , ë” ë§ì€ ìˆ˜ëŠ” ì˜ëª» ë¶„ë¥˜í•˜ê³  ìˆìŒ

> ë¶ˆê· í˜• ë°ì´í„° ë¶„ë¥˜ ë¬¸ì œëŠ” ëª¨ë¸ì´ ì •í™•íˆ ì˜ˆì¸¡ì„ í•˜ëŠ” ë° ì–´ë ¤ì›€ì´ ìˆìŒ



- ì´ëŸ¬í•œ ê²½ìš° **oversampling**ì´ë‚˜ ì—¬ëŸ¬ ì˜ì—­ì—ì„œ **ë‹¤ì–‘í•œ ëª¨ë¸ì„ í•™ìŠµ**í•˜ëŠ” ë“±ì˜ ë°©ë²•ì„ íƒí•  ìˆ˜ëŠ” ìˆì§€ë§Œ, ```ë°ì´í„°ë¥¼ ìµœëŒ€í•œ ë§ì´ ëª¨ìœ¼ëŠ” ê²ƒ```ì´ ê¶Œì¥ë¨


## **7-2. ì°¨ì› ì¶•ì†Œ(Dimension Reduction)**

- ì„ íƒëœ ë°ì´í„° ì„¸íŠ¸ì— ëª‡ ê°€ì§€ ë‹¤ë¥¸ **ì°¨ì› ì¶•ì†Œ** ë°©ë²•ì„ ì ìš©í•  ìˆ˜ ìˆìŒ

  - ì‹œê°í™” ë˜ëŠ” ê¸°ê³„ í•™ìŠµì„ ìœ„í•œ ì „ì²˜ë¦¬ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ



- ì¢…ë¥˜

1. ```PCA```(Principal Components Analysis): ë°ì´í„°ì—ì„œ ê°€ì¥ í° ë³€ë™ì´ ì¼ì–´ë‚˜ëŠ” ì°¨ì›ì„ ì°¾ìŒ

2. ```ICA```(Independent Components Analysis): ë‹¤ë³€ëŸ‰ ì‹ í˜¸ë¥¼ ë…ë¦½ì ì¸ ì‹ í˜¸ë¡œ ë¶„ë¦¬í•˜ë ¤ê³  ì‹œë„

3. ```TSNE```(T-distributed Stochastic Neighbor Embedding): ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ë§¤ë‹ˆí´ë“œì— ë§¤í•‘í•˜ì—¬ ë°ì´í„° ë‚´ì˜ ë¡œì»¬ êµ¬ì¡°ë¥¼ ìœ ì§€

4. ```UMAP```(Uniform Manifold Approximation and Projection): ë°ì´í„°ë¥¼ ì €ì°¨ì› ë§¤ë‹ˆí´ë“œì— ë§¤í•‘í•˜ì§€ë§Œ TSNEë³´ë‹¤ ë” ë§ì€ ì „ì—­ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ë ¤ê³  í•˜ëŠ” ë¹„êµì  ìƒˆë¡œìš´ ê¸°ìˆ 



- ë„¤ ê°€ì§€ ë°©ë²• ëª¨ë‘ íŒŒì´ì¬ì—ì„œ êµ¬í˜„í•˜ê¸°ê°€ ë¹„êµì  ê°„ë‹¨í•¨

- ì‹œê°í™”ë¥¼ ìœ„í•´ ì„ íƒí•œ í˜•ìƒì„ **3ì°¨ì›**ìœ¼ë¡œ ë§¤í•‘í•œ ë‹¤ìŒ ëª¨ë¸ë§ í˜•ìƒìœ¼ë¡œ ```PCA```, ```ICA``` ë° ```UMAP```ì„ ì‚¬ìš©

  - ```TSNE```ì—ëŠ” **ë³€í™˜** ë°©ë²•ì´ ì—†ìœ¼ë¯€ë¡œ ì „ì²˜ë¦¬ì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ



```python
!pip install umap
```


```python
from umap import UMAP
from sklearn.decomposition import PCA, FastICA
from sklearn.manifold import TSNE

n_components = 3 # 3ì°¨ì›

### ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ ê°ì²´ ìƒì„±
umap = UMAP(n_components = n_components)
pca = PCA(n_components = n_components)
ica = FastICA(n_components = n_components)
tsne = TSNE(n_components = n_components)
```


```python
train_df = train_selected.copy()
test_df = test_selected.copy()

for method, name in zip([umap, pca, ica, tsne], 
                        ['umap', 'pca', 'ica', 'tsne']):
    
    # TSNEëŠ” ë³€í™˜ methodê°€ ì—†ìŒ
    if name == 'tsne':
        start = timer()
        reduction = method.fit_transform(train_selected)
        end = timer()
    else:
        start = timer()
        reduction = method.fit_transform(train_selected)
        end = timer()
        
        test_reduction = method.transform(test_selected)
    
        # Add components to test data
        test_df['%s_c1' % name] = test_reduction[:, 0]
        test_df['%s_c2' % name] = test_reduction[:, 1]
        test_df['%s_c3' % name] = test_reduction[:, 2]

    # Add components to training data for visualization and modeling
    train_df['%s_c1' % name] = reduction[:, 0]
    train_df['%s_c2' % name] = reduction[:, 1]
    train_df['%s_c3' % name] = reduction[:, 2]
    
    # ì†Œìš” ì‹œê°„ ì¶œë ¥
    print(f'Method: {name} {round(end - start, 2)} seconds elapsed.')
```


```python
from mpl_toolkits.mplot3d import Axes3D

def discrete_cmap(N, base_cmap=None):
    """Create an N-bin discrete colormap from the specified input map
    Source: https://gist.github.com/jakevdp/91077b0cae40f8f8244a"""

    base = plt.cm.get_cmap(base_cmap)
    color_list = base(np.linspace(0, 1, N))
    cmap_name = base.name + str(N)
    
    return base.from_list(cmap_name, color_list, N)

cmap = discrete_cmap(4, base_cmap = plt.cm.RdYlBu)

train_df['label'] = train_labels
```


```python
### ê°ê°ì˜ ë°©ë²•ì„ í™œìš©í•˜ì—¬ ì‹œê°í™”

for method, name in zip([umap, pca, ica, tsne], 
                        ['umap', 'pca', 'ica', 'tsne']):
    
    fig = plt.figure(figsize = (8, 8))
    ax = fig.add_subplot(111, projection='3d')
    
    p = ax.scatter(train_df['%s_c1' % name], train_df['%s_c2'  % name], train_df['%s_c3'  % name], 
                   c = train_df['label'].astype(int), cmap = cmap)
    
    plt.title(f'{name.capitalize()}', size = 22)
    fig.colorbar(p, aspect = 4, ticks = [1, 2, 3, 4])
```

- ì´ëŸ¬í•œ ê·¸ë˜í”„ì—ì„œ ë§ì€ êµ°ì§‘í™”ë¥¼ ë³´ê¸°ëŠ” ì–´ë ¤ì›€

  - ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë¥¼ ê³ ë ¤í•  ë•Œ ë¹ˆê³¤ ìˆ˜ì¤€ì„ ë¶„ë¦¬í•˜ëŠ” ê²ƒì´ ì–´ë ¤ì›€ì„ ì˜ë¯¸

- ë§ˆì§€ë§‰ìœ¼ë¡œ ```PCA```, ```ICA``` ë° ```UMAP```ì„ í™œìš©í•˜ì—¬ ì¶”ê°€ì ìœ¼ë¡œ featureì˜ ê°œìˆ˜ë¥¼ ì¤„ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŒ



```python
train_df, test_df = train_df.align(test_df, axis = 1, join = 'inner')
```


```python
%%capture

submission, gbm_fi, valid_scores = model_gbm(train_df, train_labels, 
                                             test_df, test_ids, nfolds = 10,
                                             hyp = best_hyp)
```


```python
submission.to_csv('gbm_opt_10fold_dr.csv', index = False)
```


```python
model_results = model_results.append(pd.DataFrame({'model': ["GBM_OPT_10Fold_DR"], 
                                                   'cv_mean': [valid_scores.mean()],
                                                   'cv_std':  [valid_scores.std()]}),
                                    sort = True)
```


```python
model_results = model_results.sort_values('cv_mean')
model_results.set_index('model', inplace = True)
model_results['cv_mean'].plot.bar(color = 'orange', figsize = (10, 8),
                                  edgecolor = 'k', linewidth = 2,
                                  yerr = list(model_results['cv_std']))
plt.title('Model F1 Score Results')
plt.ylabel('Mean F1 Score (with error bar)')

model_results.reset_index(inplace = True)
```

- ì°¨ì›ì´ ê°ì†Œëœ ì„±ë¶„ì€ ëª¨í˜•ì˜ ì „ì²´ ì„±ëŠ¥ì— ì•½ê°„ ì˜í–¥ì„ ë¯¸ì¹¨

  - train dataì˜ **ê³¼ì í•©**ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŒ



```python
_ = plot_feature_importances(gbm_fi)
```

- ì°¨ì› ì¶•ì†Œëœ featureì˜ feature importanceê°€ ë§¤ìš° **ë†’ìŒ**

  - **overfitting**ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ

- ì°¨ì› ì¶•ì†ŒëŠ” label ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ x

  - ëª¨í˜•ì˜ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ì •ë³´ë“¤ì„ í¬í•¨í•˜ì§€ ëª»í•  ìˆ˜ë„ ìˆìŒ


## **7-3. ë‹¨ì¼ íŠ¸ë¦¬ ëª¨ë¸ ì‹œê°í™”**


- RandomForestClassifierì—ì„œ **í•˜ë‚˜**ì˜ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ë¥¼ ì‚´í´ë³¼ ìˆ˜ ìˆìŒ

  - ê°€ì‹œì„±ì„ ìœ„í•´ ```max_depth```ë¥¼ ì œí•œí•œ ë‹¤ìŒ íŠ¸ë¦¬ë¥¼ ì „ì²´ì ìœ¼ë¡œ í™•ì¥í•¨

  



```python
model = RandomForestClassifier(max_depth = 3, n_estimators=10)
model.fit(train_selected, train_labels)

### í•˜ë‚˜ì˜ íŠ¸ë¦¬ë¥¼ ì¶”ì¶œ
estimator_limited = model.estimators_[5]
estimator_limited
```

- í›ˆë ¨ëœ íŠ¸ë¦¬ë¥¼ ê°€ì ¸ì™€ ```export_graphviz```ë¥¼ ì‚¬ìš©í•˜ì—¬ ```.dot``` íŒŒì¼ë¡œ ë‚´ë³´ëƒ„



```python
### ë‹¨ì¼ íŠ¸ë¦¬ ëª¨í˜• ë‚´ë³´ë‚´ê¸°

from sklearn.tree import export_graphviz

export_graphviz(estimator_limited, out_file='tree_limited.dot', feature_names = train_selected.columns,
                class_names = ['extreme', 'moderate' , 'vulnerable', 'non-vulnerable'],
                rounded = True, proportion = False, precision = 2, filled = True)
```

- ```.dot``` íŒŒì¼ì„ ```.png``` íŒŒì¼ë¡œ ë³€í™˜



```python
!dot -Tpng tree_limited.dot -o tree_limited.png
```

- ```IPython.display```ì„ í™œìš©í•˜ì—¬ Jupyter Notebookì—ì„œ íŠ¸ë¦¬ë¥¼ í™•ì¸



```python
### íŠ¸ë¦¬ ì‹œê°í™”

from IPython.display import Image
Image(filename = 'tree_limited.png')
```

### **ğŸ“Œ ìµœëŒ€ ê¹Šì´ ì œí•œ ì—†ì´ íŠ¸ë¦¬ ì‹œê°í™”**

- ê¹Šì´ ì œí•œì´ ì—†ìœ¼ë©´, íŠ¸ë¦¬ëŠ” ë¬´í•œì • ì„±ì¥í•  ìˆ˜ ìˆìŒ

  - ë”°ë¼ì„œ, ì¼ë°˜ì ìœ¼ë¡œëŠ” **ì•½ê°„ì˜ ì œí•œ**ì„ ë‘ëŠ” ê²ƒì´ ë„ì›€ë¨



```python
# ê¹Šì´ ì œí•œ x

model = RandomForestClassifier(max_depth = None, n_estimators=10)
model.fit(train_selected, train_labels)
estimator_nonlimited = model.estimators_[5]

export_graphviz(estimator_nonlimited, out_file='tree_nonlimited.dot', feature_names = train_selected.columns,
                class_names = ['extreme', 'moderate' , 'vulnerable', 'non-vulnerable'],
                rounded = True, proportion = False, precision = 2)

!dot -Tpng tree_nonlimited.dot -o tree_nonlimited.png -Gdpi=600
```


```python
Image(filename = 'tree_nonlimited.png')
```

~ë„ì €íˆ ì•Œì•„ë³¼ ìˆ˜ ì—†ìŒ~


# **8. ê²°ë¡ **

- ì‹¤ì œ ë¬¸ì œì— ëŒ€í•œ ì „ì²´ ë°ì´í„° ê³¼í•™ ì†”ë£¨ì…˜ì˜ ë‹¨ê³„ë³„ êµ¬í˜„ì„ ìˆ˜í–‰

- í”„ë¡œì„¸ìŠ¤ ì •ë¦¬>  



```

1. ë¬¸ì œ ì´í•´

2. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)

  - ë°ì´í„° ë¬¸ì œ ì²˜ë¦¬

  - ê²°ì¸¡ê°’ ì…ë ¥

3. íŠ¹ì„± ê³µí•™(Feature Engineering)

  - ë°ì´í„° ì§‘ê³„(aggregation)

  - ë‹¨ê³„ë³„ í”¼ì³ ì„ íƒ

4. ëª¨ë¸ ì„ íƒ

  - ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì‹œë„í•˜ì—¬ ì–´ë–¤ ëª¨ë¸ì´ ê°€ì¥ ìœ ë ¥í•œì§€ í™•ì¸

  - ê¸°ëŠ¥ ì„ íƒ ê¸°ëŠ¥ë„ ì‘ë™ ê°€ëŠ¥

5. ëª¨ë¸ ìµœì í™”

  - ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì„ íƒí•˜ê³  íŠœë‹

6. ëª¨ë¸ ìµœì í™”

7. ì˜ˆì¸¡ ê²€í† 

  - ëª¨ë¸ì˜ ë‹¨ì ì„ ì‹ë³„í•©ë‹ˆë‹¤

8. ìƒˆë¡œìš´ ê¸°ìˆ ì„ ì‹œë„

```



---



- ì„œë¡ ì—ì„œ ì–¸ê¸‰í•œ ë°”ì™€ ê°™ì´, ì´ëŸ¬í•œ ë‹¨ê³„ëŠ” ì¼ë°˜ì ì¸ ìˆœì„œë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ, ì„±ëŠ¥ì— ë§Œì¡±ë˜ì§€ ëª»í•˜ëŠ” ê²½ìš° ëª¨ë¸ë§ í›„ í”¼ì³ ì—”ì§€ë‹ˆì–´ë§/ì„ íƒ í•­ëª©ìœ¼ë¡œ ë˜ëŒì•„ê°€ëŠ” ê²½ìš°ë„ ë§ìŒ

  - ì˜ˆì¸¡ì„ ì¡°ì‚¬í•œ í›„ ëª¨ë¸ë§ ë‹¨ê³„ë¡œ ëŒì•„ê°€ì„œ ì ‘ê·¼ ë°©ì‹ì„ ë‹¤ì‹œ ìƒê°í•  ìˆ˜ë„ ìˆìŒ



- ê¸°ê³„ í•™ìŠµì€ ëŒ€ë¶€ë¶„ ê²½í—˜ì ì´ë¼ëŠ” ê²ƒì„ ëª…ì‹¬í•´ì•¼ í•¨

  -  í™•ë¦½ëœ ëª¨ë²” ì‚¬ë¡€ê°€ ê±°ì˜ ì—†ìœ¼ë¯€ë¡œ ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•ì„ ê²°ì •í•˜ê¸° ìœ„í•´ ì§€ì†ì ìœ¼ë¡œ ê²½í—˜í•´ì•¼ í•¨



- ìš°ë¦¬ì˜ ìµœì¢… ëª¨ë¸ì€ ë‹¤ë¥¸ ê²½ìŸ ëª¨ë¸ë“¤ì— ë¹„í•´ ìš°ìˆ˜í•˜ì§€ë§Œ, ì „ì²´ì ìœ¼ë¡œ ë§¤ìš° ì •í™•í•˜ì§€ëŠ” ì•ŠìŒ

  - ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, ì „ë°˜ì ìœ¼ë¡œ ```ë°ì´í„°ê°€ ë¶€ì¡±``í•˜ì—¬ ì˜ˆì™¸ì ì¸ ë©”íŠ¸ë¦­ì„ ë‹¬ì„±í•  ìˆ˜ ì—†ìŒ

  - ê²°êµ­ ë°ì´í„° ê³¼í•™ í”„ë¡œì íŠ¸ì˜ ì„±íŒ¨ëŠ” ```ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì˜ í’ˆì§ˆê³¼ ì–‘```ì— ë‹¬ë ¤ ìˆìŒ







## **ğŸ“Œ ì¶”ê°€ì ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤**

1. ì¶”ê°€ì ì¸ hyper parameter tuning

  - ëª¨ë¸ì„ ìµœì í™”í•˜ëŠ” ë° ë§ì€ ì‹œê°„ì„ íˆ¬ìí•˜ì§€ ì•Šì•˜ìœ¼ë©°, ìµœì í™”ë¥¼ ìœ„í•´ ì‹œë„í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ íŒ¨í‚¤ì§€ê°€ ìˆìŒ

2. ì¶”ê°€ì ì¸ feature selection

  - ë™ì¼í•œ ì„±ëŠ¥ì„ ì–»ê¸° ìœ„í•´ ëª¨ë“  featureì„ ìœ ì§€í•  í•„ìš”ëŠ” ì—†ìŒ

3. ì†Œìˆ˜ì˜ classì— ëŒ€í•œ oversampling, ë§ì€ classì— ëŒ€í•œ undersampling

4. ì—¬ëŸ¬ ëª¨ë¸ì„ ì•™ìƒë¸”/ìŠ¤íƒœí‚¹

  - ë°ì´í„°ì˜ **ë‹¤ë¥¸** ë¶€ë¶„ì— ëŒ€í•´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê³  ê·¸ë“¤ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ í´ë˜ìŠ¤ë¥¼ ë” ì˜ ë¶„ë¦¬í•  ìˆ˜ ìˆìŒ


