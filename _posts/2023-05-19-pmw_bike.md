---
layout: single
title:  "[ECC DS 10ì£¼ì°¨] íšŒê·€ 1_ìì „ê±° ëŒ€ì—¬ ìˆ˜ìš” ì˜ˆì¸¡"
categories: ML
tags: [ECC, DS, bikeSharing] 
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


# **0. Introduction**

- í•´ë‹¹ ë°ì´í„° ì„¸íŠ¸ì—ëŠ” 2011-01ë¶€í„° 2012-12ê¹Œì§€ ë‚ ì§œ/ì‹œê°„, ê¸°ì˜¨, ìŠµë„, í’ì† ë“±ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 1ì‹œê°„ ê°„ê²© ë™ì•ˆì˜ ```ìì „ê±° ëŒ€ì—¬ íšŸìˆ˜```ê°€ ê¸°ì¬ë˜ì–´ ìˆìŒ


## **ğŸ“Œ Data Description**

- ```datetime```: hourly date + timestamp

- ```season```: 1ì€ ë´„, 2ëŠ” ì—¬ë¦„, 3ì€ ê°€ì„, 4ëŠ” ê²¨ìš¸

- ```holiday```: 1ì€ í† /ì¼ìš”ì¼ì˜ ì£¼ë§ì„ ì œì™¸í•œ êµ­ê²½ì¼ ë“±ì˜ íœ´ì¼, 0ì€ íœ´ì¼ì´ ì•„ë‹Œ ë‚ 

- ```workingday```: 1ì€ í† , ì¼ìš”ì¼ì˜ ì£¼ë§ ë° íœ´ì¼ì´ ì•„ë‹Œ ì£¼ì¤‘, 0ì€ ì£¼ë§ ë° íœ´ì¼

- ```weather```:

  - 1ì€ ë§‘ìŒ, ì•½ê°„ êµ¬ë¦„ ë‚€ íë¦¼

  - 2ëŠ” ì•ˆê°œ, ì•ˆê°œ + íë¦¼

  - 3ì€ ê°€ë²¼ìš´ ëˆˆ, ê°€ë²¼ìš´ ë¹„ + ì²œë‘¥

  - 4ëŠ” ì‹¬í•œ ëˆˆ/ë¹„, ì²œë‘¥/ë²ˆê°œ

- ```temp```: ì˜¨ë„(ì„­ì”¨)

-  ```atemp```: ì²´ê°ì˜¨ë„(ì„­ì”¨)

- ```humidity```: ìƒëŒ€ìŠµë„

- ```windspeed```: í’ì†

- ```casual```: ì‚¬ì „ì— ë“±ë¡ë˜ì§€ ì•ŠëŠ” ì‚¬ìš©ìê°€ ëŒ€ì—¬í•œ íšŸìˆ˜

- ```registered```: ì‹œì „ì— ë“±ë¡ëœ ì‚¬ìš©ìê°€ ëŒ€ì—¬í•œ íšŸìˆ˜

- ```count```: ëŒ€ì—¬ íšŸìˆ˜ --> target ë°ì´í„°


# **1. ë°ì´í„° í´ë Œì§• ë° ê°€ê³µ**



```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings("ignore", category = RuntimeWarning)
```


```python
### ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° & í™•ì¸

bike_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ECC 48á„€á…µ á„ƒá…¦á„€á…ªB/10á„Œá…®á„á…¡/data/bike_train.csv')
print(bike_df.shape)
bike_df.head(3)
```

<pre>
(10886, 12)
</pre>
<pre>
              datetime  season  holiday  workingday  weather  temp   atemp  \
0  2011-01-01 00:00:00       1        0           0        1  9.84  14.395   
1  2011-01-01 01:00:00       1        0           0        1  9.02  13.635   
2  2011-01-01 02:00:00       1        0           0        1  9.02  13.635   

   humidity  windspeed  casual  registered  count  
0        81        0.0       3          13     16  
1        80        0.0       8          32     40  
2        80        0.0       5          27     32  
</pre>

```python
### ë°ì´í„° ì •ë³´ í™•ì¸

bike_df.info()
```

<pre>
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 10886 entries, 0 to 10885
Data columns (total 12 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   datetime    10886 non-null  object 
 1   season      10886 non-null  int64  
 2   holiday     10886 non-null  int64  
 3   workingday  10886 non-null  int64  
 4   weather     10886 non-null  int64  
 5   temp        10886 non-null  float64
 6   atemp       10886 non-null  float64
 7   humidity    10886 non-null  int64  
 8   windspeed   10886 non-null  float64
 9   casual      10886 non-null  int64  
 10  registered  10886 non-null  int64  
 11  count       10886 non-null  int64  
dtypes: float64(3), int64(8), object(1)
memory usage: 1020.7+ KB
</pre>
- Null ë°ì´í„°ëŠ” ì—†ìŒ

- ëŒ€ë¶€ë¶„ì˜ ì¹¼ëŸ¼ì´ int ë˜ëŠ” float ìˆ«ìí˜•ì„

- ```datetime``` ì¹¼ëŸ¼ë§Œ objectí˜•ì„

  - ```datetime``` ì¹¼ëŸ¼ì˜ ê²½ìš° ë…„-ì›”-ì¼-ì‹œ:ë¶„:ì´ˆ ë¬¸ì í˜•ì‹ìœ¼ë¡œ ë¼ ìˆìœ¼ë¯€ë¡œ ì´ì— ëŒ€í•œ ê°€ê³µ í•„ìš”

  - ë¬¸ìì—´ì„ datetime í˜•ìœ¼ë¡œ ë³€ê²½



```python
### datetime ì¹¼ëŸ¼ ê°€ê³µ

# ë¬¸ìì—´ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€ê²½. 
bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)

# datetime íƒ€ì…ì—ì„œ ë…„, ì›”, ì¼, ì‹œê°„ ì¶”ì¶œ
bike_df['year'] = bike_df.datetime.apply(lambda x : x.year)
bike_df['month'] = bike_df.datetime.apply(lambda x : x.month)
bike_df['day'] = bike_df.datetime.apply(lambda x : x.day)
bike_df['hour'] = bike_df.datetime.apply(lambda x: x.hour)
bike_df.head(3)
```

<pre>
             datetime  season  holiday  workingday  weather  temp   atemp  \
0 2011-01-01 00:00:00       1        0           0        1  9.84  14.395   
1 2011-01-01 01:00:00       1        0           0        1  9.02  13.635   
2 2011-01-01 02:00:00       1        0           0        1  9.02  13.635   

   humidity  windspeed  casual  registered  count  year  month  day  hour  
0        81        0.0       3          13     16  2011      1    1     0  
1        80        0.0       8          32     40  2011      1    1     1  
2        80        0.0       5          27     32  2011      1    1     2  
</pre>
- ìƒˆë¡­ê²Œ ```year```, ```month```, ```day```, ```hour``` ì¹¼ëŸ¼ì´ ì¶”ê°€ë¨


- ê¸°ì¡´ì˜ ```datetime``` ì¹¼ëŸ¼ ì‚­ì œ

- ```casual``` + ```registered``` = ```count```ì´ë¯€ë¡œ ```casual```ê³¼ ```registered```ê°€ ë”°ë¡œ í•„ìš”í•˜ì§€ëŠ” x

  - ì˜¤íˆë ¤ ìƒê´€ë„ê°€ ë†’ì•„ ì˜ˆì¸¡ì„ ì €í•´í•  ìš°ë ¤ê°€ ìˆìœ¼ë¯€ë¡œ í•´ë‹¹ ì»¬ëŸ¼ë“¤ì„ ì‚­ì œ



```python
drop_columns = ['datetime','casual','registered']
bike_df.drop(drop_columns, axis = 1,inplace = True)
```

# **2. ëª¨ë¸ë§**



```python
### ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ í•¨ìˆ˜ ì •ì˜

from sklearn.metrics import mean_squared_error, mean_absolute_error

def rmsle(y, pred):
    # log ê°’ ë³€í™˜ ì‹œ NaNë“±ì˜ ì´ìŠˆë¡œ log() ê°€ ì•„ë‹Œ log1p() ë¥¼ ì´ìš©í•˜ì—¬ RMSLE ê³„ì‚°
    log_y = np.log1p(y)
    log_pred = np.log1p(pred)
    squared_error = (log_y - log_pred) ** 2
    rmsle = np.sqrt(np.mean(squared_error))

    return rmsle

# ì‚¬ì´í‚·ëŸ°ì˜ mean_square_error() ë¥¼ ì´ìš©í•˜ì—¬ RMSE ê³„ì‚°
def rmse(y,pred):
    return np.sqrt(mean_squared_error(y,pred))

# MSE, RMSE, RMSLE ë¥¼ ëª¨ë‘ ê³„ì‚° 
def evaluate_regr(y,pred):
    rmsle_val = rmsle(y,pred)
    rmse_val = rmse(y,pred)
    # MAE ëŠ” scikit-learnì˜ mean_absolute_error() ë¡œ ê³„ì‚°
    mae_val = mean_absolute_error(y,pred)
    
    print('RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}'.format(rmsle_val, rmse_val, mae_val))
```

- ```log1p()```ì˜ ê²½ìš°ëŠ” 1 + log() ê°’ìœ¼ë¡œ log ë³€í™˜ê°’ì— 1ì„ ë”í•˜ë¯€ë¡œ ì˜¤ë²„í”Œë¡œìš°/ì–¸ë”í”Œë¡œìš° ë°œìƒ ë¬¸ì œë¥¼ í•´ê²°í•´ ì¤Œ

- ```log1p()```ë¡œ ë³€í™˜ëœ ê°’ì€ ë‹¤ì‹œ ë„˜íŒŒì´ì˜ ```expm1()``` í•¨ìˆ˜ë¡œ ì‰½ê²Œ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì› ê°€ëŠ¥


**ğŸ“Œ íšŒê·€ ëª¨ë¸ ì ìš© ì „ í™•ì¸ ì‚¬í•­**

- ë°ì´í„° ì„¸íŠ¸ê°€ ```ì •ê·œ ë¶„í¬```ë¥¼ ë”°ë¥´ëŠ”ì§€

- ì¹´í…Œê³ ë¦¬í˜• íšŒê·€ ëª¨ë¸ì˜ ê²½ìš° ```ì›-í•« ì¸ì½”ë”©```ìœ¼ë¡œ í”¼ì²˜ë¥¼ ì¸ì½”ë”©í•´ì•¼ í•¨


## **2-1. ì„ í˜• íšŒê·€**


### **a) ë¡œê·¸ ë³€í™˜**



```python
### ì›ë³¸ ë°ì´í„°ë¡œ íšŒê·€ ì˜ˆì¸¡ ìˆ˜í–‰

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso

y_target = bike_df['count']
X_features = bike_df.drop(['count'],axis = 1,inplace = False)

X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, 
                                                    test_size = 0.3, random_state = 0)

lr_reg = LinearRegression() # í•™ìŠµ
lr_reg.fit(X_train, y_train) # ì˜ˆì¸¡
pred = lr_reg.predict(X_test) # í‰ê°€

evaluate_regr(y_test ,pred)
```

<pre>
RMSLE: 1.165, RMSE: 140.900, MAE: 105.924
</pre>
- ì‹¤ì œ target ë°ì´í„° ê°’ì¸ ëŒ€ì—¬ íšŸìˆ˜(Count)ë¥¼ ê°ì•ˆí•˜ë©´ ì˜ˆì¸¡ ì˜¤ë¥˜ë¡œì„œëŠ” ë¹„êµì  í° ê°’ì„



```python
### ìƒìœ„ 5ê°œì˜ ì˜¤ë¥˜ í™•ì¸

def get_top_error_data(y_test, pred, n_tops = 5):
    # DataFrameì— ì»¬ëŸ¼ë“¤ë¡œ ì‹¤ì œ ëŒ€ì—¬íšŸìˆ˜(count)ì™€ ì˜ˆì¸¡ê°’ì„ ì„œë¡œ ë¹„êµ í•  ìˆ˜ ìˆë„ë¡ ìƒì„±
    result_df = pd.DataFrame(y_test.values, columns = ['real_count'])
    result_df['predicted_count'] = np.round(pred)
    result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count'])
    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì´ ê°€ì¥ í° ë°ì´í„° ìˆœìœ¼ë¡œ ì¶œë ¥. 
    print(result_df.sort_values('diff', ascending = False)[:n_tops])
    
get_top_error_data(y_test, pred, n_tops = 5)
```

<pre>
      real_count  predicted_count   diff
1618         890            322.0  568.0
3151         798            241.0  557.0
966          884            327.0  557.0
412          745            194.0  551.0
2817         856            310.0  546.0
</pre>
- í° ìˆœì„œëŒ€ë¡œ ìƒìœ„ 5ìœ„ ì˜¤ë¥˜ ê°’ì€ 546 ~ 568ë¡œ ì‹¤ì œ ê°’ì„ ê°ì•ˆí•˜ë©´ ì˜ˆì¸¡ ì˜¤ë¥˜ê°€ ê½¤ í¼

- íšŒê·€ì—ì„œ ì˜ˆì¸¡ ì˜¤ë¥˜ê°€ í° ê²½ìš° target ê°’ì˜ ë¶„í¬ê°€ ```ì™œê³¡ëœ``` í˜•íƒœë¥¼ ì´ë£¨ê³  ìˆëŠ”ì§€ í™•ì¸

  - target ê°’ì˜ ë¶„í¬ëŠ” ```ì •ê·œ ë¶„í¬``` í˜•íƒœê°€ ê°€ì¥ ì¢‹ìŒ

  - ì™œê³¡ëœ ê²½ìš°ì—ëŠ” íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•˜ê¸° ì‰¬ì›€

  



```python
### target ê°’ì˜ ë¶„í¬ í™•ì¸

y_target.hist()
```

<pre>
<Axes: >
</pre>
<pre>
<Figure size 640x480 with 1 Axes>
</pre>
- count ì¹¼ëŸ¼ ê°’ì´ ì •ê·œ ë¶„í¬ê°€ ì•„ë‹Œ 0 ~ 200 ì‚¬ì´ì— ```ì™œê³¡```ë¼ ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ

- ì¼ë°˜ì ìœ¼ë¡œ ```ë¡œê·¸ ë³€í™˜```ì„ í†µí•´ ì™œê³¡ëœ ê°’ì„ ì •ê·œ ë¶„í¬ í˜•íƒœë¡œ ë°”ê¿ˆ

  - ë„˜íŒŒì´ì˜ ```np.log1p()``` ì´ìš©

- ë³€ê²½ëœ target ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ í›„ ì˜ˆì¸¡í•œ ê°’ì€ ë‹¤ì‹œ ```expm1()``` í•¨ìˆ˜ë¥¼ ì ìš©í•´ ì›ë˜ scale ê°’ìœ¼ë¡œ ì›ìƒ ë³µêµ¬í•˜ë©´ ë¨



```python
### ë¡œê·¸ ë³€í™˜ í›„ target ë³€ìˆ˜ì˜ ë¶„í¬ í™•ì¸

y_log_transform = np.log1p(y_target)
y_log_transform.hist()
```

<pre>
<Axes: >
</pre>
<pre>
<Figure size 640x480 with 1 Axes>
</pre>
- ì •ê·œ ë¶„í¬ í˜•íƒœëŠ” ì•„ë‹ˆì§€ë§Œ ë³€í™˜ ì „ë³´ë‹¤ëŠ” ì™œê³¡ ì •ë„ê°€ ë§ì´ í–¥ìƒë¨



```python
# target ì»¬ëŸ¼ì¸ count ê°’ì„ log1pë¡œ ë¡œê·¸ ë³€í™˜
y_target_log = np.log1p(y_target)

# ë¡œê·¸ ë³€í™˜ëœ y_target_logë¥¼ ë°˜ì˜í•˜ì—¬ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, 
                                                    test_size = 0.3, random_state = 0)
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
pred = lr_reg.predict(X_test)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ì˜ Target ê°’ì€ ë¡œê·¸ ë³€í™˜ë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ expm1ë¥¼ ì´ìš©í•˜ì—¬ ì›ë˜ scaleë¡œ ë³€í™˜
y_test_exp = np.expm1(y_test)

# ì˜ˆì¸¡ê°’ ì—­ì‹œ ë¡œê·¸ ë³€í™˜ëœ target ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµë˜ì–´ ì˜ˆì¸¡ë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ exmplìœ¼ë¡œ scaleë³€í™˜
pred_exp = np.expm1(pred)

evaluate_regr(y_test_exp ,pred_exp)
```

<pre>
RMSLE: 1.017, RMSE: 162.594, MAE: 109.286
</pre>
- ```RMSLE``` ì˜¤ë¥˜ëŠ” ì¤„ì–´ë“¤ì—ˆì§€ë§Œ, ```RMSE```ëŠ” ì˜¤íˆë ¤ ë” ëŠ˜ì–´ë‚¨



```python
### ê° í”¼ì²˜ì˜ íšŒê·€ ê³„ìˆ˜ê°’ ì‹œê°í™”

coef = pd.Series(lr_reg.coef_, index = X_features.columns)
coef_sort = coef.sort_values(ascending = False)
sns.barplot(x = coef_sort.values, y = coef_sort.index)
```

<pre>
<Axes: >
</pre>
<pre>
<Figure size 640x480 with 1 Axes>
</pre>
- ```Year``` í”¼ì²˜ì˜ íšŒê·€ ê³„ìˆ˜ ê°’ì´ ë…ë³´ì ìœ¼ë¡œ í° ê°’ì„ ê°€ì§€ê³  ìˆìŒ

- year ë³€ìˆ˜ì˜ ê²½ìš° 2011, 2012ë¡œ ë˜ì–´ ìˆìŒ

  - ì—°ë„ì— ëŒ€í•œ ì •ë³´



### **b) ì›-í•« ì¸ì½”ë”©(One-hot Encoding)**


- ìˆ«ìí˜• ì¹´í…Œê³ ë¦¬ ê°’ì„ ì„ í˜• íšŒê·€ì— ì‚¬ìš©í•  ê²½ìš° íšŒê·€ ê³„ìˆ˜ ì—°ì‚° ì‹œ ìˆ«ìí˜• ê°’ì˜ ```í¬ê¸°```ì— ì˜í–¥ì„ í¬ê²Œ ë°›ëŠ” ê²½ìš°ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ

  - ì„ í˜• íšŒê·€ì—ì„œëŠ” í”¼ì²˜ ì¸ì½”ë”©ì— ```ì›-í•« ì¸ì½”ë”©```ì„ ì ìš©í•´ ë³€í™˜í•´ì•¼ í•¨



```python
# 'year', month', 'day', hour'ë“±ì˜ í”¼ì²˜ë“¤ì„ One Hot Encoding
X_features_ohe = pd.get_dummies(X_features, columns=['year', 'month','day', 'hour', 'holiday',
                                              'workingday','season','weather'])
```


```python
# ì›-í•« ì¸ì½”ë”©ì´ ì ìš©ëœ feature ë°ì´í„° ì„¸íŠ¸ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ/ì˜ˆì¸¡ ë°ì´í„° ë¶„í• . 
X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log,
                                                    test_size = 0.3, random_state = 0)
```


```python
### ëª¨ë¸ê³¼ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ì„ ì…ë ¥í•˜ë©´ ì„±ëŠ¥ í‰ê°€ ìˆ˜ì¹˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜

def get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1 = False):
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    
    if is_expm1 :
        y_test = np.expm1(y_test) # ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ
        pred = np.expm1(pred)
    print('###',model.__class__.__name__,'###')
    
    evaluate_regr(y_test, pred)
```


```python
# model ë³„ë¡œ í‰ê°€ ìˆ˜í–‰

lr_reg = LinearRegression()
ridge_reg = Ridge(alpha = 10)
lasso_reg = Lasso(alpha = 0.01)

for model in [lr_reg, ridge_reg, lasso_reg]:
    get_model_predict(model,X_train, X_test, y_train, y_test,is_expm1 = True)
```

<pre>
### LinearRegression ###
RMSLE: 0.590, RMSE: 97.688, MAE: 63.382
### Ridge ###
RMSLE: 0.590, RMSE: 98.529, MAE: 63.893
### Lasso ###
RMSLE: 0.635, RMSE: 113.219, MAE: 72.803
</pre>
- ```ì›-í•« ì¸ì½”ë”©``` ì ìš© í›„ ì„ í˜• íšŒê·€ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë§ì´ í–¥ìƒë¨



```python
### í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ë‹¤ì‹œ ì‹œê°í™”

coef = pd.Series(lr_reg.coef_ , index = X_features_ohe.columns)
coef_sort = coef.sort_values(ascending = False)[:20]
sns.barplot(x = coef_sort.values , y = coef_sort.index)
```

<pre>
<Axes: >
</pre>
<pre>
<Figure size 640x480 with 1 Axes>
</pre>
- ```month_9```, ```month_8```, ```month_7``` ë“±ì˜ ì›” ê´€ë ¨ í”¼ì²˜ë“¤ê³¼ ```workingday``` ê´€ë ¨ í”¼ì²˜ë“¤, ê·¸ë¦¬ê³  ```hour``` ê´€ë ¨ í”¼ì²˜ë“¤ì˜ íšŒê·€ ê³„ìˆ˜ê°€ ë†’ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

  - ìƒì‹ì„ ì—ì„œ ìì „ê±°ë¥¼ íƒ€ëŠ” ë° í•„ìš”í•œ í”¼ì²˜ì˜ íšŒê·€ ê³„ìˆ˜ê°€ ë†’ì•„ì§

- ì„ í˜• íšŒê·€ ìˆ˜í–‰ ì‹œì—ëŠ” í”¼ì²˜ë¥¼ ì–´ë–»ê²Œ ì¸ì½”ë”©í•˜ëŠ”ê°€ê°€ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒ



## **2-2. íšŒê·€ íŠ¸ë¦¬ í™œìš©**



```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# ëœë¤ í¬ë ˆìŠ¤íŠ¸, GBM, XGBoost, LightGBM model ë³„ë¡œ í‰ê°€ ìˆ˜í–‰
rf_reg = RandomForestRegressor(n_estimators = 500)
gbm_reg = GradientBoostingRegressor(n_estimators = 500)
xgb_reg = XGBRegressor(n_estimators = 500)
lgbm_reg = LGBMRegressor(n_estimators = 500)

for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]:
    # XGBoostì˜ ê²½ìš° DataFrameì´ ì…ë ¥ë  ê²½ìš° ë²„ì „ì— ë”°ë¼ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥
    # ë”°ë¼ì„œ ndarrayë¡œ ë³€í™˜
    get_model_predict(model, X_train.values, X_test.values, 
                      y_train.values, y_test.values,is_expm1 = True)
```

<pre>
### RandomForestRegressor ###
RMSLE: 0.354, RMSE: 50.155, MAE: 31.126
### GradientBoostingRegressor ###
RMSLE: 0.330, RMSE: 53.324, MAE: 32.733
### XGBRegressor ###
RMSLE: 0.342, RMSE: 51.732, MAE: 31.251
### LGBMRegressor ###
RMSLE: 0.319, RMSE: 47.215, MAE: 29.029
</pre>
- ì•ì˜ ì„ í˜• íšŒê·€ ëª¨ë¸ì— ë¹„í•´ íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ê°œì„ ë¨

