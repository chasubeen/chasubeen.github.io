---
layout: single
title:  "[ECC DS 10ì£¼ì°¨] íšŒê·€ 2_ìºê¸€ ì£¼íƒ ê°€ê²©:ê³ ê¸‰ íšŒê·€ ê¸°ë²•"
categories: ML
tags: [ECC, DS, house price] 
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


# **0. Introduction**


- ë¯¸êµ­ ì•„ì´ì˜¤ì•„ ì£¼ì˜ ì—ì„ìŠ¤ ì§€ë°©ì˜ ```ì£¼íƒ ê°€ê²© ì •ë³´```ë¥¼ ê°€ì§€ê³  ìˆìŒ

- ì„±ëŠ¥ í‰ê°€ ì§€í‘œ: ```RMSLE```

  - ê°€ê²©ì´ ë¹„ì‹¼ ì£¼íƒì¼ìˆ˜ë¡ ì˜ˆì¸¡ ê²°ê³¼ ì˜¤ë¥˜ê°€ ì „ì²´ ì˜¤ë¥˜ì— ë¯¸ì¹˜ëŠ” ë¹„ì¤‘ì´ ë†’ìœ¼ë¯€ë¡œ ì´ë¥¼ ìƒì‡„í•˜ê¸° ìœ„í•´ ì˜¤ë¥˜ ê°’ì„ ë³€í™˜í•œ RMSLEë¥¼ ì´ìš©

- [ëŒ€íšŒ ë§í¬](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview)


**ğŸ“Œ Data Descrpition**  

[ëŒ€íšŒ ì•ˆë‚´](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data)


# **1. ë°ì´í„° ì‚¬ì „ ì²˜ë¦¬(Preprocessing)**



```python
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
```

## **1-1. ë°ì´í„° í™•ì¸**



```python
house_df_org = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ECC 48á„€á…µ á„ƒá…¦á„€á…ªB/10á„Œá…®á„á…¡/data/house_price.csv')
house_df = house_df_org.copy() # ë°ì´í„° ê°€ê³µì„ ì—¬ëŸ¬ ë²ˆ í•˜ê¸° ìœ„í•´ ì›ë³¸ì„ ë³µì‚¬í•´ ë‘ 
house_df.head(3)
```

<pre>
   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \
0   1          60       RL         65.0     8450   Pave   NaN      Reg   
1   2          20       RL         80.0     9600   Pave   NaN      Reg   
2   3          60       RL         68.0    11250   Pave   NaN      IR1   

  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \
0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   
1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   
2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   

  YrSold  SaleType  SaleCondition  SalePrice  
0   2008        WD         Normal     208500  
1   2007        WD         Normal     181500  
2   2008        WD         Normal     223500  

[3 rows x 81 columns]
</pre>
- target ê°’ì€ ë§¨ ë§ˆì§€ë§‰ ì¹¼ëŸ¼ì¸ ```SalesPrice```ì„



```python
### ë°ì´í„° ì„¸íŠ¸ì˜ ì „ì²´ í¬ê¸°ì™€ ì¹¼ëŸ¼ì˜ íƒ€ì…, Nullì´ ìˆëŠ” ì¹¼ëŸ¼ê³¼ ê±´ìˆ˜ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì¶œë ¥

print('ë°ì´í„° ì„¸íŠ¸ì˜ Shape:', house_df.shape)
print('\nì „ì²´ featureë“¤ì˜ type \n',house_df.dtypes.value_counts())

isnull_series = house_df.isnull().sum()
print('\nNull ì»¬ëŸ¼ê³¼ ê·¸ ê±´ìˆ˜:\n ', isnull_series[isnull_series > 0].sort_values(ascending = False))
```

<pre>
ë°ì´í„° ì„¸íŠ¸ì˜ Shape: (1460, 81)

ì „ì²´ featureë“¤ì˜ type 
 object     43
int64      35
float64     3
dtype: int64

Null ì»¬ëŸ¼ê³¼ ê·¸ ê±´ìˆ˜:
  PoolQC          1453
MiscFeature     1406
Alley           1369
Fence           1179
FireplaceQu      690
LotFrontage      259
GarageType        81
GarageYrBlt       81
GarageFinish      81
GarageQual        81
GarageCond        81
BsmtExposure      38
BsmtFinType2      38
BsmtFinType1      37
BsmtCond          37
BsmtQual          37
MasVnrArea         8
MasVnrType         8
Electrical         1
dtype: int64
</pre>
- ë°ì´í„° ì„¸íŠ¸ëŠ” 1460ê°œì˜ ë ˆì½”ë“œì™€ 81ê°œì˜ í”¼ì²˜ë¡œ êµ¬ì„±ë¼ ìˆìŒ

- í”¼ì²˜ì˜ íƒ€ì…ì€ ìˆ«ìí˜•, ë¬¸ìí˜• ë“± ë‹¤ì–‘í•¨

  - 43ê°œì˜ ë¬¸ìí˜• ì»¬ëŸ¼ + 37ê°œì˜ ìˆ«ìí˜• ì»¬ëŸ¼ + target ì»¬ëŸ¼

- ë°ì´í„° ì–‘ì— ë¹„í•´ Null ê°’ì´ ë§ì€ í”¼ì²˜ë„ ì¡´ì¬

  - Null ê°’ì´ ë„ˆë¬´ ë§ì€ í”¼ì²˜ëŠ” drop




```python
### target ê°’ì˜ ë¶„í¬ í™•ì¸
# ë¶„í¬ë„ê°€ ì •ê·œ ë¶„í¬ì¸ì§€ í™•ì¸

plt.title('Original Sale Price Histogram')
sns.distplot(house_df['SalePrice'])
```

<pre>
<Axes: title={'center': 'Original Sale Price Histogram'}, xlabel='SalePrice', ylabel='Density'>
</pre>
<pre>
<Figure size 640x480 with 1 Axes>
</pre>
- ë°ì´í„° ê°’ì˜ ë¶„í¬ê°€ ì¤‘ì‹¬ì—ì„œ ì™¼ìª½ìœ¼ë¡œ ì¹˜ìš°ì¹œ í˜•íƒœë¡œ, ì •ê·œ ë¶„í¬ì—ì„œ ë²—ì–´ë‚˜ ìˆìŒ

- ì •ê·œ ë¶„í¬ê°€ ì•„ë‹Œ ê²°ê³¼ê°’ì„ ì •ê·œ ë¶„í¬ í˜•íƒœë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ```ë¡œê·¸ ë³€í™˜(Log Transformation)```ì„ ì ìš©

  - ```np.log1p()```ë¥¼ ì´ìš©í•´ ë¡œê·¸ ë³€í™˜í•œ ê²°ê´ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ

  - ì˜ˆì¸¡ ì‹œëŠ” ë‹¤ì‹œ ê²°ê´ê°’ì„ ```expm1()```ìœ¼ë¡œ í™˜ì›



```python
### ë¡œê·¸ ë³€í™˜ ì‹œ ë¶„í¬

plt.title('Log Transformed Sale Price Histogram')
log_SalePrice = np.log1p(house_df['SalePrice'])
sns.distplot(log_SalePrice)
```

<pre>
<Axes: title={'center': 'Log Transformed Sale Price Histogram'}, xlabel='SalePrice', ylabel='Density'>
</pre>
<pre>
<Figure size 640x480 with 1 Axes>
</pre>
- ```SalesPrice```ë¥¼  ë¡œê·¸ ë³€í™˜í•´ ì •ê·œ ë¶„í¬ í˜•íƒœë¡œ ê²°ê´ê°’ì´ ë¶„í¬í•˜ê²Œ ë˜ì—ˆìŒ



## **1-2. ë¡œê·¸ ë³€í™˜ ë° ì „ì²˜ë¦¬**

- target ë³€ìˆ˜ì¸ ```SalesPrice```ëŠ” ë¡œê·¸ ë³€í™˜

- Null ê°’ì´ ë§ì€ í”¼ì²˜ë“¤ì€ ì‚­ì œ

  - ```PoolQC```, ```MiscFeature```, ```Alley```, ```Fence```, ```FireplaceQu```

- ```Id```ëŠ” ë‹¨ìˆœ ì‹ë³„ìì´ë¯€ë¡œ ì‚­ì œ

- ```LotFrontage```ëŠ” Null ê°’ì´ ë§ìœ¼ë¯€ë¡œ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´


### **a) ìˆ«ìí˜• ë³€ìˆ˜ ì „ì²˜ë¦¬**



```python
### ë¡œê·¸ ë³€í™˜ ë° ì „ì²˜ë¦¬

# SalePrice ë¡œê·¸ ë³€í™˜
original_SalePrice = house_df['SalePrice']
house_df['SalePrice'] = np.log1p(house_df['SalePrice'])

# Null ì´ ë„ˆë¬´ ë§ì€ ì»¬ëŸ¼ë“¤ê³¼ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì‚­ì œ
house_df.drop(['Id','PoolQC' , 'MiscFeature', 'Alley', 'Fence','FireplaceQu'], axis=1 , inplace=True)
# Drop í•˜ì§€ ì•ŠëŠ” ìˆ«ìí˜• Nullì»¬ëŸ¼ë“¤ì€ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
house_df.fillna(house_df.mean(),inplace = True)

# Null ê°’ì´ ìˆëŠ” í”¼ì²˜ëª…ê³¼ íƒ€ì…ì„ ì¶”ì¶œ
null_column_count = house_df.isnull().sum()[house_df.isnull().sum() > 0]
print('## Null í”¼ì²˜ì˜ Type :\n', house_df.dtypes[null_column_count.index])
```

<pre>
## Null í”¼ì²˜ì˜ Type :
 MasVnrType      object
BsmtQual        object
BsmtCond        object
BsmtExposure    object
BsmtFinType1    object
BsmtFinType2    object
Electrical      object
GarageType      object
GarageFinish    object
GarageQual      object
GarageCond      object
dtype: object
</pre>
- ìˆ«ìí˜• í”¼ì²˜ë“¤ì˜ ê²½ìš° ê²°ì¸¡ì¹˜ê°€ ë”ì´ìƒ ì¡´ì¬í•˜ì§€ ì•ŠìŒ


### **b) ë¬¸ìí˜• ë³€ìˆ˜ ì „ì²˜ë¦¬**


- ë¬¸ìí˜• í”¼ì²˜ëŠ” ëª¨ë‘ One-hot Encodingìœ¼ë¡œ ë³€í™˜

  - ```pd.get_dummies()``` í™œìš©

  - ```pd.get_dummies()```ëŠ” ìë™ìœ¼ë¡œ ë¬¸ìì—´ í”¼ì²˜ë¥¼ ì›-í•« ì¸ì½”ë”© ë³€í™˜í•˜ë©´ì„œ Null ê°’ì€ 'None' ì¹¼ëŸ¼ìœ¼ë¡œ ëŒ€ì²´í•´ì£¼ê¸° ë•Œë¬¸ì— ë³„ë„ì˜ Nullê°’ ëŒ€ì²´ ë¡œì§ì´ í•„ìš” x



```python
### ë¬¸ìí˜• ë³€ìˆ˜ ì „ì²˜ë¦¬

print('get_dummies() ìˆ˜í–‰ ì „ ë°ì´í„° Shape:', house_df.shape)
house_df_ohe = pd.get_dummies(house_df)
print('get_dummies() ìˆ˜í–‰ í›„ ë°ì´í„° Shape:', house_df_ohe.shape)

null_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() > 0]
print('## Null í”¼ì²˜ì˜ Type :\n', house_df_ohe.dtypes[null_column_count.index])
```

<pre>
get_dummies() ìˆ˜í–‰ ì „ ë°ì´í„° Shape: (1460, 75)
get_dummies() ìˆ˜í–‰ í›„ ë°ì´í„° Shape: (1460, 271)
## Null í”¼ì²˜ì˜ Type :
 Series([], dtype: object)
</pre>
- ì›-í•« ì¸ì½”ë”© í›„ í”¼ì²˜ê°€ 75ê°œì—ì„œ 271ê°œë¡œ ì¦ê°€í•¨

- Null ê°’ì„ ê°€ì§„ í”¼ì²˜ëŠ” ë”ì´ìƒ ì¡´ì¬í•˜ì§€ x


# **2. ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€**


- í•´ë‹¹ ëŒ€íšŒì—ì„œëŠ” ì„±ëŠ¥ í‰ê°€ ì§€í‘œë¥´ ```RMSLE```ë¥¼ ì±„íƒí•¨

- ì´ë¯¸ target ê°’ì¸ ```SalesPrice```ê°€ ë¡œê·¸ ë³€í™˜ë¨

  - ì˜ˆì¸¡ê°’ ì—­ì‹œ ë¡œê·¸ ë³€í™˜ëœ ```SalesPrice``` ê°’ì„ ê¸°ë°˜ìœ¼ë¡¶ ì˜ˆì¸¡í•˜ë¯€ë¡œ ì›ë³¸ SalesPriceê°€ ```ë¡œê·¸ ë³€í™˜```ëœ ê°’ì„

> ì˜ˆì¸¡ ê²°ê³¼ ì˜¤ë¥˜ì— RMSEë§Œ ì ìš©í•˜ë©´ RMSLEê°€ ìë™ìœ¼ë¡œ ì¸¡ì •ë¨



```python
### ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ í•¨ìˆ˜

from sklearn.metrics import mean_squared_error

# ë‹¨ì¼ ëª¨ë¸ì˜ RMSE ê°’ ì¸¡ì •
def get_rmse(model):
    pred = model.predict(X_test)
    mse = mean_squared_error(y_test , pred)
    rmse = np.sqrt(mse)
    print('{0} ë¡œê·¸ ë³€í™˜ëœ RMSE: {1}'.format(model.__class__.__name__,np.round(rmse, 3)))
    return rmse

# get_rmse()ë¥¼ ì´ìš©í•´ ì—¬ëŸ¬ ëª¨ë¸ì˜ RMSE ê°’ì„ ë°˜í™˜
def get_rmses(models):
    rmses = [ ]
    for model in models:
        rmse = get_rmse(model)
        rmses.append(rmse)
    return rmses
```

## **2-1. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸**



```python
### í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

y_target = house_df_ohe['SalePrice']
X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)

X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, 
                                                    test_size = 0.2, random_state = 156)

# LinearRegression, Ridge, Lasso í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)

ridge_reg = Ridge()
ridge_reg.fit(X_train, y_train)

lasso_reg = Lasso()
lasso_reg.fit(X_train, y_train)

models = [lr_reg, ridge_reg, lasso_reg]
get_rmses(models)
```

<pre>
LinearRegression ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.132
Ridge ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.128
Lasso ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.176
</pre>
<pre>
[0.13189576579154297, 0.12750846334053004, 0.17628250556471403]
</pre>

```python
### í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” í•¨ìˆ˜

def get_top_bottom_coef(model):
    # coef_ ì†ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ Series ê°ì²´ë¥¼ ìƒì„±. indexëŠ” ì»¬ëŸ¼ëª…. 
    coef = pd.Series(model.coef_, index = X_features.columns)
    
    # + ìƒìœ„ 10ê°œ, - í•˜ìœ„ 10ê°œ coefficient ì¶”ì¶œí•˜ì—¬ ë°˜í™˜.
    coef_high = coef.sort_values(ascending = False).head(10)
    coef_low = coef.sort_values(ascending = False).tail(10)
    
    return coef_high, coef_low
```


```python
### ëª¨ë¸ë³„ í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”

def visualize_coefficient(models):
    # 3ê°œ íšŒê·€ ëª¨ë¸ì˜ ì‹œê°í™”ë¥¼ ìœ„í•´ 3ê°œì˜ ì»¬ëŸ¼ì„ ê°€ì§€ëŠ” subplot ìƒì„±
    fig, axs = plt.subplots(figsize = (24,10), nrows = 1, ncols = 3)
    fig.tight_layout() 
    
    # ì…ë ¥ ì¸ìë¡œ ë°›ì€ list ê°ì²´ì¸ modelsì—ì„œ ì°¨ë¡€ë¡œ modelì„ ì¶”ì¶œí•˜ì—¬ íšŒê·€ ê³„ìˆ˜ ì‹œê°í™”
    for i_num, model in enumerate(models):
        # ìƒìœ„ 10ê°œ, í•˜ìœ„ 10ê°œ íšŒê·€ ê³„ìˆ˜ë¥¼ êµ¬í•˜ê³ , ì´ë¥¼ íŒë‹¤ìŠ¤ concatìœ¼ë¡œ ê²°í•©
        coef_high, coef_low = get_top_bottom_coef(model)
        coef_concat = pd.concat([coef_high, coef_low])
        
        # ìˆœì°¨ì ìœ¼ë¡œ ax subplotì— barcharë¡œ í‘œí˜„
        # í•œ í™”ë©´ì— í‘œí˜„í•˜ê¸° ìœ„í•´ tick label ìœ„ì¹˜ì™€ font í¬ê¸° ì¡°ì •

        axs[i_num].set_title(model.__class__.__name__+' Coeffiecents', size = 25)
        axs[i_num].tick_params(axis = "y", direction = "in", pad = -120)
        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):
            label.set_fontsize(22)
        sns.barplot(x = coef_concat.values, y = coef_concat.index, ax = axs[i_num])
```


```python
# ì• ì˜ˆì œì—ì„œ í•™ìŠµí•œ lr_reg, ridge_reg, lasso_reg ëª¨ë¸ì˜ íšŒê·€ ê³„ìˆ˜ ì‹œê°í™”

models = [lr_reg, ridge_reg, lasso_reg]
visualize_coefficient(models)
```

<pre>
<Figure size 2400x1000 with 3 Axes>
</pre>
- ```OLS ê¸°ë°˜```ì˜ LinearRegressionê³¼ Ridgeì˜ ê²½ìš°ëŠ” íšŒê·€ ê³„ìˆ˜ê°€ ìœ ì‚¬í•œ í˜•íƒœë¡œ ë¶„í¬ë¼ ìˆìŒ

- ê·¸ëŸ¬ë‚˜ LassoëŠ” ì „ì²´ì ìœ¼ë¡œ íšŒê·€ ê³„ìˆ˜ ê°’ì´ ë§¤ìš° ì‘ê³ , ê·¸ ì¤‘ ```YearBuilt```ë§Œ ê°€ì¥ í¬ê³  ë‹¤ë¥¸ í”¼ì²˜ì˜ íšŒê·€ ê³„ìˆ˜ëŠ” ë„ˆë¬´ ì‘ìŒ



## **2-2. êµì°¨ ê²€ì¦**



```python
from sklearn.model_selection import cross_val_score

def get_avg_rmse_cv(models):
    for model in models:
        # ë¶„í• í•˜ì§€ ì•Šê³  ì „ì²´ ë°ì´í„°ë¡œ cross_val_score( ) ìˆ˜í–‰
        # ëª¨ë¸ë³„ CV RMSEê°’ê³¼ í‰ê·  RMSE ì¶œë ¥
        rmse_list = np.sqrt(- cross_val_score(model, X_features, y_target,
                                             scoring = "neg_mean_squared_error", cv = 5))
        rmse_avg = np.mean(rmse_list)
        print('\n{0} CV RMSE ê°’ ë¦¬ìŠ¤íŠ¸: {1}'.format(model.__class__.__name__, np.round(rmse_list, 3)))
        print('{0} CV í‰ê·  RMSE ê°’: {1}'.format(model.__class__.__name__, np.round(rmse_avg, 3)))

# ì• ì˜ˆì œì—ì„œ í•™ìŠµí•œ lr_reg, ridge_reg, lasso_reg ëª¨ë¸ì˜ CV RMSEê°’ ì¶œë ¥           
models = [lr_reg, ridge_reg, lasso_reg]
get_avg_rmse_cv(models)
```

<pre>

LinearRegression CV RMSE ê°’ ë¦¬ìŠ¤íŠ¸: [0.135 0.165 0.168 0.111 0.198]
LinearRegression CV í‰ê·  RMSE ê°’: 0.155

Ridge CV RMSE ê°’ ë¦¬ìŠ¤íŠ¸: [0.117 0.154 0.142 0.117 0.189]
Ridge CV í‰ê·  RMSE ê°’: 0.144

Lasso CV RMSE ê°’ ë¦¬ìŠ¤íŠ¸: [0.161 0.204 0.177 0.181 0.265]
Lasso CV í‰ê·  RMSE ê°’: 0.198
</pre>
- ë°ì´í„°ì˜ êµ¬ì„±ê³¼ ê´€ê³„ì—†ì´ ë¼ì˜ì˜ ì„±ëŠ¥ì´ OLS ëª¨ë¸ì´ë‚˜ ë¦¿ì§€ ëª¨ë¸ì— ë¹„í•´ ë–¨ì–´ì§

  - êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•œ ë’¤ì—ë„ ë³„ë‹¤ë¥¸ ì„±ëŠ¥ì˜ ê°œì„ ì´ x


## **2-3. í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹**



```python
from sklearn.model_selection import GridSearchCV

def print_best_params(model, params):
    grid_model = GridSearchCV(model, param_grid = params, 
                              scoring = 'neg_mean_squared_error', cv = 5)
    grid_model.fit(X_features, y_target)
    rmse = np.sqrt(-1* grid_model.best_score_)
    print('{0} 5 CV ì‹œ ìµœì  í‰ê·  RMSE ê°’: {1}, ìµœì  alpha:{2}'.format(model.__class__.__name__,
                                        np.round(rmse, 4), grid_model.best_params_))
    return grid_model.best_estimator_

ridge_params = {'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20]}
lasso_params = {'alpha':[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10]}
best_rige = print_best_params(ridge_reg, ridge_params)
best_lasso = print_best_params(lasso_reg, lasso_params)
```

<pre>
Ridge 5 CV ì‹œ ìµœì  í‰ê·  RMSE ê°’: 0.1418, ìµœì  alpha:{'alpha': 12}
Lasso 5 CV ì‹œ ìµœì  í‰ê·  RMSE ê°’: 0.142, ìµœì  alpha:{'alpha': 0.001}
</pre>
- ë¼ì˜ ëª¨ë¸ì˜ ê²½ìš° alpha ê°’ ìµœì í™” ì´í›„ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë§ì´ ì¢‹ì•„ì§



```python
### ìµœì  alpha ê°’ì„ ì„¤ì •í•œ ë’¤, ë‹¤ì‹œ ëª¨ë¸ í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€ ìˆ˜í–‰í•´ë³´ê¸°


# ì•ì˜ ìµœì í™” alphaê°’ìœ¼ë¡œ í•™ìŠµë°ì´í„°ë¡œ í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ë° í‰ê°€ ìˆ˜í–‰. 
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)

ridge_reg = Ridge(alpha = 12)
ridge_reg.fit(X_train, y_train)

lasso_reg = Lasso(alpha = 0.001)
lasso_reg.fit(X_train, y_train)

# ëª¨ë“  ëª¨ë¸ì˜ RMSE ì¶œë ¥
models = [lr_reg, ridge_reg, lasso_reg]
get_rmses(models)

# ëª¨ë“  ëª¨ë¸ì˜ íšŒê·€ ê³„ìˆ˜ ì‹œê°í™” 
models = [lr_reg, ridge_reg, lasso_reg]
visualize_coefficient(models)
```

<pre>
LinearRegression ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.132
Ridge ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.124
Lasso ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.12
</pre>
<pre>
<Figure size 2400x1000 with 3 Axes>
</pre>
- alpha ê°’ ìµœì í™” í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§

- ëª¨ë¸ë³„ íšŒê·€ ê³„ìˆ˜ë„ ë§ì´ ë‹¬ë¼ì§

- ê¸°ì¡´ì—ëŠ” ë¼ì˜ ëª¨ë¸ì˜ íšŒê·€ ê³„ìˆ˜ê°€ ë‚˜ë¨¸ì§€ ë‘ ê°œ ëª¨ë¸ê³¼ ë§ì€ ì°¨ì´ê°€ ìˆì—ˆì§€ë§Œ, ì´ë²ˆì—ëŠ” ë¦¿ì§€ì™€ ë¼ì˜ ëª¨ë¸ì—ì„œ ë¹„ìŠ·í•œ í”¼ì²˜ì˜ íšŒê·€ ê³„ìˆ˜ê°€ ë†’ìŒ

- ë‹¤ë§Œ ë¼ì˜ ëª¨ë¸ì˜ ê²½ìš°ëŠ” ë¦¿ì§€ì— ë¹„í•´ ë™ì¼í•œ í”¼ì²˜ë¼ë„ íšŒê·€ ê³„ìˆ˜ì˜ ê°’ì´ ìƒë‹¹íˆ ì‘ìŒ


# **3. ì¶”ê°€ì ì¸ ë°ì´í„° ê°€ê³µ**


## **3-1. í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸ì˜ ë°ì´í„° ë¶„í¬ë„**



```python
from scipy.stats import skew

# objectê°€ ì•„ë‹Œ ìˆ«ìí˜• í”¼ì³ì˜ ì»¬ëŸ¼ index ê°ì²´ ì¶”ì¶œ.
features_index = house_df.dtypes[house_df.dtypes != 'object'].index
# house_dfì— ì»¬ëŸ¼ indexë¥¼ [ ]ë¡œ ì…ë ¥í•˜ë©´ í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ ë°ì´í„° ì…‹ ë°˜í™˜. apply lambdaë¡œ skew( )í˜¸ì¶œ 
skew_features = house_df[features_index].apply(lambda x : skew(x))
# skew ì •ë„ê°€ 1 ì´ìƒì¸ ì»¬ëŸ¼ë“¤ë§Œ ì¶”ì¶œ. 
skew_features_top = skew_features[skew_features > 1]
print(skew_features_top.sort_values(ascending=False))
```

<pre>
MiscVal          24.451640
PoolArea         14.813135
LotArea          12.195142
3SsnPorch        10.293752
LowQualFinSF      9.002080
KitchenAbvGr      4.483784
BsmtFinSF2        4.250888
ScreenPorch       4.117977
BsmtHalfBath      4.099186
EnclosedPorch     3.086696
MasVnrArea        2.673661
LotFrontage       2.382499
OpenPorchSF       2.361912
BsmtFinSF1        1.683771
WoodDeckSF        1.539792
TotalBsmtSF       1.522688
MSSubClass        1.406210
1stFlrSF          1.375342
GrLivArea         1.365156
dtype: float64
</pre>
- ì¼ë°˜ì ìœ¼ë¡œ skew() í•¨ìˆ˜ì˜ ë°˜í™˜ ê°’ì´ ```1 ì´ìƒ```ì¸ ê²½ìš°ë¥¼ ì™œê³¡ ì •ë„ê°€ ë†’ë‹¤ê³  íŒë‹¨í•¨

  - ìƒí™©ì— ë”°ë¼ í¸ì°¨ëŠ” ìˆìŒ

  - ì—¬ê¸°ì„œëŠ” 1 ì´ìƒì˜ ê°’ì„ ë°˜í™˜í•˜ëŠ” í”¼ì²˜ë§Œ ì¶”ì¶œí•´ ì™œê³¡ ì •ë„ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ë¡œê·¸ ë³€í™˜ì„ ì ìš©

- skew()ë¥¼ ì ìš©í•˜ëŠ” ìˆ«ìí˜• í”¼ì²˜ì—ì„œ ```ì›-í•« ì¸ì½”ë”©``` ëœ ì¹´í…Œê³ ë¦¬ ìˆ«ìí˜• í”¼ì²˜ëŠ” ì œì™¸í•´ì•¼ í•¨

  - ì½”ë“œì„± í”¼ì²˜ì´ë¯€ë¡œ ì¸ì½”ë”© ì‹œ ë‹¹ì—°íˆ ì™œê³¡ë  ê°€ëŠ¥ì„±ì´ ë†’ì§€ë§Œ, ì´ëŠ” ì™œê³¡ê³¼ëŠ” ë¬´ê´€í•¨



```python
### ì™œê³¡ì´ ì‹¬í•œ ë³€ìˆ˜ë“¤ì„ ë¡œê·¸ ë³€í™˜

house_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index])
```


```python
### ì™œê³¡ ì •ë„ë¥¼ ë‹¤ì‹œ í™•ì¸

# objectê°€ ì•„ë‹Œ ìˆ«ìí˜• í”¼ì³ì˜ ì»¬ëŸ¼ index ê°ì²´ ì¶”ì¶œ.
features_index = house_df.dtypes[house_df.dtypes != 'object'].index
# house_dfì— ì»¬ëŸ¼ indexë¥¼ [ ]ë¡œ ì…ë ¥í•˜ë©´ í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ ë°ì´í„° ì…‹ ë°˜í™˜. apply lambdaë¡œ skew( )í˜¸ì¶œ 
skew_features = house_df[features_index].apply(lambda x : skew(x))
# skew ì •ë„ê°€ 1 ì´ìƒì¸ ì»¬ëŸ¼ë“¤ë§Œ ì¶”ì¶œ. 
skew_features_top = skew_features[skew_features > 1]
print(skew_features_top.sort_values(ascending=False))
```

<pre>
PoolArea         14.348342
3SsnPorch         7.727026
LowQualFinSF      7.452650
MiscVal           5.165390
BsmtHalfBath      3.929022
KitchenAbvGr      3.865437
ScreenPorch       3.147171
BsmtFinSF2        2.521100
EnclosedPorch     2.110104
dtype: float64
</pre>
- ì—¬ì „íˆ ë†’ì€ ì™œê³¡ ì •ë„ë¥¼ ê°€ì§„ í”¼ì²˜ê°€ ìˆì§€ë§Œ, ë” ì´ìƒ ë¡œê·¸ ë³€í™˜ì„ í•˜ë”ë¼ë„ ê°œì„ í•˜ê¸°ëŠ” ì–´ë µê¸°ì— ê·¸ëŒ€ë¡œ ìœ ì§€



```python
### ë°ì´í„° ì¬ê°€ê³µ

# Skewê°€ ë†’ì€ í”¼ì²˜ë“¤ì„ ë¡œê·¸ ë³€í™˜ í–ˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ ì›-í•« ì¸ì½”ë”© ì ìš© ë° í”¼ì²˜/íƒ€ê²Ÿ ë°ì´í„° ì…‹ ìƒì„±
house_df_ohe = pd.get_dummies(house_df)
y_target = house_df_ohe['SalePrice']
X_features = house_df_ohe.drop('SalePrice',axis = 1, inplace = False)
X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, 
                                                    test_size = 0.2, random_state = 156)

# í”¼ì²˜ë“¤ì„ ë¡œê·¸ ë³€í™˜ í›„ ë‹¤ì‹œ ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì™€ RMSE ì¶œë ¥
ridge_params = {'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20]}
lasso_params = {'alpha':[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10]}
best_ridge = print_best_params(ridge_reg, ridge_params)
best_lasso = print_best_params(lasso_reg, lasso_params)
```

<pre>
Ridge 5 CV ì‹œ ìµœì  í‰ê·  RMSE ê°’: 0.1275, ìµœì  alpha:{'alpha': 10}
Lasso 5 CV ì‹œ ìµœì  í‰ê·  RMSE ê°’: 0.1252, ìµœì  alpha:{'alpha': 0.001}
</pre>
- ë¦¿ì§€ ëª¨ë¸ì˜ ê²½ìš° ìµœì  alpha ê°’ì´ 12ì—ì„œ 10ìœ¼ë¡œ ë³€ê²½ë¨

- ë‘ ëª¨ë¸ ëª¨ë‘ í”¼ì²˜ì˜ ë¡œê·¸ ë³€í™˜ ì´ì „ê³¼ ë¹„êµí•´ RMSE ê°’ì´ í–¥ìƒë¨




```python
### í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”

# ì•ì˜ ìµœì í™” alpha ê°’ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¡œ í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡/í‰ê°€ ìˆ˜í–‰
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)

ridge_reg = Ridge(alpha = 10)
ridge_reg.fit(X_train, y_train)

lasso_reg = Lasso(alpha = 0.001)
lasso_reg.fit(X_train, y_train)

# ëª¨ë“  ëª¨ë¸ì˜ RMSE ì¶œë ¥
models = [lr_reg, ridge_reg, lasso_reg]
get_rmses(models)

# ëª¨ë“  ëª¨ë¸ì˜ íšŒê·€ ê³„ìˆ˜ ì‹œê°í™” 
models = [lr_reg, ridge_reg, lasso_reg]
visualize_coefficient(models)
```

<pre>
LinearRegression ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.128
Ridge ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.122
Lasso ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.119
</pre>
<pre>
<Figure size 2400x1000 with 3 Axes>
</pre>
- ì„¸ ëª¨ë¸ ëª¨ë‘ ```GrLivArea```, ì¦‰ ì£¼ê±° ê³µê°„ í¬ê¸°ê°€ íšŒê·€ ê³„ìˆ˜ê°€ ê°€ì¥ ë†’ì€ í”¼ì²˜ê°€ ë¨


## **3-2. ì´ìƒì¹˜ ë°ì´í„° ì²˜ë¦¬**

- íšŒê·€ ê³„ìˆ˜ê°€ ë†’ì€ í”¼ì²˜, ì¦‰ ì˜ˆì¸¡ì— ë§ì€ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš” í”¼ì²˜ì˜ ì´ìƒì¹˜ ë°ì´í„°ì˜ ì²˜ë¦¬ê°€ ì¤‘ìš”í•¨



```python
### GrLivArea í”¼ì²˜ì˜ ë°ì´í„° ë¶„í¬ë„ ì‚´í´ë³´ê¸°

plt.scatter(x = house_df_org['GrLivArea'], y = house_df_org['SalePrice'])
plt.ylabel('SalePrice', fontsize=15)
plt.xlabel('GrLivArea', fontsize=15)
plt.show()
```

<pre>
<Figure size 640x480 with 1 Axes>
</pre>
- ì¼ë°˜ì ìœ¼ë¡œ ì£¼ê±° ê³µê°„ì´ í° ì§‘ì¼ìˆ˜ë¡ ê°€ê²©ì´ ë¹„ì‹¸ê¸° ë•Œë¬¸ì— GrLivArea í”¼ì²˜ëŠ” SalesPriceì™€ ì–‘ì˜ ìƒê´€ë„ê°€ ë§¤ìš° ë†’ìŒ

- ì´ìƒì¹˜ê°€ ì¡´ì¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

  - GrLivAreaê°€ ê°€ì¥ í° ë°ë„ ë¶ˆêµ¬í•˜ê³  ê°€ê²©ì´ ë§¤ìš° ë‚®ìŒ

  - GrLivAreaê°€ 4000í‰ë°©í”¼íŠ¸ ì´ìƒì„ì—ë„ ê°€ê²©ì´ 500,000 ë‹¬ëŸ¬ ì´í•˜ì¸ ë°ì´í„°ëŠ” ëª¨ë‘ ì´ìƒì¹˜ë¡œ ê°„ì£¼í•˜ê³  ì‚­ì œ 



```python
### ì´ìƒì¹˜ ë°ì´í„° ì‚­ì œ

# GrLivAreaì™€ SalePrice ëª¨ë‘ ë¡œê·¸ ë³€í™˜ë˜ì—ˆìœ¼ë¯€ë¡œ ì´ë¥¼ ë°˜ì˜í•œ ì¡°ê±´ ìƒì„±
cond1 = house_df_ohe['GrLivArea'] > np.log1p(4000)
cond2 = house_df_ohe['SalePrice'] < np.log1p(500000)
outlier_index = house_df_ohe[cond1 & cond2].index

print('ì•„ì›ƒë¼ì´ì–´ ë ˆì½”ë“œ index :', outlier_index.values)
print('ì•„ì›ƒë¼ì´ì–´ ì‚­ì œ ì „ house_df_ohe shape:', house_df_ohe.shape)

# DataFrameì˜ indexë¥¼ ì´ìš©í•˜ì—¬ ì•„ì›ƒë¼ì´ì–´ ë ˆì½”ë“œ ì‚­ì œ. 
house_df_ohe.drop(outlier_index , axis = 0, inplace = True)
print('ì•„ì›ƒë¼ì´ì–´ ì‚­ì œ í›„ house_df_ohe shape:', house_df_ohe.shape)
```

<pre>
ì•„ì›ƒë¼ì´ì–´ ë ˆì½”ë“œ index : [ 523 1298]
ì•„ì›ƒë¼ì´ì–´ ì‚­ì œ ì „ house_df_ohe shape: (1460, 271)
ì•„ì›ƒë¼ì´ì–´ ì‚­ì œ í›„ house_df_ohe shape: (1458, 271)
</pre>

```python
### ë°ì´í„° ë‹¤ì‹œ ìƒì„±

y_target = house_df_ohe['SalePrice']
X_features = house_df_ohe.drop('SalePrice',axis = 1, inplace = False)
X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, 
                                                    test_size = 0.2, random_state = 156)
```


```python
### ëª¨ë¸ ìµœì í™”

ridge_params = {'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20]}
lasso_params = {'alpha':[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10]}
best_ridge = print_best_params(ridge_reg, ridge_params)
best_lasso = print_best_params(lasso_reg, lasso_params)
```

<pre>
Ridge 5 CV ì‹œ ìµœì  í‰ê·  RMSE ê°’: 0.1125, ìµœì  alpha:{'alpha': 8}
Lasso 5 CV ì‹œ ìµœì  í‰ê·  RMSE ê°’: 0.1122, ìµœì  alpha:{'alpha': 0.001}
</pre>
- ë‹¨ ë‘ ê°œì˜ ì´ìƒì¹˜ ë°ì´í„°ë§Œ ì œê±°í•˜ì˜€ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì˜ˆì¸¡ ìˆ˜ì¹˜ê°€ ë§¤ìš° í¬ê²Œ í–¥ìƒë¨

- ë¦¿ì§€ ëª¨ë¸ì˜ ê²½ìš° ìµœì  alpha ê°’ì´ 12ì—ì„œ 8ë¡œ ë³€ê²½ë¨

  - í‰ê·  RMSEê°€ 0.1275ì—ì„œ 0.1125ë¡œ ê°œì„ ë¨

- ë¼ì˜ ëª¨ë¸ì˜ ê²½ìš° í‰ê·  RMSEê°€ 0.1252ì—ì„œ 0.1122ë¡œ ê°œì„ ë¨  


- ì´ìƒì¹˜ë¥¼ ì°¾ëŠ” ê²ƒì€ ì‰½ì§€ ì•Šì§€ë§Œ. íšŒê·€ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í”¼ì²˜ë“¤ ìœ„ì£¼ë¡œ ```ì´ìƒì¹˜ ë°ì´í„°```ë¥¼ ì°¾ìœ¼ë ¤ëŠ” ë…¸ë ¥ì€ ì¤‘ìš”í•¨

- ë³´í†µ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì„¸ìŠ¤ ì¤‘ì—ì„œ ë°ì´í„°ì˜ ê°€ê³µì€ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ê¸° ì´ì „ì— ìˆ˜í–‰í•¨

  - í•˜ì§€ë§Œ ì´ê²ƒì´ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ê¸° ì´ì „ì— ì™„ë²½í•˜ê²Œ ë°ì´í„°ì˜ ì„ ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë¼ëŠ” ì˜ë¯¸ëŠ” ì•„ë‹˜

  - ì¼ë‹¨ ëŒ€ëµì˜ ë°ì´í„° ê°€ê³µê³¼ ëª¨ë¸ ìµœì í™”ë¥¼ ìˆ˜í–‰í•œ ë’¤ ë‹¤ì‹œ ì´ì— ê¸°ë°˜í•œ ì—¬ëŸ¬ ê°€ì§€ ê¸°ë²•ì˜ ë°ì´í„° ê°€ê³µê³¼ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê¸°ë°˜ì˜ ëª¨ë¸ ìµœì í™”ë¥¼ ```ë°˜ë³µì ```ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ìƒì„± ê³¼ì •ì„



```python
# ì•ì˜ ìµœì í™”í•œ alpha ê°’ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¡œ í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡/í‰ê°€ ìˆ˜í–‰
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)

ridge_reg = Ridge(alpha = 8)
ridge_reg.fit(X_train, y_train)

lasso_reg = Lasso(alpha = 0.001)
lasso_reg.fit(X_train, y_train)

# ëª¨ë“  ëª¨ë¸ì˜ RMSE ì¶œë ¥
models = [lr_reg, ridge_reg, lasso_reg]
get_rmses(models)

# ëª¨ë“  ëª¨ë¸ì˜ íšŒê·€ ê³„ìˆ˜ ì‹œê°í™” 
models = [lr_reg, ridge_reg, lasso_reg]
visualize_coefficient(models)
```

<pre>
LinearRegression ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.129
Ridge ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.103
Lasso ë¡œê·¸ ë³€í™˜ëœ RMSE: 0.1
</pre>
<pre>
<Figure size 2400x1000 with 3 Axes>
</pre>
# **4. íšŒê·€ íŠ¸ë¦¬ í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€**


- ìˆ˜í–‰ ì‹œê°„ ì´ìŠˆë¡œ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •ì„ ë¯¸ë¦¬ í•´ë‘ê³  ìˆ˜í–‰



```python
from xgboost import XGBRegressor

xgb_params = {'n_estimators':[1000]}
xgb_reg = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, 
                       colsample_bytree = 0.5, subsample = 0.8)
best_xgb = print_best_params(xgb_reg, xgb_params)
```

<pre>
XGBRegressor 5 CV ì‹œ ìµœì  í‰ê·  RMSE ê°’: 0.1182, ìµœì  alpha:{'n_estimators': 1000}
</pre>

```python
from lightgbm import LGBMRegressor

lgbm_params = {'n_estimators':[1000]}
lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, 
                         subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1)
best_lgbm = print_best_params(lgbm_reg, lgbm_params)
```

<pre>
LGBMRegressor 5 CV ì‹œ ìµœì  í‰ê·  RMSE ê°’: 0.1163, ìµœì  alpha:{'n_estimators': 1000}
</pre>

```python
### í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”

# ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 20ê°œì˜ í”¼ì²˜ëª…ê³¼ ê·¸ë•Œì˜ ì¤‘ìš”ë„ ê°’ì„ Seriesë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def get_top_features(model):
    ftr_importances_values = model.feature_importances_
    ftr_importances = pd.Series(ftr_importances_values, index = X_features.columns)
    ftr_top20 = ftr_importances.sort_values(ascending = False)[:20]
    return ftr_top20

def visualize_ftr_importances(models):
    # 2ê°œ íšŒê·€ ëª¨ë¸ì˜ ì‹œê°í™”ë¥¼ ìœ„í•´ 2ê°œì˜ ì»¬ëŸ¼ì„ ê°€ì§€ëŠ” subplot ìƒì„±
    fig, axs = plt.subplots(figsize = (24,10), nrows = 1, ncols = 2)
    fig.tight_layout() 
    
    # ì…ë ¥ ì¸ìë¡œ ë°›ì€ list ê°ì²´ì¸ modelsì—ì„œ ì°¨ë¡€ë¡œ modelì„ ì¶”ì¶œí•˜ì—¬ í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”
    for i_num, model in enumerate(models):
        # ì¤‘ìš”ë„ ìƒìœ„ 20ê°œì˜ í”¼ì²˜ëª…ê³¼ ê·¸ë•Œì˜ ì¤‘ìš”ë„ ê°’ ì¶”ì¶œ 
        ftr_top20 = get_top_features(model)
        axs[i_num].set_title(model.__class__.__name__ + ' Feature Importances', size = 25)
        
        #font í¬ê¸° ì¡°ì •
        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):
            label.set_fontsize(22)
        sns.barplot(x = ftr_top20.values, y = ftr_top20.index, ax = axs[i_num])

# ì• ì˜ˆì œì—ì„œ print_best_params( )ê°€ ë°˜í™˜í•œ GridSearchCVë¡œ ìµœì í™” ëœ ëª¨ë¸ì˜ í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”    
models = [best_xgb, best_lgbm]
visualize_ftr_importances(models)
```

<pre>
<Figure size 2400x1000 with 2 Axes>
</pre>
# **5. íšŒê·€ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ ê²°ê³¼ í˜¼í•©ì„ í†µí•œ ìµœì¢… ì˜ˆì¸¡**


- ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì¹˜ì— ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ ìµœì¢… ì˜ˆì¸¡ì¹˜ë¥¼ ë„ì¶œ

- ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¡°ê¸ˆ ì¢‹ì€ ìª½ì— ì£¼ë¡œ ë” í° ê°€ì¤‘ì¹˜ë¥¼ ë‘ 


**1. Ridge + Lasso**



```python
def get_rmse_pred(preds):
    for key in preds.keys():
        pred_value = preds[key]
        mse = mean_squared_error(y_test, pred_value)
        rmse = np.sqrt(mse)
        print('{0} ëª¨ë¸ì˜ RMSE: {1}'.format(key, rmse))

# ê°œë³„ ëª¨ë¸ì˜ í•™ìŠµ
ridge_reg = Ridge(alpha = 8)
ridge_reg.fit(X_train, y_train)

lasso_reg = Lasso(alpha = 0.001)
lasso_reg.fit(X_train, y_train)

# ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡
ridge_pred = ridge_reg.predict(X_test)
lasso_pred = lasso_reg.predict(X_test)

# ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ê°’ í˜¼í•©ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡ê°’ ë„ì¶œ
pred = 0.4 * ridge_pred + 0.6 * lasso_pred
preds = {'ìµœì¢… í˜¼í•©': pred,
         'Ridge': ridge_pred,
         'Lasso': lasso_pred}
         
#ìµœì¢… í˜¼í•© ëª¨ë¸, ê°œë³„ëª¨ë¸ì˜ RMSE ê°’ ì¶œë ¥
get_rmse_pred(preds)
```

<pre>
ìµœì¢… í˜¼í•© ëª¨ë¸ì˜ RMSE: 0.10007930884470506
Ridge ëª¨ë¸ì˜ RMSE: 0.1034517754660323
Lasso ëª¨ë¸ì˜ RMSE: 0.10024170460890035
</pre>
- ìµœì¢… í˜¼í•© ëª¨ë¸ì˜ RMSEê°€ ê°œë³„ ëª¨ë‹ë³´ë‹¤ ì„±ëŠ¥ ë©´ì—ì„œ ì•½ê°„ ê°œì„ ë¨



**2. XGBoost + LightGBM**



```python
xgb_reg = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, 
                       colsample_bytree = 0.5, subsample = 0.8)
lgbm_reg = LGBMRegressor(n_estimators = 1000, learning_rate = 0.05, num_leaves = 4, 
                         subsample = 0.6, colsample_bytree = 0.4, reg_lambda = 10, n_jobs = -1)
xgb_reg.fit(X_train, y_train)
lgbm_reg.fit(X_train, y_train)

xgb_pred = xgb_reg.predict(X_test)
lgbm_pred = lgbm_reg.predict(X_test)

pred = 0.5 * xgb_pred + 0.5 * lgbm_pred
preds = {'ìµœì¢… í˜¼í•©': pred,
         'XGBM': xgb_pred,
         'LGBM': lgbm_pred}
        
get_rmse_pred(preds)
```

<pre>
ìµœì¢… í˜¼í•© ëª¨ë¸ì˜ RMSE: 0.10129327758047968
XGBM ëª¨ë¸ì˜ RMSE: 0.10617576258589495
LGBM ëª¨ë¸ì˜ RMSE: 0.10382510019327311
</pre>
- í˜¼í•© ëª¨ë¸ì˜ RMSEê°€ ê°œë³„ ëª¨ë¸ì˜ RMSEë³´ë‹¤ ì¡°ê¸ˆ í–¥ìƒë¨


# **6. ìŠ¤íƒœí‚¹ ëª¨ë¸ì„ í†µí•œ íšŒê·€ ì˜ˆì¸¡**


- ìŠ¤íƒœí‚¹ ëª¨ë¸ì€ ë‘ ì¢…ë¥˜ì˜ ëª¨ë¸ì„ í•„ìš”ë¡œ í•¨

  1. ê°œë³„ì ì¸ ```ê¸°ë°˜``` ëª¨ë¸

  2. ê°œë³„ ê¸°ë°˜ ëª¨ë¸ì˜ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ë§Œë“¤ì–´ì„œ í•™ìŠµí•˜ëŠ” ìµœì¢… ```ë©”íƒ€``` ëª¨ë¸ 

- ìŠ¤íƒœí‚¹ ëª¨ë¸ì˜ í•µì‹¬ì€ ì—¬ëŸ¬ ê°œë³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ê°ê° ìŠ¤íƒœí‚¹ í˜•íƒœë¡œ ê²°í•©í•´ ìµœì¢… ë©”íƒ€ ëª¨ë¸ì˜ í•™ìŠµìš© í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ìš© í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë§Œë“œëŠ” ê²ƒì„

  - ìµœì¢… ë©”íƒ€ ëª¨ë¸ì´ í•™ìŠµí•  í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸ëŠ” ì›ë³¸ í•™ìŠµ í”¼ì²˜ ì„¸íŠ¸ë¡œ í•™ìŠµí•œ ê°œë³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ìŠ¤íƒœí‚¹ í˜•íƒœë¡œ ê²°í•©í•œ ê²ƒ



```python
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error

# ê°œë³„ ê¸°ë°˜ ëª¨ë¸ì—ì„œ ìµœì¢… ë©”íƒ€ ëª¨ë¸ì´ ì‚¬ìš©í•  í•™ìŠµ/í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ 
def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):
    # ì§€ì •ëœ n_foldsê°’ìœ¼ë¡œ KFold ìƒì„±
    kf = KFold(n_splits = n_folds, shuffle = False)
    # ì¶”í›„ì— ë©”íƒ€ ëª¨ë¸ì´ ì‚¬ìš©í•  í•™ìŠµ ë°ì´í„° ë°˜í™˜ì„ ìœ„í•œ ë„˜íŒŒì´ ë°°ì—´ ì´ˆê¸°í™” 
    train_fold_pred = np.zeros((X_train_n.shape[0], 1))
    test_pred = np.zeros((X_test_n.shape[0], n_folds))
    print(model.__class__.__name__ , ' model ì‹œì‘ ')
    
    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):
        # ì…ë ¥ëœ í•™ìŠµ ë°ì´í„°ì—ì„œ ê¸°ë°˜ ëª¨ë¸ì´ í•™ìŠµ/ì˜ˆì¸¡í•  í´ë“œ ë°ì´í„° ì…‹ ì¶”ì¶œ 
        print('\t í´ë“œ ì„¸íŠ¸: ', folder_counter,' ì‹œì‘ ')
        X_tr = X_train_n[train_index] 
        y_tr = y_train_n[train_index] 
        X_te = X_train_n[valid_index]  
        
        # í´ë“œ ì„¸íŠ¸ ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ ë§Œë“¤ì–´ì§„ í•™ìŠµ ë°ì´í„°ë¡œ ê¸°ë°˜ ëª¨ë¸ì˜ í•™ìŠµ ìˆ˜í–‰
        model.fit(X_tr, y_tr)       
        # í´ë“œ ì„¸íŠ¸ ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ ë§Œë“¤ì–´ì§„ ê²€ì¦ ë°ì´í„°ë¡œ ê¸°ë°˜ ëª¨ë¸ ì˜ˆì¸¡ í›„ ë°ì´í„° ì €ì¥.
        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1)
        # ì…ë ¥ëœ ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í´ë“œ ì„¸íŠ¸ ë‚´ í•™ìŠµëœ ê¸°ë°˜ ëª¨ë¸ì—ì„œ ì˜ˆì¸¡ í›„ ë°ì´í„° ì €ì¥. 
        test_pred[:, folder_counter] = model.predict(X_test_n)
            
    # í´ë“œ ì„¸íŠ¸ ë‚´ì—ì„œ ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•œ ë°ì´í„°ë¥¼ í‰ê· í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìƒì„± 
    test_pred_mean = np.mean(test_pred, axis = 1).reshape(-1,1)    
    
    #train_fold_predëŠ” ìµœì¢… ë©”íƒ€ ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” í•™ìŠµ ë°ì´í„°, test_pred_meanì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    return train_fold_pred , test_pred_mean
```


```python
### 1. ê°œë³„ ëª¨ë¸ í•™ìŠµ

# get_stacking_base_datasets( )ì€ ë„˜íŒŒì´ ndarrayë¥¼ ì¸ìë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ DataFrameì„ ë„˜íŒŒì´ë¡œ ë³€í™˜
X_train_n = X_train.values
X_test_n = X_test.values
y_train_n = y_train.values

# ê° ê°œë³„ ê¸°ë°˜(Base)ëª¨ë¸ì´ ìƒì„±í•œ í•™ìŠµìš©/í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë°˜í™˜. 
ridge_train, ridge_test = get_stacking_base_datasets(ridge_reg, X_train_n, y_train_n, X_test_n, 5)
lasso_train, lasso_test = get_stacking_base_datasets(lasso_reg, X_train_n, y_train_n, X_test_n, 5)
xgb_train, xgb_test = get_stacking_base_datasets(xgb_reg, X_train_n, y_train_n, X_test_n, 5)  
lgbm_train, lgbm_test = get_stacking_base_datasets(lgbm_reg, X_train_n, y_train_n, X_test_n, 5)
```

<pre>
Ridge  model ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  0  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  1  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  2  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  3  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  4  ì‹œì‘ 
Lasso  model ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  0  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  1  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  2  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  3  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  4  ì‹œì‘ 
XGBRegressor  model ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  0  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  1  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  2  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  3  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  4  ì‹œì‘ 
LGBMRegressor  model ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  0  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  1  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  2  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  3  ì‹œì‘ 
	 í´ë“œ ì„¸íŠ¸:  4  ì‹œì‘ 
</pre>

```python
### 2. ìµœì¢… ë©”íƒ€ ëª¨ë¸ í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€

# ê°œë³„ ëª¨ë¸ì´ ë°˜í™˜í•œ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ì„¸íŠ¸ë¥¼ Stacking í˜•íƒœë¡œ ê²°í•©
Stack_final_X_train = np.concatenate((ridge_train, lasso_train, 
                                      xgb_train, lgbm_train), axis = 1)
Stack_final_X_test = np.concatenate((ridge_test, lasso_test, 
                                     xgb_test, lgbm_test), axis = 1)

# ìµœì¢… ë©”íƒ€ ëª¨ë¸ì€ ë¼ì˜ ëª¨ë¸ì„ ì ìš©
meta_model_lasso = Lasso(alpha = 0.0005)

# ê¸°ë°˜ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡­ê²Œ ë§Œë“¤ì–´ì§„ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¡œ ì˜ˆì¸¡í•˜ê³  RMSE ì¸¡ì •
meta_model_lasso.fit(Stack_final_X_train, y_train)
final = meta_model_lasso.predict(Stack_final_X_test)
mse = mean_squared_error(y_test , final)
rmse = np.sqrt(mse)
print('ìŠ¤íƒœí‚¹ íšŒê·€ ëª¨ë¸ì˜ ìµœì¢… RMSE ê°’ì€:', rmse)
```

<pre>
ìŠ¤íƒœí‚¹ íšŒê·€ ëª¨ë¸ì˜ ìµœì¢… RMSE ê°’ì€: 0.09751138566662847
</pre>
- ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ í‰ê°€ë¥¼ ë³´ì—¬ì¤Œ

- ìŠ¤íƒœí‚¹ ëª¨ë¸ì€ ë¶„ë¥˜ë¿ë§Œ ì•„ë‹ˆë¼ íšŒê·€ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆìŒ

